

\documentclass[11pt]{scrartcl}
\usepackage[sexy]{../evan}
\usepackage{cmbright}
\usepackage{cancel}
\usepackage[T1]{fontenc}
%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
%\usepackage[pdfborder={0 0 0},colorlinks=true,citecolor=red{}]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{systeme}
\usepackage[vlined]{algorithm2e}
\usepackage{mathtools}
\SetStartEndCondition{ }{}{}%
\SetKwProg{Fn}{def}{\string:}{}
\SetKwFunction{Range}{range}%%
\SetKw{KwTo}{in}\SetKwFor{For}{for}{\string:}{}%
\SetKwIF{If}{ElseIf}{Else}{if}{:}{elif}{else:}{}%
\SetKwFor{While}{while}{:}{fintq}%
\newcommand{\forcond}{$i=0$ \KwTo $n$}
\SetKwFunction{FRecurs}{FnRecursive}%
\AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%
\newcommand{\opt}{\text{OPT}}
\newcommand{\maxp}{\text{MaxPath}}
\newcommand{\ctn}{\text{MaxContains}}
\newcommand{\dctn}{\text{MaxDoesNotContain}}
\newcommand{\mpath}{\text{MinPath}}

\title{CS 577: HW 3}
\author{Daniel Ko}
\date{Summer 2020}

\begin{document}
\maketitle

%problem 1
\section{
  Kleinberg Chapter 6, Q14
 }

%problem 1a
\subsection{
	Suppose it is possible to choose a single path $P$ that is an $s-t$ path in
	each of the graphs $G_0, G_1, \cdots , G_b$. Give a polynomial-time algorithm
	to find the shortest such path.
}
%More formally, we assume that there exists a path $p_\phi$ such that $p_\phi \subseteq G_i$ for all $i$.
We define the set of edges that exist in all points in time as $$E^* = \bigcap_{i=0}^{b} E_i$$
Since our graph is unweighted, we can perform breadth first search to find the shortest path from
$s$ to $t$ on $G^* = (V, E^*)$.
%We define $n$ to be size of the biggest $E_i$
%or more formally, $n = \max(\{|E_i| : 0 \leq i \leq b\})$.
%The complexity of computing $E^*$ is $\Theta(nb)$.
%The computing complexity of breadth first search is $\Theta(|V| + |E^*|)$.
%The total complexity would be $\Theta(nb + |V| + |E^*|) = O(n(b + |V|))$
%which is polynomial-time.

%We will assume that each of these graphs Gi is connected.
\iffalse
	We define $\delta(v,t)$ to be the length of the shortest
	path from $v$ to $t$.
	$$
		\delta(v,t) =
		\begin{cases}
			1                                                   & \textbf{when } (v, t) \in E^*             \\
			\infty                                              & \textbf{when } v \text{ has been visited} \\
			\min(\{ 1 + \delta(\phi,t) \mid (v,\phi) \in E^*\}) & \textbf{otherwise}                        \\
		\end{cases}
	$$
	Similiar to (6.23) from the book, we can modify the Bellmanâ€“Ford algorithm
	to find the shortest path from $s$ to $t$ that exists in all $G_i$. We define $\opt(i,v)$ to be the length of the shortest
	path from $v$ to $t$ using at most $i$ edges.
	$$\opt(i,v) =
		\begin{cases}
			\min \bigg(\opt(i-1,v),\min\Big(\{1 + \opt(i-1,w) \mid  (v,w) \in E^*\}\Big) \bigg) & \textbf{when } i > 0 \\
			0                                                                                   & \textbf{when } v = t \\
			\infty                                                                              & \textbf{otherwise}
		\end{cases}
	$$
\fi


%problem 1b
\subsection{
	Give a polynomial-time algorithm to find a sequence of paths
	$P_0, P_1, \cdots , P_b$ of minimum cost, where $P_i$ is an $s-t$ path in $G_i$ for
	$i = 0, 1, \cdots, b$.
}
For an $s-t$ path $P$ in one
of the graphs $G_i$, we define the length of $P$ to be simply the number of
edges in $P$, and we denote this to be $\ell(P)$.
Formally, we
define $\operatorname{changes}(P_{0}, P_{1}, \ldots, P_{b})$ to be the number of indices $i \ (0 \leq i \leq b - 1)$
for which $P_i \neq P_{i+1}$.
Fix a constant $K>0 .$ We define the cost of the sequence of paths
$P_{0}, P_{1}, \ldots, P_{b}$
\[
	\operatorname{cost}\left(P_{0}, P_{1}, \ldots, P_{b}\right)=
	\sum_{i=0}^{b} \ell\left(P_{i}\right)+K \cdot \operatorname{changes}\left(P_{0}, P_{1}, \ldots, P_{b}\right)
\]
\subsubsection{
	Set up the recursive formula and justify its correctness.
}

We define $\mpath(i)$ to be the recurrence for the a sequence of paths of minimum cost
in $G_0, \cdots, G_i$. Let the set of edges that exist from graphs $G_a, \cdots, G_b$
be defined as
$$E_{a,b} = \bigcap_{i=a}^{b} E_i$$
Let $\ell_E(E_{a,b})$ be the length of the shortest $s-t$ path in $E_{a,b}$. If a path does not exist
we define $\ell_E(E_{a,b}) = \infty$. \\
The main idea behind MinPath is that there are two major cases for the minimum sequence of paths.
The first major case is if there exists a single path for all graphs from $G_0$ to $G_i$. 
Then, we just calculate the length of this single path and multiply it by the amount of graphs 
we are considering. Note that since there are no changes we do not have to worry about the cost 
of changing paths, i.e. the $K$ value in the cost function. The second major case is 
if we need to change paths. We look at all places where the sequence of paths changes to a path
that is contained in $G_i$. This path change happens between the graph $\phi$ and $\phi + 1$.
We recursively calculate the minimum cost of paths between $0$ and $\phi$ and compute 
the cost of the path from $\phi + 1$ to $i$, which includes the cost of changing paths, $K$. 
We also add in a special condition when we consider this second case which is $i > 0$. This is because 
if $i=0$, there would be no switching paths as there is only one graph to consider. We need at least 
2 graphs for switching paths to make sense.
We formally define MinPath as 
$$
	\mpath(i) = \min \Bigg(\ell_E\Big(E_{0,i}\Big) \cdot \Big(i+1\Big),
	\min\Big(\Big\{\mpath(\phi)	+ \ell_E\Big(E_{\phi+1,i}\Big) \cdot \Big(i - \phi \Big)+ K
	 \mid (0 \leq \phi < i) \land (i > 0) \Big\}\Big) \Bigg)
$$
We assume that the min function for an empty set will return an empty set. Additionally,
if one of parameters of the min function is an empty set, it will return the other parameter. 
%We have a special case for $\mpath$ when $i=0$. It is trivial to see that when there is only one graph,
%the minimum cost will be the length of the shortest path of graph $G_0$.   
%$$
%	\mpath(0) = \ell_E\Big(E_{0,0}\Big)
%$$

\begin{proof}
	We show by strong induction that our recurrence relation is correct. Let $P(i)$ be the predicate,
	"MinPath correctly computes the sequence of paths of minimum cost in $G_0, \cdots, G_i$".
	We define $i \in \mathbb{N}$. \\
	\textbf{Base Case:} When $i=0$, it is trivial to see that the correct minimum cost is
	$\ell_E(E_{0,0})$ as there is only one graph to consider. We see that value in the left
	parameter of the min function evaluates to $\ell_E\big(E_{0,0}\big) \cdot \big(0+1\big) = \ell_E(E_{0,0})$.
	The right parameter never evaluates. Thus, the min of these two values is $\ell_E(E_{0,0})$,
	which is the correct minimum cost. Therefore, $P(0)$ holds.\\
	\textbf{Inductive step:} Suppose $P(i)$ holds for all $0 \leq i \leq k$. We show that $P(k + 1)$ holds.
	We have two major cases. The first case is if there exists a single minimum path from graphs
	$G_0, \cdots, G_{k+1}$.  Then the cost of this sequence will be $\sum_{i=0}^{k+1} \ell_E(E_{0,k+1}) =
		\ell_E(E_{0,k+1})((k+1)+1)$, which is exactly what is in the first parameter of our min function.
	The second case is if the minimum path at $G_{k+1}$ is not the same as all the paths before it.
	We check by exhaustion where the optimal change is, where $\phi$ is the index of the last graph
	where its minimum path is different than the minimum path at graph $k+1$. The cost of the entire path
	would be the cost of the sequence of minimum paths from graph $0$ to graph $\phi$,
	which would be $\mpath(\phi)$,
	plus the length of the minimum path in graph $k+1$ times the amount of graphs this path
	shows up in plus some constant $K$. If no such path exists from $\phi + 1$ to $k + 1$, then
	$\ell_E(E_{\phi+1,k+1}) = \infty$ as defined. Since $\phi \leq k$, by our strong
	inductive hypothesis $\mpath(\phi)$ is correct. We do this for all values of $\phi$ 
	and pick the smallest cost. We choose the smaller of both cases and
	return the minimum cost. \\
	By strong induction, we have proven that $P(i)$ holds for all $i \in \mathbb{N}$.
	Therefore, MinPath is correct.
\end{proof}

\subsubsection{
	Write the pseudocode for the iterative version of the algorithm to find the minimum
	cost. You are not required to write pseudocode to find the shortest path.
}
\begin{algorithm}
	\KwData{A list $G$, containing graphs $G_0, \cdots, G_b$}
	\KwResult{The minimum cost of the sequence of paths in $G_0, \cdots, G_b$}
	\Fn{minCost($G$)}{
		dp $\leftarrow$ a list contain the minimum cost for the sequence of paths from $G_0, \cdots, G_i$\\
		\For{$i$ from $0$ to $b$}{
			dp$[i] = $ \\$\min \Big( \ell_E(E_{0,i}) \cdot (i+1),
			\min (\{ \text{dp}[\phi]+ \ell_E (E_{\phi+1,i}) \cdot (i - \phi )+ K \mid 
			(0 \leq \phi < i) \land (i > 0) \})\Big)$
		}
		\Return{\text{\normalfont dp[b]}}
	}
\end{algorithm}



\subsubsection{
	Analyze the computing complexity.
}
$O(b^2L_e)$
\begin{proof}
	Let the number of nodes in $G$ be $n$.
	Computing $\ell_E(E_{i,j})$ takes $n$ time as we can use breath first search.  
\end{proof}



%problem 2
\section{
  Given a	rooted tree $T = (V, E)$ and an integer $k$, find the largest possible
  number of disjoint paths in $T$, where each path has length $k$.
 }
\subsection{
	Set up the recursive formula and justify its correctness.
}
We define $\maxp(v)$ to be the recurrence for the maximum number of disjoint paths of size $k$ in a sub tree
of $T$ with root $v$. We consider two major cases, the maximum number of disjoint paths may or
may not contain $v$.

$$
	\maxp(v) = \max \Big(\dctn(v), \ctn(v, 0) \Big)
$$
In the case where maximum number of disjoint paths does not contain $v$, we define
$\dctn(v)$ to be the sum of
the maximum number of disjoint paths of size $k$ for each sub tree generated by the children of $v$,
i.e. the sub trees having $c$ as the root where $(v,c) \in E$.
We will define the set of child nodes of a vertex $v$ as $$C_v = \Big\{c \mid (v,c) \in E\Big\}$$
Adding up all the maximum paths for each sub tree
gives us the total amount of maximum paths for the entire tree.

$$
	\dctn(v) = \sum_{c \in C_v } \maxp(c)
$$
In the case where the maximum number of disjoint paths contains $r$, there are a couple of subcases.
We define $\ctn(v,\delta)$ to be the maximum number of disjoint paths of size $k$
in a sub tree of $T$ with root $v$, given that
$v$ is currently a part of a path of size $\delta$, where $\delta$ is bounded by $0 \leq \delta \leq k$.
The first subcase is if $v$ is a leaf node and the current size of the path it's on is less than $k$,
i.e $\delta < k$, there would be $0$ disjoint paths of size $k$ in this sub tree.
The second subcase is when $v$ is the last node in a path of size $k$, i.e. $\delta = k$. Then, the
maximum paths will be the sum of the maximum paths on the sub trees generated by the children of
$v$, or equivalently $\dctn(v)$, plus the path that $v$ is on.
The third subcase covers when $v$ is on some path that is not complete nor trivially ends on $v$.
The maximum number of disjoint paths in the sub tree of root $v$ currently a part of a path of size $\delta$,
will be the maximum number of disjoint paths of some sub tree $c \in C_v$ which would be
part of a path of size $\delta + 1$, plus the sum of the maximum number of disjoint paths for
the rest of the children of $v$. We try out all combinations of $c$ and pick the maximal one.

$$
	\ctn(v,\delta) =
	\begin{cases}
		0            & \textbf{when } \delta < k \textbf{ and } v \text{ is a leaf} \\
		\dctn(v) + 1 & \textbf{when } \delta = k                                    \\
		\max\bigg(\Big\{
		\ctn(c,\delta + 1)                                                          \\
		\qquad \qquad \qquad
		+ \displaystyle\sum_{c' \in C_v \setminus \{c\}} \maxp(c') \mid c \in C_v
		\Big\}\bigg) & \textbf{otherwise}                                           \\
	\end{cases}
$$

\begin{proof}
	We show by strong induction that our recurrence relation is correct. Let $P(n,k)$ be the predicate,
	"MaxPath correctly computes the maximum number of disjoint paths of size $k$ in a sub tree
	of $T$ which has $n$ number of nodes and where $v$ is the root node".
	We assume that the size of a path must be at least 1, as a path of 0 would lead to infinite amount of
	paths, and that a sub tree must contain at least one node, as our sub tree has a root.
	Hence, we define $n,k \in \mathbb{N}^+$. \\
	\textbf{Base Case:} When $n = 1, k = 1$, MaxPath returns 0. $\ctn(v,0)$ returns 0 because
	$0 < 1$ and $v$ is a leaf. $\dctn(v)$ returns 0 because $v$ has no children. The max of $\{0,0\}$
	equals $0$. It is clear to see that there are no paths for a tree that consists of a single node.
	Hence, $P(1,1)$ holds.\\
	\textbf{Inductive step for number of nodes:} Suppose $P(\eta,\kappa)$ holds for all $n \leq \eta$ and $k \leq \kappa$.
	We show that $P(\eta + 1,\kappa)$ holds. MaxPath(v) calls $\ctn(v,0)$ and $\dctn(v)$. Since $v$
	is not a leaf and $0 \neq k$, third condition holds in $\ctn$.
	Notice that the number of nodes the sub tree where the root is $c$ will have strictly less nodes than $n + 1$,
	which is amount of nodes in the sub tree where the root is $v$ because $c$ is a child of $v$.
	%Given the previous statement and $\kappa-1 \leq k$, 
	Hence, $\ctn(c, 1)$ holds by our inductive hypothesis.
	Without loss of generality, $\maxp(c')$ and $\dctn(v)$ holds by our inductive hypothesis.
	Therefore, $P(\eta + 1,\kappa)$ holds.\\
	%\ctn is true by ih , tree of c is less
	%\dctn is true by ih 
	\textbf{Inductive step for length of path:} Suppose $P(\eta,\kappa)$ holds for all $n \leq \eta$ and
	$k \leq \kappa$. We show that $P(\eta,\kappa + 1)$ holds. MaxPath(v) calls $\ctn(v,0)$ and $\dctn(v)$.
	We continue with the third condition in $\ctn(v,0)$.
	Notice that $\ctn(c, 1)$ is equivalent to $\ctn(c,0)$ if we fix $k = \kappa$.
	Since $\kappa \leq \kappa + 1$, $\ctn(c,0)$ where $k = \kappa$ and subsequently
	$\ctn(c, 1)$ where $k = \kappa + 1$ holds by our inductive hypothesis.
	Without loss of generality, $\maxp(c')$ and $\dctn(v)$ holds by our inductive hypothesis
	because they will eventually reach $\ctn(v', 1)$. Therefore, $P(\eta,\kappa+ 1)$ holds.\\
	By strong induction, we have proven that $P(n,k)$ holds for all $n,k \in \mathbb{N}^+$.
	Therefore, MaxPath is correct.
\end{proof}




\subsection{
	Write the pseudocode for the iterative version of the algorithm to find the maximum
	number of players that can play at the same time.
}
The main idea of this algorithm is to compute bottom up the optimal cost to view event $n$
when we start on some event $i$. We do this by creating a length $n$ list which the $i$th index represents the
most amount of events you can view, given that you go straight from the starting coordinate, 0,
to event $i$, and visit the most maximum amount of events from $i$ to $n$.
We will assume that $|\text{coordinate of n}| \leq n$ and are using zero based numbering for lists.

\begin{algorithm}
	\KwData{$T = (V, E)$, a tree with root $r$; $k$, length of path}
	\KwResult{Maximum number of disjoint paths of size $k$ in the tree $T$}
	\Fn{max($T, k$)}{
		contains $\leftarrow$ a map\\
		notContains $\leftarrow$ a map\\
		maxPaths $\leftarrow$ a map\\
		\For{$v \in V$}{
			\#base case \\
			\If{$v$ is a leaf}{
				maxPaths.insert(v, 0)\\
				notContains.insert(v, 0)\\
				\For{i from 0 to k - 1}{
					contains.insert(<v,i>, 0)
				}
				contains.insert(<v,k>, 1)\\
			}
			\Else{
				notContainVal = 0\\
				\For{each child $c$ of $v$}{
					notContainVal+=maxPaths.get(c)
				}
				notContains.insert(v,notContainVal)\\
				\For{i from 0 to k-1}{
					containsVal $\leftarrow$ a empty list\\
					\For{each child $c$ of $v$}{
						\#Where $c' \in C_v \setminus\{c\}$\\
						val = contains.get(c,i+1) + $\sum$ maxPaths.get(c')\\
						containsVal.insert(val)
					}
					contains.insert(<v,i>, max(containsVal))
				}
				contains.insert(<v,k>, 1+ notContains.get(v))\\
				maxPaths.insert(v, max(contains.get(<v,0>),notContains.get(v)))
			}
		}
		\Return{maxPaths.get(r)}
	}
\end{algorithm}



\subsubsection{
	Analyze the computing complexity.
}


\iffalse
	The main idea of this algorithm is to compute bottom up the optimal cost to view event $n$
	when we start on some event $i$. We do this by creating a length $n$ list which the $i$th index represents the
	most amount of events you can view, given that you go straight from the starting coordinate, 0,
	to event $i$, and visit the most maximum amount of events from $i$ to $n$.
	We will assume that $|\text{coordinate of n}| \leq n$ and are using zero based numbering for lists.

	\begin{algorithm}
		\KwData{crd, a list of coordinates that corresponds to events.}
		\KwResult{Maximal amount of events we can view, given that we must view the last event.}
		\Fn{optimal(crd)}{
			\#dp holds the maximum amount of events you can view given you visit crd[i]\\
			dp $\leftarrow$ a list of 0's with the same length of crd\\
			dp[n-1] = 1 \\
			\For{\text{int i = length of event - 2; i $\geq$ 0; i--}}{
				possibleEvents $\leftarrow$ a empty list\\
				\For{int j = length of event - 1; i < j; j--}{
					\If{abs(crd[j] - crd[i]) $\leq$ j - i \textbf{and} i + 1 - abs(crd[i]) $\geq$ 0}{
						add crd[j] to possibleEvents
					}
				}
				\If{possibleEvents is not empty} {
					dp[i] = 1 + max element of possibleEvents
				}
			}
			\Return max element of dp
		}
	\end{algorithm}

	\subsection{
		Show the algorithm for tracing the events selected.
	}
	This algorithm looks through the list, dp, generated in \textit{optimal} and returns the
	right most value for each number starting from the max to 1. This will provide which
	events to view such that the number of views is maximized.
	\begin{algorithm}
		\KwData{dp, the list used for memoization\\
			\qquad \enspace \ crd, a list of coordinates that corresponds to events \\
			\qquad \enspace \ max, the maximal amount of events we can view }
		\KwResult{List coordiantes of events to view.}
		\Fn{events(dp, crd, max)}{
			view $\leftarrow$ a empty list\\
			\For{each num in dp}{
				find the right most max and add its index to view\\
				decrement max
			}
			\Return{view}
		}
	\end{algorithm}\-\\

	%\subsubsection{Partial correctness}
	%\begin{proof}
	%	Suppose fraudMajority halts. 
	%\end{proof}
	%\subsubsection{Termination}


	\subsection{
		Give a brief argument of correctness.
	}

	\[
		\text{MaxEvent}(i)=
		\begin{cases}
			1 & \textbf{when } i = n - 1                                                                           \\
			1 + \text{max}(\text{MaxEvent}[j_1], \text{MaxEvent}[j_2], \cdots )
			  & \textbf{if } \exists j > i \text{ s.t.} \mid \text{MaxEvent}[i]-\text{MaxEvent}[j] \mid \leq i - j \\
			  & \text{and } i + 1 - \mid \text{MaxEvent}[i] \mid  \geq 0                                           \\
			0 & \textbf{otherwise}
		\end{cases}
	\]

	Note that MaxEvent$[n-1]$ corresponds to the $n$th event because of our zero based numbering for lists. We compute
	the recurrence relation descending, i.e. from $i = n-1$ to $i=0$.
	\begin{theorem*}
		The recurrence relation, MaxEvent$(i)$ is most amount of events you can visit given that the first event you visit is at $i$
		and you end on $n$th event.

	\end{theorem*}
	\begin{proof}
		We show by strong backwards induction that our recurrence relation is correct. Let $P(i)$ be the predicate,
		"MaxEvent correctly computes the most amount of events you can visit given that the first event you visit is at $i$
		and you end on $n$th event".
		We define $i\in \mathbb{N}$. \\
		\textbf{Base Case:} When $i = n - 1$, MaxEvent returns 1 which is the correct output because if you start
		at the $n$th event, the last event, you can only visit one event. Hence, $P(n-1)$ holds.\\
		\textbf{Inductive step:} Suppose $P(\alpha)$ holds for all $\alpha \leq n - 1$. We show that $P(\alpha-1)$ holds.
		Our recurrence relation looks for all $j > i = \alpha-1$ such that if you start at event $i$, you can visit event $j$.
		This part is given by, $$\exists j > i \text{ s.t. } \mid \text{MaxEvent}[i]-\text{MaxEvent}[j] \mid \leq i - j$$
		\par The second condition of the recurrence checks if it is possible to view event $i$, given you start at coordinate 0
		and can move 1 distance before the first event, i.e the second assumption given in the homework. This part is given by,
		$$ i + 1 - \mid \text{MaxEvent}[i] \mid  \geq 0$$
		\par If both conditions hold, the most amount of events you can visit is the event at $i$, which is 1 event, plus
		the biggest MaxEvent from the $j$'s. By our strong backwards inductive hypothesis,
		MaxEvent$(j)$ returns the correct amount of events because we defined
		$ j > i = \alpha - 1 \Leftrightarrow j \geq \alpha$.
		This will give us the correct amount of events you can visit in the case where we can visit $i$ and $j$.
		\par In all other cases, we cannot visit $i$ or $j$ which means the max amount of events we can visit is 0.
		\par By strong backwards induction, we have proven that $P(i)$ is true for all $i \in \mathbb{N}$.
		Therefore, MaxEvent is correct.
	\end{proof}
	\begin{corollary*}
		The algorithm, optimal, returns the maximal amount of events we can view, given that we must view the last event.

		\begin{proof}
			Our algorithm evaluates the recurrence MaxEvent(i) bottom-up by storing MaxEvent(i) in dp[i].
			All values for subproblems referenced by the recurrence for MaxEvent(i) will have already been computed
			because we evaluate dp[i] as i decreases from n-1 to 0.
			After all the values in dp are computed, the algorithm returns the max element,
			which corresponds to the most amount of events you can visit somewhere in the list crd,
			which in turn is the most amount of events you can visit in the entire list crd.
		\end{proof}

	\end{corollary*}
\fi


\iffalse
	\begin{corollary*}
		The algorithm, events, correctly traces the events selected.
		\begin{proof}
			$$\textbf{DO THIS PART }$$
		\end{proof}
	\end{corollary*}

\fi

\iffalse
	\subsection{
		Analyze the running time.
	}
	We claim that the running time of \textit{optimal} is $O(n^2)$.
	\begin{proof}
		We have a nested for loop, which run at most $n$ times each.
		In the outer for loop, we search for the max element of an array
		which at most is size $n$. The rest of the computations in the outer for loop take constant time.
		Additionaly, after all the loops have been run, we search for the max element of dp which is
		size $n$. This leads to a total time complexity of $n(2n) + n = 2n^2 + n$.
		Thus, our total time complexity of our algorithm is $O(n^2)$
	\end{proof}
	We claim that the running time of \textit{events} is $O(n)$.
	\begin{proof}
		We have a for loop, which run at most $n$ times each.
		In the for loop, we check if the current value is the current max element of an array takes constant time.
		Thus, our total time complexity of our algorithm is $O(n)$
	\end{proof}
	%problem 2
	\section{Design an $O(n^3)$ algorithm using dynamic programming methodology to find an optimal
	  (least total testing cost) assembly order.}

	\subsection{
		Use iterative implementation for the algorithm to find the optimal cost.
	}
	We define $\alpha$ and $\omega$ to each be some piece between $[1,n]$ parts. Each piece consists of an interval $[a,b]$.
	The main idea of this algorithm is to compute bottom up the optimal cost. We do this by creating a
	$nxn$ table which represents the optimal cost of combining $[\alpha,\omega]$. We know that it costs
	nothing to combine the same piece, i.e. when $\alpha = \omega$ so we initialize those values to be zero.
	Then, we start computing the optimal cost of each possible interval in the linear structure. We know that intervals
	that consist of 2 or more parts can be broken down two sections, from $[\alpha, \phi]$ and $[\phi+1,\omega]$. We search through our table given
	all possible intervals and pick the smallest---most optimal---sum. Finally, we add the cost of assembling
	from $[\alpha,\omega]$ and add it to our sum. This will give us the most optimal cost of combining
	$[\alpha,\omega]$, and we save this value to our table so that subsequent subproblems will be able to
	use this value.

	%ss\begin{algorithm}
	\KwData{pieces, a size $n$ list containing ordered pairs representing the interval of parts, i.e. $[a,b]$ where some part
		begins at $a$ and ends at $b$.
		%Total length of structure is $\omega$
	}
	\KwResult{Optimal cost to assemble the linear structure.}
	\Fn{assemble(pieces)}{
		dp $\leftarrow$ a $nxn$ matrix used for memoization, initialized with 0's \\
		\For{$\alpha$ decreasing from n to 1}{
			\For{$\omega$ increasing from $\alpha + 1$ to n }{
				$$\text{dp}[\alpha, \omega] = f(\text{pieces}[\alpha][a], \text{pieces}[\omega][b]) +
					\min\Big(\Big\{\text{dp}[\alpha, \phi] + \text{dp}[\phi + 1, \omega] \mid \alpha \leq \phi < \omega\Big\} \Big)$$
			}
		}

		\Return dp$[1, n]$
	}
	%\end{algorithm}\-\\



	\subsection{
		Show the algorithm for finding the optimal order.
	}
	This algorithm performs a depth traversal to find the optimal order.
	We begin at dp$[\alpha, \omega]$. If the interval of this part $\alpha, \omega$
	is not 1, i.e. contains 2 or more pieces we subtract calculate the next optimal value to search for
	which would be dp$[\alpha, \omega] - f([\alpha][a],[\omega][b])$. We find the index of this value and call
	the algorithm using the index as the new parameter. Once we hit the bottom of this recursion,
	we print out the first two parts we want to assemble, then the next two parts, etc. until we finally
	have two parts remaining and we assemble the last two parts.
	\begin{algorithm}
		\KwData{dp, matrix used for memoization from \textit{assemble}\\
			\qquad \quad pieces
		}
		\KwResult{The optimal order to assemble the linear structure.}
		\Fn{optOrder($\alpha, \omega$)}{
			min $\leftarrow$ dp$[\alpha, \omega]$\\
			\If{$\omega - \alpha == 0$}{
				output nothing
			}
			\Else{
				nextMin $\leftarrow$ min - $f([\alpha][a],[\omega][b])$\\
				$\alpha', \omega' \leftarrow$ search for nextMin in the column and row that min belongs, find its index \\
				optOrder($\alpha', \omega'$)\\
				output $\Big[\text{pieces}[\alpha][a], \text{pieces}[\omega][b]\Big]$
			}
		}
	\end{algorithm}

	Call \textit{optOrder(1,n)}


	\subsection{
		Give a brief argument of correctness.
	}


	$a_\alpha$ represents where the piece $\alpha$ begins.
	Likewise, $b_\omega$ represents where the piece $\omega$ ends.
	\[
		\text{MinCost}(\alpha,\omega)=
		\begin{cases}
			f(a_\alpha,b_\omega) + \min\Big(
			\Big\{\text{MinCost}(\alpha, \phi) + \text{MinCost}(\phi + 1, \omega) \mid \alpha \leq \phi < \omega\Big\} \Big)
			  & \textbf{if } \omega > \alpha \\
			0 & \textbf{otherwise}
		\end{cases}
	\]

	\begin{theorem*}
		The recurrence relation, MinCost$(\alpha,\omega)$ is the minimum cost to assemble the linear structure from $\alpha$ to $\omega$.
		\begin{proof}
			Let $\psi = \omega - \alpha$.
			We show by strong induction that our recurrence relation is correct. Let $P(\psi)$ be the predicate,
			"MinCost correctly computes the minimum cost to assemble the linear structure that has $\psi$ pieces".
			We define $\psi \in \mathbb{N}$. \\
			\textbf{Base Case:} When $\psi = 0$, MinCost correctly returns 0 because there is
			nothing to join when you have no elements. Hence, $P(0)$ holds.\\
			\textbf{Inductive step:} Suppose $P(\psi)$ holds for all $0 \leq \psi \leq k$. We show that $P(k + 1)$ holds.
			For all $\phi$ where $\alpha \leq \phi \leq \omega$, we know that $\phi - \alpha \leq k$ and $\omega - (\phi + 1) \leq k$.
			It follows by our strong inductive hypothesis that all values contained in the set inside the min function will be correct.
			The min function then returns the minimum cost to assemble two adjacent pieces linear structure of combined size $k + 1$.
			This value is then added to the cost needed to merge remaining these two pieces, $f(a_\alpha,b_\omega)$.
			\par By strong induction, we have proven that $P(\psi)$ is true for all $\psi  \in \mathbb{N}$.
			Therefore, MinCost is correct.
		\end{proof}
	\end{theorem*}

	\begin{corollary*}
		The algorithm, \textit{assemble}, correctly returns the optimal cost to assemble the linear structure.

		\begin{proof}
			Our algorithm evaluates the recurrence MinCost bottom-up by storing MinCost$(\alpha,\omega)$ in dp[$\alpha,\omega$].
			All values for subproblems referenced by the recurrence for MinCost$(\alpha,\omega)$ will have already been computed
			because we evaluate dp[$\alpha,\omega$] as $\alpha$ decreases from $n$ to $1$ and $\omega$ increasing from $\alpha + 1$ to $n$.
			After all the values in dp are computed, the algorithm returns dp$[1, n]$,
			which corresponds to the the minimum cost to assemble the linear structure from $1$ to $n$, i.e. the entire linear
			structure.
		\end{proof}

	\end{corollary*}

	\iffalse
		\begin{corollary*}
			The algorithm, optOrder, correctly traces the events selected.
			\begin{proof}
				$$\textbf{DO THIS PART }$$
			\end{proof}
		\end{corollary*}
	\fi

	\subsection{
		Analyze the running time.
	}
	We claim that the running time of \textit{assemble} is $O(n^3)$.
	We assume computing $f(a,b)$ takes constant time.

	\begin{proof}
		We have a nested for loop, which run at most $n$ times each.
		In the inner for loop, we calculate dp$[\alpha,\omega]$ which requires computing the minimum of at most $n-1$ values
		in dp. The rest of the computations take constant time.
		Thus, our total time complexity of our algorithm is $O(n^3)$
	\end{proof}\-\\
	We claim that the running time of \textit{optOrder} is $O(n^2)$. We assume computing $f(a,b)$ takes constant time.
	\begin{proof}
		We have a recursive call which makes at most 1 recursive call. These recursive call
		ends once the size for the subproblem becomes 1, i.e. when $\omega$ and $\alpha$ are the same
		value. Since the new $\omega' - \alpha'$ generated are strictly decreasing, these recursive calls happen at most $n$ times.
		Inside the recursive call, we search a row and column which takes $2n$ time. The rest of the computations take constant time.
		Thus, our total time complexity of our algorithm is $n*2n = O(n^2)$
	\end{proof}
\fi
\end{document}

