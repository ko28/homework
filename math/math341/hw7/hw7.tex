\documentclass[11pt]{scrartcl}
\usepackage[sexy]{../../evan}
\usepackage{cmbright}
\usepackage{cancel}
\usepackage[T1]{fontenc}
%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
%\usepackage[pdfborder={0 0 0},colorlinks=true,citecolor=red{}]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{systeme}

\makeatletter
\newenvironment{Dequation}
  {%
  \def\tagform@##1{%
    \maketag@@@{\makebox[0pt][r]{(\ignorespaces##1\unskip\@@italiccorr)}}}%
  \ignorespaces
  }
  {%
  \def\tagform@##1{\maketag@@@{(\ignorespaces##1\unskip\@@italiccorr)}}%
  \ignorespacesafterend
  }
\makeatother

\title{Math 341: Homework 7}
\author{Daniel Ko}
\date{Spring 2020}

\begin{document}
\maketitle

\section{A}
For each of the following homogeneous systems of linear equations, find the dimension of and a basis for the
solution space.

\begin{enumerate}[label=\alph*.]
	\item{
		\systeme{
			x_1 + 3x_2 = 0,
			2x_1 + 6x_2 = 0
		}
		\begin{proof}
			Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
			\[
				A=
			\begin{bmatrix}
				1 & 3\\
				2 & 6\\
			\end{bmatrix}\\
			\]
			It is clear that rank(A) = 1 because the two columns are a multiples of each other. 
			If K is	the solution set of this system, then dim(K) = 2 - 1 = 1. 
			Thus any nonzero solution constitutes a basis for K. For example, since
			$
			\begin{pmatrix}
				-3\\
				1\\
			\end{pmatrix}
			$
			is a solution to the given system,
			$
			\begin{Bmatrix}
				\begin{pmatrix}
					-3\\
					1\\
				\end{pmatrix}
			\end{Bmatrix}
			$ 
			is a basis for K by Corollary 2 of Theorem 1.10.
		\end{proof}
	}

	\item{
		\systeme{
			x_1 + 2x_2 - x_3 = 0,
			2x_1 + x_2 + x_3 = 0
		}
		\begin{proof}
			Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
			\[
				A=
			\begin{bmatrix}
				1 & 2 & -1\\
				2 & 1 & 1\\
			\end{bmatrix}\\
			\]
			It is clear that rank(A) = 2 because there are two linearly independent rows (rank of a transposed matrix is the same as the rank of a matrix).
			If K is	the solution set of this system, then dim(K) = 3 - 2 = 1. 
			Thus any nonzero solution constitutes a basis for K. For example, since
			$
			\begin{pmatrix}
				1\\
				-1\\
				-1\\
			\end{pmatrix}
			$
			is a solution to the given system,
			$
			\begin{Bmatrix}
				\begin{pmatrix}
					1\\
					-1\\
					-1\\
				\end{pmatrix}
			\end{Bmatrix}
			$ 
			is a basis for K by Corollary 2 of Theorem 1.10.
		\end{proof}
	}
	
	\item{
		\systeme{
			x_1 + 2x_2 - 3x_3 + x_4 = 0
		}
		\begin{proof}
			Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
			\[
				A=
			\begin{bmatrix}
				1 & 2 & -1 & 1\\
			\end{bmatrix}
			\]
			It is clear that rank(A) = 1 because there are one linearly independent row (rank of a transposed matrix is the same as the rank of a matrix).
			If K is	the solution set of this system, then dim(K) = 4 - 1 = 3. 
			Note that, 
			$
			\begin{pmatrix}
				1\\
				1\\
				1\\
				0\\
			\end{pmatrix},
			\begin{pmatrix}
				0\\
				0\\
				1\\
				3\\
			\end{pmatrix},
			\begin{pmatrix}
				1\\
				0\\
				0\\
				-1\\
			\end{pmatrix}
			$
			are linearly independent vectors in K. Thus they form a basis by Corollary 2 of Theorem 1.10.
		\end{proof}
	}
	\item{
		\systeme{
		x_1 + 2x_2 + x_3 + x_4 = 0,
		x_2 - x_3 + x_4 = 0
		}	
		\begin{proof}
			Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
			\[
				A=
			\begin{bmatrix}
				1 & 2 & 1 & 1\\
				0 & 1 & -1 & 1\\
			\end{bmatrix}
			\]
			It is clear that rank(A) = 2 because there are 2 linearly independent rows (rank of a transposed matrix is the same as the rank of a matrix).
			If K is	the solution set of this system, then dim(K) = 4 - 2 = 2. 
			Note that, 
			$
			\begin{pmatrix}
				2\\
				0\\
				-1\\
				-1\\
			\end{pmatrix},
			\begin{pmatrix}
				-4\\
				2\\
				1\\
				-1\\
			\end{pmatrix}
			$
			are linearly independent vectors in K. Thus they form a basis by Corollary 2 of Theorem 1.10.
		\end{proof}
	}
	\item{
		\systeme{
			x_1 + x_2 - x_3 = 0,
			4x_1 + x_2 - 2x_3 = 0
			}	
		\begin{proof}
			Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
			\[
				A=
			\begin{bmatrix}
				1 & 1 & -1\\
				4 & 1 & -2\\
			\end{bmatrix}
			\]
			It is clear that rank(A) = 2 because there are 2 linearly independent rows (rank of a transposed matrix is the same as the rank of a matrix).
			If K is	the solution set of this system, then dim(K) = 3 - 2 = 1. 
			Thus any nonzero solution constitutes a basis for K. For example, since
			$
			\begin{pmatrix}
				1\\
				2\\
				3\\
			\end{pmatrix}
			$
			is a solution for K, it forms a basis by Corollary 2 of Theorem 1.10.
		\end{proof}
		}
	\item{
		\systeme{
			2x_1 + x_2 - x_3 = 0,
			x_1 - x_2 + x_3 = 0,
			x_1 + 2x_2 - 2x_3 = 0
		}
		\begin{proof}
			Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
			\[
				A=
			\begin{bmatrix}
				2 & 1 & -1\\
				1 & -1 & 1\\
				1 & 2 & -2\\
			\end{bmatrix}
			\]
			It is clear that rank(A) = 2 because there are 2 linearly independent columns (third column is second column multiplied by -1).
			If K is	the solution set of this system, then dim(K) = 3 - 2 = 1. 
			Thus any nonzero solution constitutes a basis for K. For example, since
			$
			\begin{pmatrix}
				0\\
				1\\
				1\\
			\end{pmatrix}
			$
			is a solution for K, it forms a basis by Corollary 2 of Theorem 1.10.
		\end{proof}
	}
	\item{
	\systeme{
		x_1 + 2x_2 = 0,
		x_1 - x_2 = 0
		}
	\begin{proof}
		Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
		\[
			A=
		\begin{bmatrix}
			1 & 2\\
			1 & -1\\
		\end{bmatrix}
		\]
		It is clear that rank(A) = 2 because there are 2 linearly independent columns.
		If K is	the solution set of this system, then dim(K) = 2 - 2 = 0. 
		This means the zero vector is the basis for K, i.e.
		$
		\begin{pmatrix}
			0\\
			0\\
		\end{pmatrix}
		$.
	\end{proof}
	}
\end{enumerate}

\section{B}
Using the results of Exercise 2, find all solutions to the following systems.
\begin{enumerate}[label=\alph*.]

	\item{
		\systeme{
			x_1 + 3x_2 = 5,
			2x_1 + 6x_2 = 10
		}
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				2\\
				1\\
			\end{pmatrix}
			$.
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					2\\
					1\\
				\end{pmatrix}
				+ 
				t\begin{pmatrix}
					-3\\
					1\\
				\end{pmatrix}: t \in \mathbb{R}
			\right\}
			\]
		\end{proof}
	}
	
	\item{
		\systeme{
			x_1 + 2x_2 - x_3 = 3,
			2x_1 + x_2 + x_3 = 6
		}
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				1\\
				2\\
				2\\
			\end{pmatrix}
			$.
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					1\\
					2\\
					2\\
				\end{pmatrix}
				+ 
				t\begin{pmatrix}
					1\\
					-1\\
					-1\\
				\end{pmatrix}: t \in \mathbb{R}
			\right\}
			\]
		\end{proof}
	}
	\item{
		\systeme{
			x_1 + 2x_2 - 3x_3 + x_4 = 1
		}
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				1\\
				0\\
				0\\
				0\\
			\end{pmatrix}
			$.	
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					1\\
					0\\
					0\\
					0\\
				\end{pmatrix} +
				t_1\begin{pmatrix}
					1\\
					1\\
					1\\
					0\\
				\end{pmatrix}+
				t_2\begin{pmatrix}
					0\\
					0\\
					1\\
					3\\
				\end{pmatrix}+
				t_3\begin{pmatrix}
					1\\
					0\\
					0\\
					-1\\
				\end{pmatrix}: t_1, t_2, t_3 \in \mathbb{R}
			\right\}
			\]
		\end{proof}
		}

	\item{ 
		\systeme{
			x_1 + 2x_2 + x_3 + x_4 = 1,
			x_2 - x_3 + x_4 = 1	
		}	
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				0\\
				0\\
				0\\
				1\\
			\end{pmatrix}
			$.	
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					0\\
					0\\
					0\\
					1\\
				\end{pmatrix} +
				t_1\begin{pmatrix}
					2\\
					0\\
					-1\\
					-1\\
				\end{pmatrix}+
				t_2\begin{pmatrix}
					-4\\
					2\\
					1\\
					-1\\
				\end{pmatrix}: t_1, t_2 \in \mathbb{R}
			\right\}
			\]
		\end{proof}
	}
	\item{
		\systeme{
			x_1 + x_2 - x_3 = 1,
			4x_1 + x_2 - 2x_3 = 3
			}	
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				1\\
				1\\
				1\\
			\end{pmatrix}
			$.	
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					1\\
					1\\
					1\\
				\end{pmatrix} +
				t\begin{pmatrix}
					1\\
					2\\
					3\\
				\end{pmatrix}: t \in \mathbb{R}
			\right\}
			\]
		\end{proof}
	}
	\item{
		\systeme{
			2x_1 + x_2 - x_3 = 5,
			x_1 - x_2 + x_3 = 1,
			x_1 + 2x_2 - 2x_3 = 4
		}
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				2\\
				1\\
				0\\
			\end{pmatrix}
			$.	
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					2\\
					1\\
					0\\
				\end{pmatrix} +
				t\begin{pmatrix}
					0\\
					1\\
					1\\
				\end{pmatrix}: t \in \mathbb{R}
			\right\}
			\]
		\end{proof}
	}
	\item{
		\systeme{
			x_1 + 2x_2 = 5,
			x_1 - x_2 = -1
			}
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				1\\
				2\\
			\end{pmatrix}
			$.	
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					1\\
					2\\
				\end{pmatrix} +
				t\begin{pmatrix}
					0\\
					0\\
				\end{pmatrix}: t \in \mathbb{R}
			\right\} = 
			\left\{
				\begin{pmatrix}
					1\\
					2\\
				\end{pmatrix} 
			\right\}
			\]
		\end{proof}	
	}
\end{enumerate}

\section{C}
Prove that the system of linear equations $Ax = b$ has a solution if and only if $b \in R(L_A)$.
\begin{proof}
	Let $A$ be an $mxn$ matrix.
	\begin{enumerate}[i.]
		\item{
			$Ax = b$ has a solution $\Rightarrow b \in R(L_A)$.\\\-\\
			Let $s$ be a solution to $Ax = b$. This means $A_1s_1 + \cdots + A_ns_n = b$. 
			Notice that $b$ is a linear combination of the columns of $A$, 
			which is equivalent to $R(L_A)$ by the proof given in Theorem 3.5. 
			Therefore, $b \in R(L_A)$.
		}
		\item{
			$b \in R(L_A) \Rightarrow Ax = b$ has a solution.\\\-\\
			$b \in R(L_A)$ means that $b$ is a linear combination of the columns of $A$. 
			So, $b = A_1s_1 + \cdots + A_ns_n$. 
			This means there exists an $x$, composed of the $s_1, \cdots, s_n$ as seen in the previous equation, such that $Ax = b$.  
			Therfore, $Ax = b$ has a solution.
		}
	\end{enumerate}
\end{proof}

\section{D}
Prove or give a counterexample to the following statement: If the comatrix of a system of $m$ linear equations in
$n$ unknowns has rank $m$, then the system has a solution.
\begin{proof}
Let $A$ be a $mxn$ comatrix for the system $Ax=b$. By defintion, $L_A: \mathbb{F}^n \rightarrow \mathbb{F}^m$. 
Another way of saying that the system has a solution is to say that $b \in R(L_A)$, as we proved
in problem C. Given that the rank$(A)=m$, it follows that that dim$(R(L_A)) = m $ by Theorem 3.5. 
Since the range and the codomain of $L_A$ are the same, it means that $L_A$ is onto by definition. 
Thus, $\forall b \in R(L_A) [\exists s \in \mathbb{F}^n \text{ s.t. } As = b]$.
Therefore, if the comatrix of a system of $m$ linear equations in
$n$ unknowns has rank $m$, then system has a solution.
\end{proof}
	

\section{E}
Suppose that the augmented matrix of a system $Ax = b$ is transformed into a matrix $(A'|b')$ in reduced row
echelon form by a finite sequence of elementary row operations.
\begin{enumerate}[label=\alph*.]
	\item{
		Prove that rank($A'$) $\neq$ rank$(A'|b')$ if and only if $(A'|b')$ contains a row in which the only nonzero entry lies
		in the last column.
		\begin{proof}\
			\begin{enumerate}[i.]
				\item{
					rank($A'$) $\neq$ rank$(A'|b')$ $\Rightarrow$ $(A'|b')$ contains a row in which the only nonzero entry lies
					in the last column.\\\-\\
					If rank($A'$) $\neq$  rank$(A'|b')$, then we know that $b'$ is a linearly independent vector to $A'$.
					This must mean that $(A'|b')$ contains a row in which the only nonzero entry lies
					in the last column, which is $b'$.
				}
				\item{
					$(A'|b')$ contains a row in which the only nonzero entry lies
					in the last column $\Rightarrow$ rank($A'$) $\neq$ rank$(A'|b')$ \\\-\\
					$(A'|b')$ contains a row in which the only nonzero entry lies
					in the last column means that $b'$ is a linearly independent vector to $A'$. Then this means that 
					rank($A'$) $\neq$ rank$(A'|b')$.
				}
			\end{enumerate}
		\end{proof}
	}
	\item{
		Deduce that $Ax = b$ is consistent if and only if $(A'|b')$ contains no row in which the only nonzero entry lies
		in the last column.
		\begin{proof}\
			\begin{enumerate}[i.]
				\item{
					$Ax = b$ is consistent $\Rightarrow$  $(A'|b')$ contains no row in which the only nonzero entry lies
					in the last column.\\
					In problem C, we proved that 
					$Ax = b$ is consistent means that $b$ is a linear combination of the columns in $A$, i.e. linearly dependent. This also must mean that
					$b'$ is a linearly dependent of the columns in $A'$	because row reduction is a 
					rank and row space preserving operation. Thus, $(A'|b')$ contains no row in which the only nonzero entry lies in the last column
					because $b'$ is a linearly dependent.
				}
				\item{
					$(A'|b')$ contains no row in which the only nonzero entry lies
					in the last column $\Rightarrow$ $Ax = b$ is consistent.\\
					From part (a), we know that the above premise is equivalent to rank($A'$) $\neq$ rank$(A'|b')$. By Theorem 3.11, 
					we know that $A'x=b'$ must be consistent. $Ax = b$ must also be consistent because row reduced echelon form preserves rank and row space.
				}
			\end{enumerate}
		\end{proof}
	}
\end{enumerate}


\section{F}
Let the reduced row echelon form of $A$ be
\[ R = \begin{bmatrix}
	1 & 0 & 2 & 0 & -2 \\
	0 & 1 & -5 & 0 & -3 \\
	0 & 0 & 0 & 1 & 6
\end{bmatrix}\]
Determine $A$ if the first, second, and fourth columns of $A$ are
$$
\begin{bmatrix}
1 \\
-1 \\
3
\end{bmatrix}, \quad \begin{bmatrix}
0 \\
-1 \\
1
\end{bmatrix}, \text { and } \begin{bmatrix}
1 \\
-2 \\
0
\end{bmatrix}
$$
\begin{proof}
Notice that $r_{j_{3}} = 2r_{j_{1}} - 5r_{j_{2}}$ and $r_{j_{5}} = -2r_{j_{1}} - 3r_{j_{2}} + 6r_{j_{4}}$ where $r_{j_{i}}$ is the 
$i$th column of $R$. Using Theorem 3.16(c) and (d) we can compute $A$. 

\[ A = \begin{bmatrix}
	1  & 0  & 2  & 1  & 4\\
	-1 & -1 & 3 & -2 & -7 \\
	3  & 1  & 1  & 0  & -9
\end{bmatrix}\]

\end{proof}

\section{G}
Let the reduced row echelon form of $A$ be
\[ 
R = \begin{bmatrix}
1 & -3 & 0 & 4 & 0 & 5 \\
0 & 0 & 1 & 3 & 0 & 2 \\
0 & 0 & 0 & 0 & 1 & -1 \\
0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\]
Determine $A$ if the first, third, and sixth columns of $A$ are
$$
\begin{bmatrix}
	1 \\
	-2 \\
	-1 \\
	3
	\end{bmatrix}, \quad \begin{bmatrix}
	-1 \\
	1 \\
	2 \\
	-4
	\end{bmatrix}, \text { and } \begin{bmatrix}
	3 \\
	-9 \\
	2 \\
	5
\end{bmatrix}
$$
\begin{proof}
	Notice that 
	$r_{j_{2}} = -3r_{j_{1}}$,
	$r_{j_{4}} = 4r_{j_{1}} + 3r_{j_{3}}$, and 
	$r_{j_{5}} = 5r_{j_{1}} + 2r_{j_{3}} - r_{j_{6}}$
	where $r_{j_{i}}$ is the 
	$i$th column of $R$. Using Theorem 3.16(c) and (d) we can compute $A$. 
	\[ 
		A = \begin{bmatrix}
		1 & -3 & -1 & 1 & 0 & 3 \\
		-2 & 6 & 1 & -5 & 1 & -9\\
		-1 & 3 & 2 & 2 & -3 & 2\\
		3 & -9 & -4 & 0 & 2 & 5
		\end{bmatrix}
		\]
\end{proof}

\section{H} 
Let $W$ be the subspace of $M_{2 \times 2}(R)$ consisting of the symmetric $2 \times 2$ matrices. The set
$$
S=\left\{\left(\begin{array}{rr}
0 & -1 \\
-1 & 1
\end{array}\right),\left(\begin{array}{cc}
1 & 2 \\
2 & 3
\end{array}\right),\left(\begin{array}{cc}
2 & 1 \\
1 & 9
\end{array}\right),\left(\begin{array}{rr}
1 & -2 \\
-2 & 4
\end{array}\right),\left(\begin{array}{rr}
-1 & 2 \\
2 & -1
\end{array}\right)\right\}
$$
generates $W$. Find a subset of $S$ that is a basis for $W$.
\begin{proof}
We put $S$ in an augmented matrix.
\[
A=
\begin{bmatrix}
	0 & 1 & 2 & 1 & -1 \\
	-1 & 2 & 1 & -2 & 2 \\
	-1 & 2 & 1 & -2 & 2 \\
	1 & 3 & 9 & 4 & -1
\end{bmatrix}
\]
Then we compute reduced row echelon form of this matrix. 
\[
R=
\begin{bmatrix}
	1 & 0 & 3 & 0 & 4\\
	0 & 1 & 2 & 0 & 1\\
	0 & 0 & 0 & 1 & -2 \\
	0 & 0 & 0 & 0 & 0
\end{bmatrix}
\]
Notice that the first, second, and fourth columns of R is linearly independent. 
Then, the cooresponding columns in A would be the linearly independent subset by Theorem 3.16.
\[
S'=\left\{\left(\begin{array}{rr}
	0 & -1 \\
	-1 & 1
	\end{array}\right),\left(\begin{array}{cc}
	1 & 2 \\
	2 & 3
	\end{array}\right),\left(\begin{array}{rr}
	1 & -2 \\
	-2 & 4
	\end{array}\right)\right\}
\]
forms a basis for $W$.
\end{proof}

\section{I} 
Prove the corollary to Theorem 3.16: The reduced row echelon form of a matrix is unique.
\begin{proof}
	\begin{lemma}
		The pivot columns of matrix $A$'s reduced row echelon form is unique
		\begin{proof}
			We proceed by induction on $n \in \mathbb{N}$. Let $P(n)$ be the following predicate 
			"pivot columns of matrix $A$'s reduced row echelon form is unique, where matrix $A$ has $n$ columns".
			\begin{enumerate}[label=\alph*.]
				\item{
					Base case:\\
					We prove $P(1)$.
					There is only one column so it must be unique.
				}
				\item{
					Inductive step:\\
					Suppose $P(k)$ holds. 
					We show that P($k+1$) holds.\\
					Let $A = [A'|u_{k+1}]$ where $A'$ is a matrix which as $k$ columns and has unique pivot columns in its reduced row echelon form.
					Consider two cases: where $u_{k+1}$ is linearly dependent to $A'$ and where it is linearly independent to $A'$.\\\-\\
					Case 1: $u_{k+1}$ is linearly dependent to $A'$\\
					If $u_{k+1}$ is linearly dependent it means that rank$(A) = $ rank($A'$). That must mean that $A$ has a unique amount of pivot columns because $A'$
					has a unique amount of pivot columns by our induction hypothesis.\-\\
	
					Case 2: $u_{k+1}$ is linearly independent to $A'$\\
					If $u_{k+1}$ is linearly independent it means that rank($A$) = rank($A') + 1 = r$, where $r$ is just some fixed integer. 
				}
			\end{enumerate}
			Therefore, by induction we have proved this lemma.
			\end{proof}
	\end{lemma}

Let's assume for the sake of contradiction that matrix $A$ has two unique reduced row echelon forms, $B$ and $B'$.
Theorem 3.16(b) states that "For each $k = 1, 2, \cdots, n$, if column $k$ of $B$ is $d_1e_1 + d_2e_2 + \cdots + d_re_r $, 
then column $k$ of $A$ is $d_1a_{j_1} + d_2a_{j_2} + \cdots + d_ra_{j_r}$. Let's the $k$ column of $B$ be $d_1e_1 + d_2e_2 + \cdots + d_re_r$
and the $k$ column of $B'$ be $d'_1e_1 + d'_2e_2 + \cdots + d'_re_r$ where $kth$ column of $B$ does not equal the $kth$ column of $B'$ because they are both unique.
However, this would be that the $k$th column of the original matrix $A$ are different depending if you compute using $B$ or $B'$. Thus, this is a contradiction
and $A$ must have a unique reduced row echelon form. 
\end{proof}
\end{document}