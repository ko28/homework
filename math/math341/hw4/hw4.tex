\documentclass[11pt]{scrartcl}
\usepackage[sexy]{../../evan}
\usepackage{cmbright}
\usepackage{cancel}
\usepackage[T1]{fontenc}
%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
%\usepackage[pdfborder={0 0 0},colorlinks=true,citecolor=red{}]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{systeme}

\makeatletter
\newenvironment{Dequation}
  {%
  \def\tagform@##1{%
    \maketag@@@{\makebox[0pt][r]{(\ignorespaces##1\unskip\@@italiccorr)}}}%
  \ignorespaces
  }
  {%
  \def\tagform@##1{\maketag@@@{(\ignorespaces##1\unskip\@@italiccorr)}}%
  \ignorespacesafterend
  }
\makeatother

\title{Math 341: Homework 4}
\author{Daniel Ko}
\date{Spring 2020}

\begin{document}
\maketitle

\section{A}
Let V be a vector space having dimension n, and let S be a subset of V that generates V.

\begin{enumerate}[label=\alph*.]
	\item{
			Prove that there is a subset of S that is a basis for V. (Be careful not to assume that S is finite).
			\begin{proof}
			\-\\
			Since V is finite dimensional, there exists a basis for V.\\
			$B = \{v_1, v_2, \cdots, v_n\}$\\
			Any $v \in B$ can be expressed as a linear combination of S because span(S) = V.\\
			%$v_i = \{s_1, s_2, \cdots, s_m\}$
			Let the subset of S that generates $v_i$ be $S_i$\\
			$v_i = \sum_{j=1}^{m^k} a_j^k s_j^k$ where $a\in F$ and $s \in S_i$\\
			The span of the union of the sets that generates v, $\text{span(} \bigcup_{i=1}^{n}S_i \text{)} = V$\\ 
			Corollary 2(a) of Theorem 1.10 states that a generating set for V that contains exactly n vectors is a basis for V.
			The set above, which is a subset of S, contains exactly n vectors and generates V. Therefore, there is subset of S that is a basis for V. 
			\end{proof}

	}
	\item{
			Prove that S contains at least n vectors.
			\begin{proof}
			\-\\
			From (a) we know there is a subset of S that forms a basis. Since that subset contains n vectors, S must contain n or more vectors. 
			\end{proof}
		}
\end{enumerate}

\section{B}
Let $f(x)$ be a polynomial of degree n in $P_n(R)$. Prove that for any $g(x) \in P_n(R)$ there exists scalars $c_0, c_1, \cdots, c_n$ such that
$$g(x) = c_0f(x) + c_1f'(x) + c_2f''(x) + \cdots + c_nf^{(n)}(x)$$
\begin{proof}
	\-\\
	Let $B = \{f, f', f'', \cdots, f^{(n)}\}$.\\
	If $B$ forms a basis we can express any $g(x) \in P_n(R)$ in the format above (a linear combination).\\
	We can determine if $B$ is basis by seeing if it is linearly independent by using a matrix.\\
	$\mu_0f + \mu_1f' + \mu_2f'' + \cdots + \mu_nf^{(n)} = 0$\\

	\[
	\begin{bmatrix}
	a_n & a_{n-1} & \cdots & \cdots & a_0 \\
  	& na_n & \cdots & \cdots & a_1\\
  	& \ddots & \cdots & \cdots & \vdots \\
 	& & \ddots & \cdots & \vdots \\
	& & & n!a_n & (n-1)!a_{n-1} \\
 	& & & & n!a_n
	\end{bmatrix}
	\begin{bmatrix}
		\mu_0\\
		\mu_1\\
		\vdots\\
		\vdots\\
		\mu_{n-1}\\
		\mu_{n}\\
	\end{bmatrix}
	=
	\begin{bmatrix}
		0\\
		0\\
		\vdots\\
		\vdots\\
		0\\
		0\\
	\end{bmatrix}
	\]
	Solving this system of equations:\\
	Looking at the bottom row, $n!a_n \mu_n = 0$\\
	$\mu_n = \frac{0}{n!a_n} = 0$, $a_n$ is non zero because f is a nth degree polynomial and $a_n$ is its coefficient.\\
	Looking at row n - 1,$n!a_n \mu_{n-1} + (n-1)!a_{n-1} \mu{n} = 0$\\
	Because $\mu_n = 0$, $n!a_n \mu_{n-1} + 0 = 0$\\
	$\mu_{n-1} = \frac{0}{n!a_n} = 0$\\
	By back substitution, $\mu_n = \mu_{n-1} = \cdots = \mu_1 = \mu_0 = 0$\\
	This means that B is linearly independent, which also means that B is a basis.\\
	Therefore, any $g(x) \in P_n(R)$ can be a linear combination of B with the scalars $c_0, c_1, \cdots, c_n$
\end{proof}


\section{C}
\begin{enumerate}[label=\alph*.]
	\item{
			prove blah blah
			\begin{proof}
			\-\\
			$W_1$ and $W_2$ are finite dimensional subspaces of $V \Rightarrow $ subspace $W_1 + W_2$ is finite dimensional and $\text{dim}(W_1 + W_2) = \text{dim}(W_1) + \text{dim}(W_2) - \text{dim}(W_1 \cap W_2)$\\
			Let $B_{1\cap 2}$ be a basis for $W_1 \cap W_2$\\
			$B_{1\cap 2} = \{u_1, u_2, \cdots, u_k\}$\\
			By using the replacement theorem, we can extend $B_{1\cap 2}$ to be a basis for $W_1$\\
			So the basis for $W_1$ is $B_1$\\
			$B_1 = \{u_1, u_2, \cdots, u_k, v_1, v_2, \cdots, v_m\}$\\
			Likewise, we can extend $B_{1\cap 2}$ to be a basis for $W_2$\\
			$B_2 = \{u_1, u_2, \cdots, u_k, w_1, w_2, \cdots, w_p\}$\\
			Basis for $W_1 + W_2$ will be $B_1 \cup B_2$, however they may contain the same vectors twice.\\
			To prevent double counting, we must subtract $B_1 \cap B_2$ from $B_1 \cup B_2$\\
			Thus the basis for $W_1 + W_2$ is\\
			$B_{1+2} = \{u_1, u_2, \cdots, u_k, v_1, v_2, \cdots, v_m, w_1, w_2, \cdots, w_p\}$\\
			$W_1 + W_2$ is finite dimensional because its basis contains only a finite amount of vectors. \\
			$\text{dim}(W_1 + W_2) = \text{dim}(W_1) + \text{dim}(W_2) - \text{dim}(W_1 \cap W_2)$\\
			$k + m + p = k + m + k + p - k$\\
			$k + m + p = k + m + p$
			\end{proof}
		}
	\item{
			Let W 1 and W 2 be finite-dimensional subspaces of a vector space V, and let V = W 1 + W 2 . Deduce that V
			is the direct sum of W 1 and W 2 if and only if dim(V = dim)W 1 )( + dimW 2 ())\\
			$V = W_1 \oplus W_2 \Leftrightarrow dim(V) = dim(W_1) + dim(W_2)
			\begin{proof}
				\-\\
				$V = W_1 \oplus W_2 \Rightarrow dim(V) = dim(W_1) + dim(W_2)$\\
				From the definition of direct sum, $W_1 \cap W_2 = \{0\}$\\
				This means $dim(W_1 \cap W_2) = 0$\\
				From (a), we proved that $dim(V) = dim(W_1) + dim(W_2) - dim(W_1 \cap W_2)$\\
				$ = dim(W_1) + dim(W_2) - 0$\\
				$ = dim(W_1) + dim(W_2)$\\
				\-\\
				$dim(V) = dim(W_1) + dim(W_2) \Rightarrow V = W_1 \oplus W_2 $\\
				$V = W_1 \oplus W_2$ if and only if $V = W_1 + W_2 $ and $ W_1 \cap W_2 = \{0\}$\\
				$V = W_1 + W_2$ is true by the definition of the problem.\\
				From part (a), $dim(V) = dim(W_1) + dim(W_2) - dim(W_1 \cap W_2)$\\
				Our antecedent is $dim(V) = dim(W_1) + dim(W_2)$\\ 
				Setting the two equations equal to each other:\\
				$dim(W_1) + dim(W_2) - dim(W_1 \cap W_2) = dim(W_1) + dim(W_2)$\\ 
				$dim(W_1 \cap W_2) = 0$\\
				This means $ W_1 \cap W_2 = \{0\}$\\
				Thus, $dim(V) = dim(W_1) + dim(W_2)$ \\
				\-\\
				Therefore, $V = W_1 \oplus W_2 \Leftrightarrow dim(V) = dim(W_1) + dim(W_2)
			\end{proof}
		}
\end{enumerate}

\section{D}

\section{E}
Let V be the vector space of sequences. Define the functions T,U : V â†’ V by
T(a 1 ,a 2 ,.. . = ()a 2 ,a 3 ,.. .) and U(a 1 ,a 2 ,.. .) = (0,a 1 ,a 2 ,.. .
T and U are called the left shift and right shift operators o)n V respectively.
\begin{enumerate}[label=\alph*.]
	\item{
		Prove that T and U are linear.
		\begin{proof}
			\-\\
			T is linear if and only if $T(x+y) = T(x) + T(y)$ and $T(cx) = cT(x)$\\	
			Let $x,y \in V \quad c \in F$\\
			$x = (x_1, x_2, \cdots) \quad y = (y_1, y_2, \cdots)$\\
			$x + y = (x_1 + y_1, x_2 + y_2, \cdots)$ \\
			$T(x + y) = (x_2 + y_2, x_3 + y_3, \cdots)$ \\
			$T(x) = (x_2, x_3, \cdots)$\\
			$T(y) = (y_2, y_3, \cdots)$\\
			$T(x) + T(y) = (x_2 + y_2, x_3 + y_3, \cdots)$ \\
			Thus, $T(x + y) = T(x) + T(y)$\\
			\-\\
			$x = (x_1, x_2, \cdots)$\\
			$cx = (cx_1, cx_2, \cdots)$\\
			$T(cx) = (cx_2, cx_3, \cdots)$\\
			$T(x) = (x_2, x_3, \cdots)$\\
			$cT(x) = (cx_2, cx_3, \cdots)$\\
			Thus, $T(cx) = cT(x)$\\
			\-\\
			Therefore, T is linear. The proof for U being linear is similiar.
		\end{proof}
		}
		\item{
			T is onto but not one to one\\
			Let $v \in V$

			}
		\item{
			U is one to one but not onto.
			}
\end{enumerate}
	

\section{F}
Let S be the subspace of $M_{nxn}(R)$ generated by all matrices of the form $AB - BA$ with $A$ and $B$ in $M_{nxn}(R)$.
Prove that dim$(S) = n^{2} - 1$. (You may want to use the trace together with the rank-nullity theorem)
\begin{proof}
	\-\\
	Trace is a linear transformation.\\
	$\text{Tr}: M_{nxn}(R) \rightarrow R $\\
	The subspace S is defined as $\{AB - BA : A,B \in M_{nxn}(R)\}$\\
	$\text{Tr}(AB - BA) = \text{Tr}(AB) - \text{Tr} (BA)$\\
	$= \text{Tr}(AB) - \text{Tr} (AB)$\\
	$= 0$\\
	All matrices that can be expressed as $AB - BA$ is in the null space of Tr. This means that N(Tr)$ = S$.\\
	The rank-nullity theorem states:\\
	dim(N(Tr)) + dim(R(Tr)) = dim($M_{nxn}(R)$)\\
	N(Tr)$ = S$, so dim(S) + dim(R(Tr)) = dim($M_{nxn}(R)$)\\
	dim(S) = dim($M_{nxn}(R)$) - dim(R(Tr))\\
	= $n^2 - $ dim(R)\\
	= $n^2 - $ 1 \\
\end{proof}

\section{G}
Let T be a linear transformation of a vector space V into itself. Suppose that $x \in V$ is such that $T^m (x) = 0$, and
$T^{m-1}(x) \neq 0$ for some positive m. Show that $x,T(x),T^2(x),\cdots ,T^{m-1}(x)$ are linearly independent.
\begin{proof}
	\-\\
	The linear combination of the above set is $$ a_0x + a_1T(x) + a_2T^2(x) + \cdots + a_{n-1}T^{m-1}(x) = 0$$
	Notice that $T^n(x) = 0$ for all $n > m$.\\
	$T^{m+1}(x) = T(T^{m}(x)) = T(0) = 0$\\
	Let's take $T^{m-1}$ on both sides of the linear combination.	
	\begin{align*}
		T^{m-1}(a_0x + a_1T(x) + a_2T^2(x) + \cdots + a_{n-1}T^{m-1}(x)) &= T^{m-1}(0)\\
		T^{m-1}(a_0x)&= 0 \\
	\end{align*}
	So $z_1 = \sum_{i=1}^{n}a_ix_i$ where $a \in F$ and $x \in S_1$\\
	%$z_2 = \sum_{i=1}^{n}b_iy_i$ where $b \in F$ and $y \in S_2$\\
	If $S_1 \subseteq S_2$, then $x \in S_2$\\
	So $z_1 \in \text{span}(S_2)$ because we can write $z_1$ as a linear combination of $S_2$\\
	Therefore, if  $S_1 \subseteq S_2$ then $\text{span}(S_1) \subseteq \text{span}(S_2) \quad (*)$\\
	%As defined in the problem $S_2 \subseteq V$\\
	Defined in the problem, span($S_1$) = $V$\\
	By $(*)$, span($S_1$) $= V \subseteq$ span($S_2$)\\
	Using theorem 1.5, span($S_2$) $\subseteq V$\\
	Therefore, span($S_2$) $\subseteq V \subseteq$ span($S_2$) $\Leftrightarrow V = $ span($S_2$)
\end{proof}

\section{H} 
Let $T: R^3 \rightarrow R^3$
\begin{enumerate}[label=\alph*.]
	\item{
	If T(a,b,c) = (a,b,0), show that T is the projection on the xy-plane along the z-axis.	
	\begin{proof}
	\-\\
	We want to projection to be on the xy-plane along the z-axis. Let the projection be (x,y,0).\\
	To minimize the distance, we must choose x and y such that
	$$(a - x)^2 + (b - y)^2 + (c - 0)^2$$
	is minimum.
	Since the equation above is a difference of squares, x = a and b = y will give us the minimum value.
	Therefore, the projection on the xy-plane will be (a,b,0), which is T.
	\end{proof}
	}
	
	\item{
	Find a formula for T(a,b,c), where T represents the projection on the z-axis along the xy-plane.
	\begin{proof}
	\-\\
	We want to projection to be on the z-axis along the xy-plane. Let the projection be (0,0,z).\\
	To minimize the distance, we must choose z such that
	$$(a - 0)^2 + (b - 0)^2 + (c - z)^2$$
	is minimum.
	z = c will give us the minimum value.
	Therefore, the equation for T will be T(a,b,c)=(0,0,c).
	\end{proof}
	}

	\item{
	If T(a,b,c) = (a-c,b,0), show that T is the projection on the xy-plane along the line L $= \{(a,0,a):a \in R\}$
	\begin{proof}
	\-\\
	We want to projection to be on the xy-plane along the line L. Let the projection be $(x,y,0)$.\\
	A vector that is on L is $(1,0,1)$.
	To minimize the distance, we must choose $\lambda$ such that
	$$(a,b,c) + \lambda (1,0,1) = (x,y,0)$$
	is minimum. Writing the equation above as a system:
	\[
	\begin{align*}
		a + \lambda &= x\\
		b &= y\\
		c + \lambda &= 0
	\end{align*}
	\]
	Solving this system gives us, $x = a - c, y = b$\\ 
	Therefore, the projection on the xy-plane along the line L will be $(a-c,b,0)$.\\
	\end{proof}
	}

\end{enumerate}



\section{I} 
In $M_{mxn}(F)$, let $E^{ij}$ denote the matrix whose only nonzero entry is 1 in the $i$th row and $j$th column. Prove that $\{E^{ij}: 1 \leq i \leq m,
1 \leq j \leq n\}$ is linearly independent.
\begin{proof}
	\-\\
	If $E^{ij}$ is linearly independent then $a_{1,1}E^{1,1} + \cdots + a_{m,n}E^{m,n} \neq 0$\\
	This sum can only equal the 0 matrix if all a are 0.\\
	Therefore, $E^{ij}$ is linearly independent.
	%If $E^{ij}$ is linearly independent then $\sum_{/i=1}^{m} \sum_{j=1}^{n} a_{ij}E_{ij} \neq 0$ where not all a are 0.\\
	%The only time where the above sum equals the 0 matrix is if all a are 0. 
	%Given the definition of $E^{ij}$, we can rewrite the above sum as $\sum_{i=1}^{m} a_{i0} + \sum_{j=1}^{n}a_{0j}$ 
\end{proof}
\section{J}
Let $u$ and $v$ be distinct vectors in a vector space $V$. Show that $\{u,v\}$ is linearly dependent if and only if $u$ or $v$ is a
multiple of the other.

\begin{proof}
\-\\
	Let's first show that if $u$ or $v$ is a multiple of the other then $\{u,v\}$ is linearly dependent.\\
	Being a mutiple means $u = nv$ or $v = nu$ where $n \in F$ \\
	If $\{u,v\}$ is linearly dependent then $a_1u + a_2v = 0$ where $a \in F$\\
	Using definition of mutiple $a_1u + a_2nu = 0$\\
	Factoring, $u(a_1 + a_2n) = 0$\\
	This means $(a_1 + a_2n) = 0$\\	
	So, $n = \frac{-a_1}{a_2}$ which is a solution for linearly dependency.\\
	Without loss of generality, we can prove the case where $ v = nu$\\
	Therefore, $\{u,v\}$ is linearly dependent.\\
	\-\\
	Now let's show that if $\{u,v\}$ is linearly dependent then $u$ or $v$ is a multiple of the other.\\
	If $\{u,v\}$ is linearly dependent then $a_1u + a_2v = 0$ where $a \in F$\\
	We can rewrite the equation above as $a_1u = -a_2v$\\
	$u = \frac{-a_2}{a_1}v$\\
	Thus, $u$ is a multiple of $v$.\\
	Without loss of generality, we can prove $v$ is a multiple of $u$.\\
	Therefore, $u$ or $v$ is a mutiple of the other.
\end{proof}
\end{document}
