\documentclass[11pt]{scrartcl}
\usepackage[sexy]{../../evan}
\usepackage{cmbright}
\usepackage{cancel}
\usepackage[T1]{fontenc}
%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
%\usepackage[pdfborder={0 0 0},colorlinks=true,citecolor=red{}]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{systeme}

\title{Math 341: Midterm 2}
\author{Daniel Ko}
\date{Spring 2020}

\begin{document}
\maketitle

%problem 1
\section{}
Let
\begin{equation}
	\mathbf{A} = \left [ \begin{array}{cc}
			a & b \\
			c & d
		\end{array} \right ],
	\mbox{ and  } \mathbf{b} = \left ( \begin{array}{c}
			e \\
			f
		\end{array} \right )
\end{equation}
\begin{enumerate}[label=\alph*.]
	\item{
	      Suppose that $a\neq 0 $, compute the solution of $\mathbf{Ax = b}$
	      using row reduction and provide the conditions on $a,b,c,d$ such that your computations are valid.
	      Express the result as a simplified expression. (\textbf{Hint:} recall that you can not divide by zero)
	      \begin{proof}
		      We perform reduced row echelon form (rref) on the augmented matrix
		      \begin{align*}
			      (A|b)=
			      \left[\begin{array}{cc|c}
					      a & b & e \\
					      c & d & f
				      \end{array}\right] \\
			      R_2 \leftarrow R_2 - \frac{c}{a} R_1
			      \left[\begin{array}{cc|c}
					      a & b                & e                \\
					      0 & d - \frac{cb}{a} & f - \frac{ce}{a}
				      \end{array}\right] \\
			      \left[\begin{array}{cc|c}
					      a & b               & e               \\
					      0 & \frac{ad-cb}{a} & \frac{af-ce}{a}
				      \end{array}\right] \\
			      R_2 \leftarrow \frac{a}{ad-cb}R_2 \quad \text{Assuming that $ab-cd \neq 0$} \quad
			      \left[\begin{array}{cc|c}
					      a & b & e                   \\
					      0 & 1 & \frac{af-ce}{ad-cb}
				      \end{array}\right] \\
			      R_1 \leftarrow R_1 - bR_2
			      \left[\begin{array}{cc|c}
					      a & 0 & e -b \frac{af-ce}{ad-cb} \\
					      0 & 1 & \frac{af-ce}{ad-cb}
				      \end{array}\right] \\
			      R_1 \leftarrow \frac{R_1}{a}
			      \left[\begin{array}{cc|c}
					      1 & 0 & \frac{1}{a}(e -b \frac{af-ce}{ad-cb}) \\
					      0 & 1 & \frac{af-ce}{ad-cb}
				      \end{array}\right] \\
			      \left[\begin{array}{cc|c}
					      1 & 0 & \frac{de-bf}{ad-cb} \\
					      0 & 1 & \frac{af-ce}{ad-cb}
				      \end{array}\right]
		      \end{align*}

		      \[x =
			      \begin{bmatrix}
				      \frac{de-bf}{ad-cb} \\
				      \frac{af-ce}{ad-cb}
			      \end{bmatrix} =
			      \frac{1}{ad-cb}
			      \begin{bmatrix}
				      de-bf \\
				      af-ce
			      \end{bmatrix}
			      \text{ where } ad-cb \neq 0
		      \]
	      \end{proof}
	      }
	\item{
	      If $a = 0$, and $c\neq 0$, is your above computation still valid?
	      How would you modify it? (explain briefly)
	      (\textbf{Hint:} recall that you can swap the equations and the result is the same)
	      \begin{proof}
		      If $a = 0$, and $c\neq 0$, then the above computation will not be valid as we
		      divided by $a$ multiple times when we computed the rref. I would swap the first
		      and second rows so that it would look like
		      \[
			      \left[\begin{array}{cc|c}
					      c & d & f \\
					      0 & b & e
				      \end{array}\right]
		      \]
		      and compute the rref, assuming that $b\neq0$. We obtain the rref,
		      \[
			      \left[\begin{array}{cc|c}
					      1 & 0 & \frac{bf-de}{bc} \\
					      0 & 1 & \frac{e}{b}
				      \end{array}\right]
		      \]
		      and the solution
		      \[
			      x=
			      \begin{bmatrix}
				      \frac{bf-de}{bc} \\
				      \frac{e}{b}
			      \end{bmatrix} \quad \text{ where } b \neq 0
		      \]
	      \end{proof}
	      }
	\item{
	      If $a = 0$, $c=0$, but $b \neq 0$, $d \neq 0$,
	      what are the conditions on $e$ and $f$ such that the system $\mathbf{Ax=b}$ has a solution?
	      Is the solution unique?  (\textbf{Hint:} recall that $\mathbf{Ax = b}$ has a solution if and only if
	      $\mathbf{b}$ can be written as a linear combination of the columns of $\mathbf{A}$)
	      \begin{proof}
		      If $a = 0$, $c=0$, $b \neq 0$, $d \neq 0$, we get the augmented matrix
		      \[
			      \left[\begin{array}{cc|c}
					      0 & b & e \\
					      0 & d & f
				      \end{array}\right]
		      \]
		      Performing row reduction,
		      \[
			      \left[\begin{array}{cc|c}
					      0 & 1 & \frac{e}{b} \\
					      0 & 1 & \frac{f}{d}
				      \end{array}\right]
		      \]
		      Having an infinite amount of solutions is by definition another way of saying
		      that a system that is consistent and that the solutions are not unique.

		      It is clear that det$(\mathbf{A}) = 0$ by multiplying the diagonal entries because it is an
		      upper triangular matrix. This means that
		      the solution, if it exists, is not unique by
		      Theorem 3.10 and the corollary to Theorem 4.7. A system is consistent if and only if
		      $rank(\mathbf{A}) = rank(\mathbf{A}|\mathbf{b})$ by Theorem 3.11.
		      For this condition to hold,
		      \[
			      \mathbf{b} \in span(\mathbf{A}) \Leftrightarrow
			      \begin{pmatrix}
				      \frac{e}{b} \\
				      \frac{f}{d} \\
			      \end{pmatrix}
			      =
			      c_1
			      \begin{pmatrix}
				      0 \\
				      0 \\
			      \end{pmatrix} +
			      c_2
			      \begin{pmatrix}
				      1 \\
				      1 \\
			      \end{pmatrix}
			      \text{ where } c_1,c_2 \in F
		      \]
		      So the condition of the solution is,
		      \begin{align*}
			      c_2         & = \frac{e}{b}  \\
			      c_2         & =  \frac{f}{d} \\
			      \frac{e}{b} & = \frac{f}{d}
		      \end{align*}
		      %$$x_2 = \frac{e}{b} = \frac{f}{d}$$
		      Thus, there exists a infinite amount of solution.
	      \end{proof}
	      }
	\item{
	      Solve the system
	      \begin{equation}
		      \left [ \begin{array}{cc}
				      \sqrt{2}  & 3\sqrt{2} \\
				      2\sqrt{2} & \sqrt{2}
			      \end{array} \right ] \left ( \begin{array}{c}
				      x_1 \\
				      x_2
			      \end{array} \right ) = \left ( \begin{array}{c}
				      5\sqrt{2} \\
				      5\sqrt{2}
			      \end{array} \right ).
	      \end{equation}
	      (\textbf{Hint:} You may want to use the formula you just deduced)
	      }
	      \begin{proof}
		      \begin{align*}
			      x_1 & = \frac{de-bf}{ad-cb}                                                        \\
			          & = \frac{\sqrt2(5\sqrt2) - 3\sqrt2(5\sqrt2)}{\sqrt2\sqrt2 - 3\sqrt2(2\sqrt2)} \\
			          & = \frac{10 - 30}{2-12}                                                       \\
			          & = \frac{-20}{-10}                                                            \\
			          & = 2                                                                          \\
			          &                                                                              \\
			      x_2 & = \frac{af-ce}{ad-cb}                                                        \\
			          & = \frac{\sqrt2(5\sqrt2) - 2\sqrt2(5\sqrt2)}{\sqrt2\sqrt2 - 3\sqrt2(2\sqrt2)} \\
			          & = \frac{10-20}{-10}                                                          \\
			          & = 1
		      \end{align*}
	      \end{proof}
\end{enumerate}

%problem 2
\section{}
Let
\begin{equation}
	\mathbf{A} = \left [ \begin{array}{cccc}
			0  & - \alpha & 2  & 0             \\
			1  & 1        & 0  & 1             \\
			2  & 2        & 2  & 3             \\
			-2 & -2       & 4  & 2\alpha       \\
			0  & \alpha   & -1 & 2\alpha + 1/2
		\end{array} \right ], \qquad  \mbox{and }
	\mathbf{b} =  \left [ \begin{array}{c}
			2          \\
			1          \\
			4          \\
			2 + \alpha \\
			2 \beta + \alpha -2
		\end{array} \right ]
\end{equation}
What are the conditions on $\alpha$ and $\beta$ such that the system $\mathbf{A} \mathbf{x} = \mathbf{b}$:
\begin{enumerate}[label=\alph*.]
	\item{
	      Has no solution?
	      \begin{proof}
		      We begin by putting the augmented matrix $(\mathbf{A}|\mathbf{b})$ in its reduced form.
		      \begin{align*}
			      (\mathbf{A}|\mathbf{b})=
			      \left [ \begin{array}{cccc|c}
					      0  & - \alpha & 2  & 0             & 2                  \\
					      1  & 1        & 0  & 1             & 1                  \\
					      2  & 2        & 2  & 3             & 4                  \\
					      -2 & -2       & 4  & 2\alpha       & 2 + \alpha         \\
					      0  & \alpha   & -1 & 2\alpha + 1/2 & 2\beta + \alpha -2
				      \end{array} \right ] \\
			      R_5 \leftarrow R_5 + R_1
			      \left [ \begin{array}{cccc|c}
					      0  & - \alpha & 2 & 0             & 2               \\
					      1  & 1        & 0 & 1             & 1               \\
					      2  & 2        & 2 & 3             & 4               \\
					      -2 & -2       & 4 & 2\alpha       & 2 + \alpha      \\
					      0  & 0        & 1 & 2\alpha + 1/2 & 2\beta + \alpha
				      \end{array} \right ] \\
			      R_3 \leftarrow R_3 - 2R_2
			      \left [ \begin{array}{cccc|c}
					      0  & - \alpha & 2 & 0             & 2               \\
					      1  & 1        & 0 & 1             & 1               \\
					      0  & 0        & 2 & 1             & 2               \\
					      -2 & -2       & 4 & 2\alpha       & 2 + \alpha      \\
					      0  & 0        & 1 & 2\alpha + 1/2 & 2\beta + \alpha
				      \end{array} \right ] \\
			      R_4 \leftarrow R_4 + 2 R_2
			      \left [ \begin{array}{cccc|c}
					      0 & - \alpha & 2 & 0             & 2               \\
					      1 & 1        & 0 & 1             & 1               \\
					      0 & 0        & 2 & 1             & 2               \\
					      0 & 0        & 4 & 2\alpha  +2   & 4 + \alpha      \\
					      0 & 0        & 1 & 2\alpha + 1/2 & 2\beta + \alpha
				      \end{array} \right ] \\
			      R_4 \leftarrow R_4 - 2R_3,R_5 \leftarrow R_5 - \frac12R_3
			      \left [ \begin{array}{cccc|c}
					      0 & - \alpha & 2 & 0       & 2                   \\
					      1 & 1        & 0 & 1       & 1                   \\
					      0 & 0        & 2 & 1       & 2                   \\
					      0 & 0        & 0 & 2\alpha & \alpha              \\
					      0 & 0        & 0 & 2\alpha & 2\beta + \alpha + 1
				      \end{array} \right ] \\
			      R_5 \leftarrow R_5 - R_4
			      \left [ \begin{array}{cccc|c}
					      0 & - \alpha & 2 & 0       & 2          \\
					      1 & 1        & 0 & 1       & 1          \\
					      0 & 0        & 2 & 1       & 2          \\
					      0 & 0        & 0 & 2\alpha & \alpha     \\
					      0 & 0        & 0 & 0       & 2\beta - 1
				      \end{array} \right ] \\
			      R_1 \leftrightarrow R_2 \quad (\mathbf{A'}|\mathbf{b'} ) =
			      \left [ \begin{array}{cccc|c}
					      1 & 1        & 0 & 1       & 1          \\
					      0 & - \alpha & 2 & 0       & 2          \\
					      0 & 0        & 2 & 1       & 2          \\
					      0 & 0        & 0 & 2\alpha & \alpha     \\
					      0 & 0        & 0 & 0       & 2\beta - 1
				      \end{array} \right ]
		      \end{align*}
		      By Theorem 3.11 and 3.13, a system is consistent if and only if $rank(\mathbf{A'}) = rank(\mathbf{A'}|\mathbf{b'})$.
		      Thus this system will have no solution if $2\beta - 1 \neq 0$ because then $\mathbf{b'} \notin span(\mathbf{A'})$.

		      This is when $\beta \neq \frac12$.
		      We observe that there will be no conditions on $\alpha$.
		      %show that b \in R(L_A) ?? 
	      \end{proof}
	      }
	\item{
	      Has an unique solution? Find the solution.
	      (\textbf{Hint:} you will need to row reduce the augmented system to echelon form,
	      and then use the theorems seen in class to impose the conditions on $\alpha$ and $\beta$).
	      \begin{proof}
		      Combining Theorem 3.10 and the corollary to Theorem 4.7, we get that a system has a
		      unique solution if and only if det$(\mathbf{A}) \neq 0$.
		      The determinant of an upper triangular matrix is the product of its diagonal
		      entries by property 4 of determinants in section 4.4.
		      %IF the above fact is not allowed, I will write out the full recursive definition 
		      Using this fact we can compute the condition of $\alpha$ as such that
		      \begin{align*}
			      1*-\alpha*2*2\alpha & \neq 0 \\
			      -4\alpha^2          & \neq 0 \\
			      \alpha              & \neq 0
		      \end{align*}
		      and from (a), $\beta = \frac12$. Combining these two conditions we get the following system,
		      \[
			      \left [ \begin{array}{cccc|c}
					      1 & 1        & 0 & 1       & 1      \\
					      0 & - \alpha & 2 & 0       & 2      \\
					      0 & 0        & 2 & 1       & 2      \\
					      0 & 0        & 0 & 2\alpha & \alpha \\
					      0 & 0        & 0 & 0       & 0
				      \end{array} \right ]
		      \]
		      By performing back substitution we compute the unique solution
		      \begin{align*}
			      2\alpha x_4 = \alpha   & \Leftrightarrow x_4 = \frac12                     \\
			      2x_3 + x_4 = 2         & \Leftrightarrow x_3 = \frac34                     \\
			      -\alpha x_2 + 2x_3 = 2 & \Leftrightarrow x_2  = -\frac{1}{2\alpha}         \\
			      x_1 + x_2 + x_4 = 1    & \Leftrightarrow x_ 1 =\frac12 + \frac{1}{2\alpha}
		      \end{align*}

		      \[
			      x = \begin{bmatrix}
				      \frac12 + \frac{1}{2\alpha} \\
				      -\frac{1}{2\alpha}          \\
				      \frac34                     \\
				      \frac12
			      \end{bmatrix}
		      \]
		      as desired.
	      \end{proof}
	      }

	\item{
	      Has infinite amount of solutions? Find the solution set in parametric form.
	      (\textbf{Hint:} You may have one equations for $\alpha$ and one for $\beta$ that have to be satisfied simultaneously).
	      \begin{proof}
		      Having an infinite amount of solutions is by definition another way of saying
		      that a system that is consistent and that the solutions are not unique.
		      A system is consistent if and only if $rank(\mathbf{A}) = rank(\mathbf{A}|\mathbf{b})$
		      by Theorem 3.11. If det$(\mathbf{A}) = 0$, the solution, if it exists, is not unique by
		      Theorem 3.10 and the corollary to Theorem 4.7.

		      Using this fact we can compute the condition of $\alpha$ as such that
		      \begin{align*}
			      1*-\alpha*2*2\alpha & = 0 \\
			      -4\alpha^2          & = 0 \\
			      \alpha              & = 0
		      \end{align*}
		      Given that $\alpha = 0$, and that $\beta = \frac12$ from part (a)
		      we get the following system,

		      \[
			      (\mathbf{A'}|\mathbf{b'})=
			      \left [ \begin{array}{cccc|c}
					      1 & 1 & 0 & 1 & 1 \\
					      0 & 0 & 2 & 0 & 2 \\
					      0 & 0 & 2 & 1 & 2 \\
					      0 & 0 & 0 & 0 & 0 \\
					      0 & 0 & 0 & 0 & 0
				      \end{array} \right ]
		      \]
		      It is clear that  $rank(\mathbf{A'}) = rank(\mathbf{A'}|\mathbf{b'})$ because
		      $\mathbf{b'}$ is a linear combination of the second and third column from
		      $\mathbf{A'}$. By Theorem 3.13, we know that the solution set for $\mathbf{A'x=b'}$
		      is equivalent to the solution set for $\mathbf{Ax=b}$. Hence, both systems are consistent. \\
		      We can compute a solution space to $\mathbf{Ax=b}$ as outlined in Theorem 3.9. We start by first computing the
		      solution set to $\mathbf{Ax}=0$ denoted by $K_H$. It is clear that $rank(\mathbf{A}) = 3$ because the first two
		      columns are the same and the rest of the columns are linearly independent from each other.
		      By Theorem 3.8, $dim(K_H) = 4 - 3 = 1$. Thus any nonzero solution constitutes a basis for K. For example, since
		      $$
			      \begin{pmatrix}
				      1  \\
				      -1 \\
				      0  \\
				      0
			      \end{pmatrix}
		      $$
		      is a solution to the $\mathbf{Ax}=0$, it is a basis for $K_H$ by Corollary 2 of Theorem 1.10.
		      So a solution set to $K_H$ would be
		      \[
			      K_H=
			      \left\{
			      t\begin{pmatrix}
				      1  \\
				      -1 \\
				      0  \\
				      0
			      \end{pmatrix}: t \in \mathbb{R}
			      \right\}
		      \]
		      A solution to $\mathbf{Ax=b}$ is
		      $$
			      \begin{pmatrix}
				      1 \\
				      0 \\
				      1 \\
				      0
			      \end{pmatrix}
		      $$
		      Therefore, by Theorem 3.9 we compute solution space when this system has infinite amount of solutions, which is when $\alpha = 0$ and $\beta = \frac12$ as
		      \[
			      K=
			      \left\{
			      \begin{pmatrix}
				      1 \\
				      0 \\
				      1 \\
				      0
			      \end{pmatrix}+
			      t\begin{pmatrix}
				      1  \\
				      -1 \\
				      0  \\
				      0
			      \end{pmatrix}: t \in \mathbb{R}
			      \right\}
		      \]
	      \end{proof}
	      }
\end{enumerate}

%problem 3
\section{}
Let $A \in M_{n\times n}(F)$, for a field $F$.
We want to prove that $rank(A^2) - rank(A^3) \leq rank(A) - rank(A^2)$.
The solution to this exercise requires the notion of quotient spaces.
Even though you should already be familiar with quotient spaces we will prove a few properties that will be useful.\\ \-\\
Let \(\mathrm{W}\) be a subspace of a vector space \(\mathrm{V}\) over a field \(F .\) For any \(v \in \mathrm{V}\) the set \(\{v\}+\mathrm{W}=\{v+w: w \in \mathrm{W}\}\) is called the coset of W containing \(v .\) It is customary to denote this coset by \(v+W\) rather than \(\{v\}+W\).
Following this notation we write $\mathrm{V}/\mathrm{W} = \{ v + \mathrm{W}: v \in \mathrm{V} \}$, which is usually called the quotient space $\mathrm{V}$ module $\mathrm{W}$. Addition and scalar multiplication by scalars can be defined in the collection \(\mathrm{V}/\mathrm{W}\) as follows:
$$
	\left(v_{1}+\mathrm{W}\right)+\left(v_{2}+\mathrm{W}\right)=\left(v_{1}+v_{2}\right)+\mathrm{W}
$$
for all \(v_{1}, v_{2} \in \mathrm{V}\) and
$$
	a(v+W)=a v+W
$$
for all \(v \in \mathrm{V}\) and \(a \in F\)
\begin{enumerate}[label=\alph*.]
	%CHECK THIS , just copied and pasted from hw2 lol
	\item{
	      Prove that \(v_{1}+\mathrm{W}=v_{2}+\mathrm{W}\) if and only if \(v_{1}-v_{2} \in \mathrm{W}\).
	      (\textbf{Hint}: recall that $v_{1}+\mathrm{W}$ is a set, thus you need to prove equality between sets)
	      \begin{proof}\-\
		      \begin{enumerate}[i.]
			      \item{
			            $v_1 + W = v_2 + W \Rightarrow v_1 - v_2 \in W$\par
			            If $v_1 + W = v_2 + W$, then $\exists w_1, w_2 \in W$ such that
			            $v_1 + w_1 = v_2 + w_2$\\
			            $v_1 - v_2 = w_2 - w_1$\\
			            Since, $w_2 - w_1 \in W$ (clourse under addition)\\
			            Therefore, $v_1 - v_2 \in W$
			            }
			      \item{
			            $v_1 - v_2 \in W \Rightarrow v_1 + W = v_2 + W $\par
			            This means $v_1 - v_2 = w$ where $w \in W \ (*)$\\
			            Now let $x \in v_1 + W$\\
			            By definition, $\exists w_x \in W \text{ such that } x = v_1 + w_x$\\
			            By $(*) \ v_1 = v_2 + w$\\
			            So, $x = v_2 + w + w_x$\\
			            Since, $w + w_x \in W$ (closure under addition)\\
			            We have $x \in v_2 + W$\\
			            So, $v_1 + W \subseteq v_2 + W$\\
			            Without loss of generality, we can show $v_2 + W \subseteq v_1 + W$\\
			            Therefore, $v_1 + W = v_2 + W$
			            }
		      \end{enumerate}
		      Therefore, \(v_{1}+\mathrm{W}=v_{2}+\mathrm{W}\) if and only if \(v_{1}-v_{2} \in \mathrm{W}\)
	      \end{proof}
	      }

	\item{
	      Show that \(\mathrm{V}/\mathrm{W}\) with the operations defined above is a linear vector space.
	      \begin{enumerate}[label=VS \arabic*:]
		      \item{
		            For all $x, y$ in $\mathrm{V}/\mathrm{W}, x+y=y+x$ (commutativity of addition)
		            \begin{proof}
			            Let $x = v_x + W, y = v_y + W$ where $v_x,v_y \in V$.
			            \begin{align*}
				            x + y & = (v_x + W) + (v_y + W) \\
				                  & = (v_x + v_y) + W       \\
				                  & = (v_y + v_x) + W       \\
				                  & = (v_y + W) + (v_x + W) \\
				                  & = y + x
			            \end{align*}
		            \end{proof}
		            }
		      \item{
		            For all $x, y, z$ in $\mathrm{V}/\mathrm{W},(x+y)+z=x+(y+z)$ (associativity of addition)
		            \begin{proof}
			            Let $x = v_x + W, y = v_y + W, z = v_z + W,$ where $v_x,v_y,v_z \in V$.
			            \begin{align*}
				            (x + y) + z & = ((v_x + W) + (v_y + W)) + (v_z + W) \\
				                        & = ((v_x + v_y) + W) + (v_z + W)       \\
				                        & = (v_y + v_x + v_z) + W               \\
				                        & = (v_x + W) + ((v_y + v_z) + W)       \\
				                        & = (v_x + W) + ((v_y + W) + (v_z + W)) \\
				                        & = x + (y + z)
			            \end{align*}
		            \end{proof}
		            }
		      \item{
		            There exists an element in $\mathrm{V}/\mathrm{W}$ denoted by $\mathbf{0}$ such that $x+\mathbf{0}=x$ for each $x$ in $\mathrm{V}/\mathrm{W}$
		            \begin{proof}
			            Let $x = v_x + W$ where $v_x \in V$. Let $\mathbf{0}$ in $\mathrm{V}/\mathrm{W}$ be defined
			            as $0 + W$, where $0 \in V$.
			            \begin{align*}
				            x + \mathbf{0} & = x + (0 + W)         \\
				                           & = (v_x + W) + (0 + W) \\
				                           & = (v_x + 0) + W       \\
				                           & = v_x + W             \\
				                           & = x
			            \end{align*}
		            \end{proof}
		            }
		      \item{
		            For each element $x$ in $\mathrm{V}/\mathrm{W}$ there exists an element $y$ in $\mathrm{V}/\mathrm{W}$ such that $x+y=\mathbf{0}$
		            \begin{proof}
			            Let $x = v_x + W$ where $v_x \in V$. Fix y such that $y = -v_x + W$.
			            \begin{align*}
				            x + y & = (v_x + W) + (-v_x + W) \\
				                  & = (v_x - v_x) + W        \\
				                  & = 0 + W                  \\
				                  & = \mathbf{0}
			            \end{align*}
		            \end{proof}
		            }
		      \item{
		            For each element $x$ in $\mathrm{V}/\mathrm{W}, 1 x=x$
		            \begin{proof}
			            Let $x = v_x + W$ where $v_x \in V$.
			            \begin{align*}
				            1x & = 1(v_x + W) \\
				               & =(1v_x + W)  \\
				               & = (v_x + W)  \\
				               & = x
			            \end{align*}
		            \end{proof}
		            }
		      \item{
		            For each pair of elements $a, b$ in $\mathbb{F}$ and each element $x$ in $\mathrm{V}/\mathrm{W}$, $(a b) x=a(b x)$
		            \begin{proof}
			            Let $x = v_x + W$ where $v_x \in V$.
			            \begin{align*}
				            (a b) x & = abv_x + W  \\
				                    & =a(bv_x + W) \\
				                    & =a(bx)
			            \end{align*}
		            \end{proof}
		            }
		      \item{
		            For each element a in $\mathbb{F}$ and each pair of elements $x, y$ in $\mathrm{V}/\mathrm{W}$, $a(x+y)=a x+a y$
		            \begin{proof}
			            Let $x = v_x + W, y = v_y + W$ where $v_x,v_y \in V$.
			            \begin{align*}
				            a(x + y) & = a((v_x + v_y) + W)       \\
				                     & =((av_x + av_y) + W )      \\
				                     & =(av_x + W ) +  (av_y + W) \\
				                     & =a(v_x + W ) + a(v_y + W)  \\
				                     & =ax +ay
			            \end{align*}
		            \end{proof}
		            }
		      \item{
		            For each pair of elements $a, b$ in $\mathbb{F}$ and each element $x$ in $\mathrm{V}/\mathrm{W}$, $(a+b) x=a x+b x$
		            \begin{proof}
			            Let $x = v_x + W$ where $v_x \in V$.
			            \begin{align*}
				            (a + b)x & = (a + b)(v_x + W)         \\
				                     & = ((a + b)v_x) + W         \\
				                     & = ((av_x + bv_x) + W       \\
				                     & = (av_x + W) +  (bv_x + W) \\
				                     & = a(v_x + W) + b(v_x + W)  \\
				                     & = ax + bx
			            \end{align*}
		            \end{proof}
		            }
	      \end{enumerate}
	      Therefore, $\mathrm{V}/\mathrm{W}$ is a vector space because it holds all the properties above.
	      }

	\item{
	      Prove that if $dim(\mathrm{V}) < \infty$ then $dim(\mathrm{V}/\mathrm{W}) = dim(\mathrm{V}) - dim(\mathrm{W})$.
	      (Hint: Define a linear map $T:\mathrm{V} \rightarrow \mathrm{V}/\mathrm{W}$ such that the range of $T$ is $\mathrm{V}/\mathrm{W}$, and then use the rank-nullity theorem)
	      \begin{proof}We define the linear map $T:\mathrm{V} \rightarrow \mathrm{V}/\mathrm{W}$ by \[T(v) = v + W\]
		      We first prove that $T$ is in fact linear, where $v_1,v_2 \in V$ and $c \in F$.
		      \begin{align*}
			      T(cv_1 + v_2) & = (cv_1 + v_2) + W         \\
			                    & =  (cv_1 + W)+ ((v_2) + W) \\
			                    & =  c(v_1 + W)+ ((v_2) + W) \\
			                    & =  cT(v_1)+ T(v_2)
		      \end{align*}
		      We claim that $N(T) = W$ and $R(T) = \mathrm{V}/\mathrm{W}$
		      \begin{enumerate}[label=\arabic*.]
			      \item{
			            $R(T) = \mathrm{V}/\mathrm{W}$
			            \begin{enumerate}[label=\roman*.]
				            \item{
				                  $R(T) \subseteq \mathrm{V}/\mathrm{W}$\par
				                  By Theorem 2.1
				                  }
				            \item{
				                  $\mathrm{V}/\mathrm{W} \subseteq R(T)$\par
				                  Let $y \in \mathrm{V}/\mathrm{W}$ such that $y = v_y + W$ where $v_y \in V$.\\
				                  To prove that $y \in R(T)$ we need to show that  $\exists x$ such that $ T(x) = y$.\\
				                  Notice that we can fix $x = v_y$. Therfore, $y \in R(T)$ so $\mathrm{V}/\mathrm{W} \subseteq R(T)$.
				                  }
			            \end{enumerate}
			            }
			      \item{
			            $N(T) = W$
			            \begin{enumerate}[label=\roman*.]
				            \item{
				                  $N(T) \subseteq W$
				                  \begin{lemma}
					                  $w_1 + W = w_2 + W = W$ where $w_1, w_2 \in W$.
					                  \begin{proof}\-\\
						                  Let $x_1 \in w_1 + W$ and $x_2 \in w_2 + W$\\
						                  By definition, $w_1 + W = \{w_1 + w : w \in W\}$.\\
						                  We know that $w_1 +w \in W$ because $W$ is a subspace and has closure under addition.
						                  Hence, $x_1 \in W$ and similarly $x_2 \in W$. Therefore, $w_1 + W \subseteq W$ and  $w_2 + W \subseteq W$.\par
						                  Now consider $x_1 \in W$ and $x_2 \in W$.\\
						                  $x_1 \in w_1 + W$ because $\exists w \text{ s.t. } w = x_1 - w_1$.
						                  Similarly, $x_2 \in w_2 + W$.\\
						                  Hence, $W \subseteq w_1 + W$ and $W \subseteq w_2 + W$.\par
						                  Therefore, $w_1 + W = W$ and $W = w_2 + W$ so, $w_1 + W = w_2 + W = W$.
					                  \end{proof}
				                  \end{lemma}
				                  Let $x \in N(T)$.\\ %By Theorem 2.1, we know that $N(T) \in V$, thus $x \in V$.\\
				                  By definition of $N(T)$ and using the lemma above,
				                  \begin{align*}
					                  T(x) & = \mathbf{0} \\
					                       & = 0 + W      \\
					                       & = w + W
				                  \end{align*}
				                  where $0, w \in W$ and $\mathbf{0} \in \mathrm{V}/\mathrm{W}$.\\
				                  This must mean that $x = w$ and since $w \in W$, $x \in W$. Therefore, $N(T) \subseteq W$\\
				                  }
				            \item{
				                  $W \subseteq N(T)$\\
				                  Let $w \in W$.
				                  \begin{align*}
					                  T(w) & = w + W      \\
					                       & = 0 + W      \\
					                       & = \mathbf{0}
				                  \end{align*}
				                  }
				                  So $w \in N(T)$. Therefore, $W \subseteq N(T)$.
			            \end{enumerate}
			            }
		      \end{enumerate}
		      Notice that $\mathrm{V}/\mathrm{W} \subseteq \mathrm{V}$
		      so $\mathrm{V}/\mathrm{W}$ is finite dimensional.
		      Since $T$ is a linear map and that $\mathrm{V}$ and $\mathrm{V}/\mathrm{W}$ are indeed finite dimensional vector spaces
		      (by part b) we can use the rank-nullity theorem (Theorem 2.3).
		      \begin{align*}
			      dim(N(T)) + dim(R(T))               & =   dim(V)        \\
			      dim(W) + dim(\mathrm{V}/\mathrm{W}) & = dim(V)          \\
			      dim(\mathrm{V}/\mathrm{W})          & = dim(V) - dim(W)
		      \end{align*}
		      as desired.
	      \end{proof}
	      }

	      \iffalse
		      Alternate solution that might work but not using the hint.

		      We will prove this fact by creating a basis for $\mathrm{V}/\mathrm{W}$ and computing its dimension.
		      Since $\mathrm{V}$ is a finite dimensional vector space, so $\mathrm{W}$ is also finite dimensional by Theorem 1.11.
		      Let the dimension of $\mathrm{V}$ and $\mathrm{W}$ be denoted by $k$ and $j$, respectively.
		      We can construct a basis $\beta_W$ for $\mathrm{W}$, where $\beta_W = \{u_1, u_2, \cdots, u_j\}$.
		      We can extend $\beta_W$ to be a basis for $\mathrm{V}$ by the corollary of Theorem 1.11.
		      Let this basis be $\beta_V = \{u_1, u_2, \cdots, u_j, u_{j+1}, u_{j+2}, \cdots , u_{k}\}$.
		      We claim that the basis for $\mathrm{V}/\mathrm{W}$ is
		      $\beta_{V/W} = \{u_{j+1} + W, u_{j+2} + W, \cdots , u_{k} + W\}$.
		      \begin{enumerate}[label=\roman*.]
			      \item{
			            $\beta_{V/W}$ is linearly independent
			            \begin{align*}
				            \alpha_{j+1}(u_{j+1} + W) + \alpha_{j+2}(u_{j+2} + W) + \cdots + \alpha_{k}(u_{k} + W) & = \mathbf{0} \\
				            (\alpha_{j+1}u_{j+1} + \alpha_{j+2}u_{j+2} + \cdots + \alpha_{k}u_{k}) + W             & = 0 + W
			            \end{align*}
			            This means that $(\alpha_{j+1}u_{j+1} + \alpha_{j+2}u_{j+2} + \cdots + \alpha_{k}u_{k}) = 0 \in V$.
			            We can rewrite this as $(0u_1 + 0u_2 + \cdots + 0u_j + \alpha_{j+1}u_{j+1} + \alpha_{j+2}u_{j+2} + \cdots + \alpha_{k}u_{k}) = 0$.
			            Thus, $ \alpha_{j+1} = \alpha_{j+2} = \cdots = \alpha_{k} = 0$ because it is written in terms
			            of the basis for $V$. So, $\beta_{V/W}$ is linearly independent.
			            }
			      \item{
			            $\beta_{V/W}$ generates $\mathrm{V}/\mathrm{W}$\\\-\\
			            Let $x = v_x + W$ where $x \in \mathrm{V}/\mathrm{W}, v_x \in V$.
			            \begin{align*}
				            x & = v_x + W                                                                                                                            \\
				              & = (\alpha_1u_1 + \alpha_2u_2 + \cdots + \alpha_ju_j + \alpha_{j+1}u_{j+1} + \alpha_{j+2}u_{j+2} + \cdots + \alpha_{k}u_{k}) + W      \\
				              & =  ((\alpha_1u_1 + \alpha_2u_2 + \cdots + \alpha_ju_j) + (\alpha_{j+1}u_{j+1} + \alpha_{j+2}u_{j+2} + \cdots + \alpha_{k}u_{k})) + W \\
				              & = (\alpha_{j+1}u_{j+1} + \alpha_{j+2}u_{j+2} + \cdots + \alpha_{k}u_{k}) + W                                                         \\
				              & = \alpha_{j+1}(u_{j+1} + W) + \alpha_{j+2}(u_{j+2} + W) + \cdots + \alpha_{k}(u_{k} + W)
			            \end{align*}
			            }
			            Since $\alpha_1u_1 + \alpha_2u_2 + \cdots + \alpha_ju_j \in W$, it can be removed
			            because of $\mathrm{W}$ (the fourth line). So, $\beta_{V/W}$ generates $\mathrm{V}/\mathrm{W}$.
		      \end{enumerate}
		      By definition, $\beta_{V/W}$ is a basis for $\mathrm{V}/\mathrm{W}$.
		      Therefore,
		      \begin{align*}
			      dim(\mathrm{V}/\mathrm{W}) & =  k - j                            \\
			                                 & = dim(\mathrm{V}) - dim(\mathrm{W})
		      \end{align*}
	      \fi



	\item{
	      Let $K = F^n$, define $AK = R(L_A)$, and $A^2K = R(L_{A^2})$.
	      Show that $AK/A^2K$ is a vector space of dimension $rank(A) - rank(A^2)$.
	      \begin{proof}
		      We begin by proving that $AK/A^2K$ is a vector space. It is enough to show that $A^2K$ is a subspace of $AK$
		      to prove that $AK/A^2K$ is a vector space, by part (b). It is clear that $AK$ is a subspace because $AK = R(L_A)$
		      and Theorem 2.3.\par
		      We claim that $R(L_{A^2}) \subseteq R(L_A)$.  Let $y \in R(L_{A^2})$.
		      By definition, $\exists x \in F^n : A^2x = y$. We can rewrite $A^2x = y$ as
		      $A(Ax) = y$ so it follows that $y \in R(L_A)$.\par
		      Thus, $A^2K \subseteq AK$ because $R(L_{A^2}) \subseteq R(L_A)$.
		      Because $A^2K = R(L_{A^2})$, $A^2K$ has the properties of a vector space because of Theorem 2.1.
		      It follows from part (c) that
		      \begin{align*}
			      dim(AK/A^2K) & = dim(AK) - dim(A^2K))          \\
			                   & = dim(R(L_A)) - dim(R(L_{A^2})) \\
			                   & = rank(A) - rank(A^2)
		      \end{align*}
		      as desired.
	      \end{proof}
	      }
	\item{
	      Show that $A^2K/A^3K$ is a vector space of dimension $rank(A^2) - rank(A^3)$, where $A^3K= R(L_{A^3})$.
	      \begin{proof}
		      Similiar to what we did in part (d), we can prove that $A^2K/A^3K$ is a vector space.
		      Then it follows that,
		      \begin{align*}
			      dim(A^2K/A^3K) & = dim(A^2K) - dim(A^3K))            \\
			                     & = dim(R(L_{A^2})) - dim(R(L_{A^3})) \\
			                     & = rank(A^2) - rank(A^3)
		      \end{align*}
		      as desired.
	      \end{proof}
	      }
	\item{
	      Define $T: AK/A^2K \rightarrow A^2K/A^3K$, by $T(v) = L_A(v)$, i.e, we left multiply each element of $v$ by the matrix $A$.
	      Show that $R(T) = A^2K/A^3K$.
	      \begin{proof}
		      To show that $R(T) = A^2K/A^3K$, is another way of saying show that $T$ is onto.
		      We must show that $\forall y \in A^2K/A^2K [\exists x \in AK/A^2K : T(x) = y$].
		      Let $y \in A^2K/A^3K$ such that $y = A^2k + A^3K$ where $k \in \mathbb{F}^n$.
		      We claim that $T(x) = y$ where $x = Ak + A^2K$.
		      \begin{align*}
			      T(x) & = T(Ak + A^2K)        \\
			           & = L_A(Ak + A^2K)      \\
			           & = L_A(Ak) + L_A(A^2K) \\
			           & = A^2k + A^3K         \\
			           & = y
		      \end{align*}
		      as desired. Therefore, $T$ is onto i.e. $R(T) = A^2K/A^3K$.
	      \end{proof}
	      }
	\item{
	      Use the rank-nullity theorem on $T$ to conclude that $rank(A^2) - rank(A^3) \leq rank(A) - rank(A^2)$.
	      \begin{proof}
		      We begin by proving that T is linear. We define $x_1 = Ak_1 + A^2K$ and $x_2 = Ak_2 + A^2K$
		      such that $x_1, x_2 \in AK/A^2K$ and $k_1, k_2 \in F^n$ and $c \in F$. We use properties of matrices
		      as outlined in Theorem 2.12.
		      \begin{align*}
			      T(ck_1 + k_2) & = L_A(ck_1 + k_2)                    \\
			                    & = A(ck_1 + k_2)                      \\
			                    & = A(c(Ak_1 + A^2K) + (Ak_2 + A^2K))  \\
			                    & = A(c(Ak_1 + A^2K)) + A(Ak_2 + A^2K) \\
			                    & = c(A(Ak_1 + A^2K)) + A(Ak_2 + A^2K) \\
			                    & = cL_A(k_1)+ L_A(k_2)                \\
			                    & = cT(k_1)+ T(k_2)
		      \end{align*}
		      Since T is linear, we can use rank nullity theorem. From part (d), (e), (f), and assuming that $nullity(T) \geq 0$ because the number of elements of a set cannot be negative
		      \begin{align*}
			      rank(T) + nullity(T)  & = dim(AK/A^2K)           \\
			                            & = rank(A) - rank(A^2)    \\
			      rank(T)               & \leq rank(A) - rank(A^2) \\
			      rank(A^2) - rank(A^3) & \leq rank(A) - rank(A^2)
		      \end{align*}
		      as desired.
	      \end{proof}
	      }
\end{enumerate}

%problem 4
\section{}
Let $\mathrm{V}$ be a finite-dimensional vector space. Let $T$ and $P$ be two linear transformations from $\mathrm{V}$ to itself, such that $T^2= P^2 = 0$, and $T \circ P + P \circ T = I$, where $I$ is the identity in $\mathrm{V}$.
\begin{enumerate}[label=\alph*.]
	\item{
	      Denote $N_T = N(T)$ and $N_P = N(P)$, the null spaces of $T$ and $P$, respectively.
	      Show that $N_P = P(N_T)$, and $N_T = T(N_P)$, where $T(N_P) = \{ T(v): v \in N_P \}$ and $P(N_T) = \{ P(v): v \in N_T \}$.
	      \begin{proof}\
		      \begin{enumerate}[label=\roman*.]
			      \item{
			            Show $N_P = P(N_T)$
			            \begin{enumerate}[label=\arabic*.]
				            \item{
				                  $N_P \subseteq P(N_T)$\par
				                  Let $x \in N_P$. By definition, $P(x) = 0$.
				                  \begin{align*}
					                  P \circ T(x) + T \circ P(x) & = x \\
					                  P \circ T(x) + T(0)         & = x \\
					                  P \circ T(x)                & = x
				                  \end{align*}
				                  $T(x) \in N_T$ because $T \circ T(x) = 0$.
				                  So $x \in P(N_T)$. Thus, $N_p \subseteq P(N_T)$.
				                  }
				            \item{
				                  $P(N_T) \subseteq N_P $\par
				                  Let $P(x) \in P(N_T)$ where $x \in N_T$.\\
				                  $P \circ P(x) = 0$ so $P(x) \in N_P$. Thus, $P(N_T) \subseteq N_P$.
				                  }
			            \end{enumerate}
			            Therfore, $N_P = P(N_T)$.
			            }
			      \item{
			            Show $N_T = T(N_P)$
			            \begin{enumerate}[label=\arabic*.]
				            \item{
				                  $N_T  \subseteq T(N_P)$\par
				                  Let $x \in N_T$. By definition, $T(x) = 0$.
				                  \begin{align*}
					                  P \circ T(x) + T \circ P(x) & = x \\
					                  P(0) + T \circ P(x)         & = x \\
					                  T \circ P(x)                & = x
				                  \end{align*}
				                  Notice that $P(x) \in N_P$, so $x \in T(N_P)$. Thus, $N_T  \subseteq T(N_P)$.
				                  }
				            \item{
				                  $T(N_P) \subseteq N_T$\par
				                  Let $T(x) \in T(N_P)$ where $x \in N_P$.\\
				                  $T \circ T(x) = 0$ so $T(x) \in N_T$. Thus, $T(N_P) \subseteq N_T$.
				                  }
			            \end{enumerate}
			            Therfore, $N_P = P(N_T)$.
			            }
		      \end{enumerate}
	      \end{proof}
	      }
	\item{
	      Show that $\mathrm{V} = N_T \oplus N_P$.
	      \begin{proof}
		      Need to prove the following two conditions.
		      \begin{enumerate}[label=\roman*.]
			      \item{
			            $N_T \cap N_P = \{0\}$.	\par
			            Let $x \in N_T$ and $x \in N_P$. This means that $T(x) = 0$ and $P(x) = 0$.
			            \begin{align*}
				            x & = P \circ T(x) + T \circ P(x) \\
				              & = P(0) + T(0)                 \\
				              & = 0
			            \end{align*}
			            Thus, $N_T \cap N_P = \{0\}$.\\
			            }
			      \item{
			            $N_T + N_P = \mathrm{V}$.
			            \begin{enumerate}[label=\arabic*.]
				            \item{
				                  $N_T + N_P \subseteq \mathrm{V}$\\
				                  Let $x \in N_T + N_P$ such that $x = n_t + n_p$ where $n_t \in N_T \subseteq \mathrm{V}$ and $n_p \in N_P \subseteq \mathrm{V}$
				                  by Theorem 2.11. Since $\mathrm{V}$ is a vector space $n_t + n_p \in V$ by closure under addition.\\
				                  Thus, $ x \in \mathrm{V}$ and it follows that $N_T + N_P \subseteq \mathrm{V}$.
				                  }
				            \item{
				                  $\mathrm{V} \subseteq N_T + N_P $\\
				                  Let $x \in V$. We know that $x = P \circ T(x) + T \circ P(x)$.\\
				                  Notice that $P \circ T(x) \in N_P$ because $P \circ (P \circ T(x)) = 0$.\\
				                  Similarly, $T \circ P(x) \in N_T$ because $T \circ (T \circ P(x)) = 0$.\\
				                  Thus, $N_T + N_P \subseteq \mathrm{V}$.
				                  }
			            \end{enumerate}
			            Therefore, $N_T + N_P = \mathrm{V}$.
			            }
		      \end{enumerate}
		      Therefore, by definition $\mathrm{V} = N_T \oplus N_P$.
	      \end{proof}
	      }
	\item{
	      Prove that the dimension of $\mathrm{V}$ is even.
	      \begin{proof}\
		      \begin{lemma}
			      Let $W_1 + W_2 = V$. Then, $\mathrm{V} = W_1 \oplus W_2 \Leftrightarrow dim(V) = dim(W_1) + dim(W_2)$
			      \begin{proof}
				      We begin by proving that $dim(W_1 + W_2) = dim(W_1) + dim(W_2) - dim(W_1 \cap W_2)$\\
				      Let
				      \[B_{1\cap 2} = \{u_1, u_2, \cdots, u_k\}\]
				      be a basis for $W_1 \cap W_2$\\
				      By using the replacement theorem, we can extend $B_{1\cap 2}$ to be a basis for $W_1$.\\
				      So the basis for $W_1$ is $B_1$
				      \[B_1 = \{u_1, u_2, \cdots, u_k, v_1, v_2, \cdots, v_m\}\]
				      Likewise, we can extend $B_{1\cap 2}$ to be a basis for $W_2$
				      \[B_2 = \{u_1, u_2, \cdots, u_k, w_1, w_2, \cdots, w_p\}\]
				      Basis for $W_1 + W_2$ will be $B_1 \cup B_2$, however they may contain the same vectors twice.\\
				      To prevent double counting, we must subtract $B_1 \cap B_2$ from $B_1 \cup B_2$\\
				      Thus the basis for $W_1 + W_2$ is
				      \[B_{1+2} = \{u_1, u_2, \cdots, u_k, v_1, v_2, \cdots, v_m, w_1, w_2, \cdots, w_p\}\]
				      It follows that
				      \[\text{dim}(W_1 + W_2) = \text{dim}(W_1) + \text{dim}(W_2) - \text{dim}(W_1 \cap W_2)\]
				      Now we continue the proof for the lemma.
				      \begin{enumerate}[label=\roman*.]
					      \item{
					            $V = W_1 \oplus W_2 \Rightarrow dim(V) = dim(W_1) + dim(W_2)$\\
					            From the definition of direct sum, $W_1 \cap W_2 = \{0\}$\\
					            This means $dim(W_1 \cap W_2) = 0$.
					            Hence,
					            \begin{align*}
						            dim(V) & = dim(W_1) + dim(W_2) - dim(W_1 \cap W_2) \\
						                   & = dim(W_1) + dim(W_2) - 0                 \\
						                   & = dim(W_1) + dim(W_2)
					            \end{align*}
					            as desired.
					            }
					      \item{
					            $dim(V) = dim(W_1) + dim(W_2) \Rightarrow V = W_1 \oplus W_2 $\\
					            $V = W_1 \oplus W_2$ if and only if $V = W_1 + W_2 $ and $ W_1 \cap W_2 = \{0\}$\\
					            $V = W_1 + W_2$ is a preconditon for this lemma.\\
					            We proved earlier that, $dim(V) = dim(W_1) + dim(W_2) - dim(W_1 \cap W_2)$\\
					            Our hypothesis is $dim(V) = dim(W_1) + dim(W_2)$\\
					            Setting the two equations equal to each other:
					            \begin{align*}
						            dim(W_1) + dim(W_2) - dim(W_1 \cap W_2) & = dim(W_1) + dim(W_2) \\
						            dim(W_1 \cap W_2)                       & = 0
					            \end{align*}
					            This means $ W_1 \cap W_2 = \{0\}$. Thus, $dim(V) = dim(W_1) + dim(W_2)$.
					            Therefore, by definition $V = W_1 \oplus W_2$.
					            }
				      \end{enumerate}
				      Therefore, if $W_1 + W_2 = V$, then $V = W_1 \oplus W_2 \Leftrightarrow dim(V) = dim(W_1) + dim(W_2)$
			      \end{proof}
		      \end{lemma}
		      %It is obvious that $dim(T(N_P)) \leq dim(N_P)$. From (a), we have proved that
		      %$T(N_P) = N_T$. This means that $dim(N_T) \leq dim(N_P)$.
		      %Likewise, \[dim(P(N_T)) \leq dim(N_T) \Rightarrow dim(N_P) \leq dim(N_T)\]
		      %Thus, $dim(N_P) = dim(N_T)$. 
		      We begin by claiming that $dim(T(N_P)) \leq dim(N_P)$. Let $\beta = \{v_1, v_2, \cdots, v_n\}$
		      be a basis for $N_P$. We want to show that $\gamma = \{T(v_1), T(v_2), \cdots, T(v_n)\}$
		      spans $T(N_P)$ so that the above statement holds. Let $w \in T(N_P)$. This means that
		      $\exists x \in N_P : T(x) = w$. We can write $x$ as a linear combination of $\beta$.
		      $\exists c_1, \cdots, c_n \in F$ such that $x = c_1v_1 + c_2v_2 + \cdots + c_nv_n$ \par
		      Applying $T$ to both sides of this equation,
		      \begin{align*}
			      x    & = c_1v_1 + c_2v_2 + \cdots + c_nv_n          \\
			      T(x) & = T(c_1v_1 + c_2v_2 + \cdots + c_nv_n)       \\
			           & = T(c_1v_1) + T(c_2v_2) + \cdots + T(c_nv_n) \\
			           & = c_1T(v_1) + c_2T(v_2) + \cdots + c_nT(v_n)
		      \end{align*}
		      Notice that $w = T(x)$. Since $w$ was picked arbitrarily, $\gamma$ spans $T(N_P)$. It directly
		      follows that $dim(T(N_P)) \leq n$ because $dim(T(N_P))$ is only as large as its basis which is a minimal
		      spanning set. Therefore, $dim(T(N_P)) \leq dim(N_P)$. Without loss of generality, we can show that
		      $dim(P(N_T)) \leq dim(N_T)$ \par
		      It follows from the claim above and from (a) that
		      \begin{align*}
			      dim(T(N_P)) & \leq dim(N_P) \\
			      dim(N_T)    & \leq dim(N_P) \\
			                  &               \\
			      dim(P(N_T)) & \leq dim(N_T) \\
			      dim(N_P)    & \leq dim(N_T)
		      \end{align*}
		      This means that $dim(N_P) = dim(N_T)$ \par

		      By using the lemma above,
		      \begin{align*}
			      dim(V) & = dim(N_T) + dim(N_P)                       \\
			             & = dim(N_T) + dim(N_T) = dim(N_P) + dim(N_P) \\
			             & = 2dim(N_T) = 2dim(N_P)
		      \end{align*}
		      as desired. Therefore, $dim(V)$ is even.
	      \end{proof}
	      }
	\item{
	      Suppose that the dimension of $\mathrm{V}$ is two. Prove that $\mathrm{V}$ has a basis $\beta$, such that
	      \begin{equation}
		      [T]_{\beta} = \left [ \begin{array}{cc}
				      0 & 1 \\
				      0 & 0
			      \end{array} \right] \qquad \text{ and } \qquad [P]_{\beta} = \left [ \begin{array}{cc}
				      0 & 0 \\
				      1 & 0
			      \end{array} \right] .
	      \end{equation}
	      \begin{proof}
		      We claim that $T^2 = P^2 = 0$ implies that $T$ and $P$ are not invertible.

		      Assume for the sake of contradiction that $T$ had an inverse $U$.
		      Then \[UT = I \Rightarrow UT(T) = IT \Rightarrow UT^2 = T \Rightarrow U0 = T \Rightarrow T=0\]
		      which is a contradiction because the zero matrix
			  is not left invertible. Therefore, $T$ is not invertible and without loss of generality, 
			  $P$ is not invertible. \par

		      This means that $N_T,N_P$ are
		      nontrivial null spaces. From (b), we know that $V = N_T \oplus N_P$. Asumming that $dim(V) = 2$,
		      we get that $2 = dim(N_T) + dim(N_P)$ by our lemma from (c). Since $N_T,N_P$ are nontrivial null spaces,
		      this necessarily means that $dim(N_T) = dim(N_P) = 1$.\par

		      Since $N_P$ is non trival, there exists a nonzero vector in $N_P$. Let $v \in N_P$ be nonzero. Since
		      $dim(N_P) = 1$, any nonzero vector forms a basis for $N_P$ So, $\{v\}$ is a basis for $N_P$.\par

		      Since $V = N_T \oplus N_P$, by definition $N_T \cap N_P = \{0\}$. So $v \notin N_T$. Thus, $T(v) \neq 0$.
		      We know that $T(v) \in N_T$, is nonzero, and that $dim(N_T) = 1$, so $\{T(v)\}$ is a basis for $N_T$.\par

		      We claim that $\beta = \{T(v), v\}$ is a basis for $V$. Notice that $|\beta| = dim(V)$ and $T(v), v \in V$.
		      So, it is enough to show that $\beta$ is linearly independent for $\beta$ to be a basis for $V$
		      by Theorem 1.10 Corollary 2(b). Let $c_1, c_2 \in F$.
		      \begin{align*}
			      c_1T(v) +  c_2v & = 0        \\
			      c_1T(v)         & = - c_2v   \\
			      T(c_1T(v))      & = T(-c_2v) \\
			      c_1T(T(v))      & = -c_2T(v) \\
			      c_10            & = -c_2T(v) \\
			      0               & = -c_2T(v) \\
			      \frac{0}{T(v)}  & = -c_2     \\
			      c_1             & = c_2 = 0
		      \end{align*}
		      as desired. $\beta$ is a basis for $V$. \par
		      Now we can compute $[T]_{\beta}$ and $[P]_{\beta}$.
		      \begin{align*}
			      [T]_{\beta} & = \Big[[T(T(v))]_{\beta} \; [T(v)]_{\beta}\Big] \\
			                  & = \Big[[0]_{\beta} \; [T(v)]_{\beta}\Big]       \\
			                  & = \left [ \begin{array}{cc}
					      0 & 1 \\
					      0 & 0
				      \end{array} \right]
		      \end{align*}
		      We claim that $P(T(v)) = v$. Given that $T \circ P + P \circ T = I$,
		      \begin{align*}
			      P(T(v)) + T(P(v)) & = v           \\
			      P(T(v))           & = v - T(P(v)) \\
			                        & = v
		      \end{align*}
		      because $v \in N_P$.
		      We finish with computing $[P]_{\beta}$.
		      \begin{align*}
			      [P]_{\beta} & = \Big[[P(T(v))]_{\beta} \; [P(v)]_{\beta}\Big] \\
			                  & = \Big[[v]_{\beta} \;  [0]_{\beta}\Big]         \\
			                  & = \left [ \begin{array}{cc}
					      0 & 0 \\
					      1 & 0
				      \end{array} \right]
		      \end{align*}
		      as desired.
	      \end{proof}
	      }
\end{enumerate}

\end{document}
%dab on this midterm