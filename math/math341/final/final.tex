\documentclass[11pt]{scrartcl}
\usepackage[sexy]{../../evan}
\usepackage{cmbright}
\usepackage{cancel}
\usepackage[T1]{fontenc}
%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
%\usepackage[pdfborder={0 0 0},colorlinks=true,citecolor=red{}]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{systeme}

\title{Math 341: Final}
\author{Daniel Ko}
\date{Spring 2020}

\begin{document}
\maketitle

%problem 1
\section{}
For $c \in \mathbb{R},$ we define the matrix $\mathbf{A}_{\mathbf{c}} \in \mathbb{R}^{3 \times 3}$ by
$$
	\mathbf{A}_{\mathbf{c}}=\begin{bmatrix}
		1 & -1 & 1 \\
		2 & 2  & 0 \\
		3 & c  & 2
	\end{bmatrix}
$$
\begin{enumerate}[label=\alph*.]
	\item {
	      Compute $\det(\mathbf{A}_{\mathbf{c}})$. Does it depend of $c$?
	      \begin{proof}
		      \begin{align*}
			      \det(\mathbf{A}_{\mathbf{c}}) & =1 \cdot \det \begin{bmatrix}2&0\\ c&2\end{bmatrix}-\left(-1\right)\det \begin{bmatrix}2&0\\ 3&2\end{bmatrix}+1\cdot \det \begin{bmatrix}2&2\\ 3&c\end{bmatrix} \\
			                                    & = (4) + (4) + (2c-6)                                                                                                        \\
			                                    & = 2c + 2
		      \end{align*}
		      Yes, it depends on $c$.
	      \end{proof}
	      }
	\item{
	      For which $c$ is the matrix $\mathbf{A}_{\mathbf{c}}$ invertible?
	      \begin{proof}
		      Corollary to Theorem 4.7 states that a matrix is invertible if and only its determinant does not equal 0.
		      From (a),
		      \begin{align*}
			      \det(\mathbf{A}_{\mathbf{c}}) = 2c + 2 & = 0  \\
			      c                                      & = -1
		      \end{align*}
		      $\det(\mathbf{A}_{\mathbf{c}}) = 0$ when $c = -1$. Hence, $\mathbf{A}_{\mathbf{c}}$ is invertible
		      for all $c$ except $c = -1$.
	      \end{proof}
	      }
	\item{
	      Compute $\mathbf{A}_{\mathbf{0}}^{-1}$ (i.e. when $c = 0$).
	      \begin{proof}
		      Using the proof to Theorem 3.2, we compute the inverse by constructing an agumented matrix
		      $(\mathbf{A}_{\mathbf{0}}|I_3)$ and applying elementary row operations to transform it into
		      the form of $(I_3|\mathbf{A}_{\mathbf{0}}^{-1})$.
		      \begin{align*}
			      (\mathbf{A}_{\mathbf{0}}|I_3)                                         & =
			      \left[\begin{array}{@{}ccc|ccc@{}}
					      1 & -1 & 1 & 1 & 0 & 0 \\
					      2 & 2  & 0 & 0 & 1 & 0 \\
					      3 & 0  & 2 & 0 & 0 & 1
				      \end{array}\right]                                    \\
			      R_2 \leftarrow R_2 - 2R_1 \quad R_3 \leftarrow R_3 - 3R_1             & =
			      \left[\begin{array}{@{}ccc|ccc@{}}
					      1 & -1 & 1  & 1  & 0 & 0 \\
					      0 & 4  & -2 & -2 & 1 & 0 \\
					      0 & 3  & -1 & -3 & 0 & 1
				      \end{array}\right]                                    \\
			      R_1 \leftarrow R_1 + \frac14R_2 \quad R_3 \leftarrow R_3 - \frac34R_2 & =
			      \left[\begin{array}{@{}ccc|ccc@{}}
					      1 & 0 & \frac12 & \frac12  & \frac14  & 0 \\
					      0 & 4 & -2      & -2       & 1        & 0 \\
					      0 & 0 & \frac12 & -\frac32 & -\frac34 & 1
				      \end{array}\right]                                    \\
			      R_1 \leftarrow R_1 - R_3 \quad R_2 \leftarrow R_2 + 4R_3              & =
			      \left[\begin{array}{@{}ccc|ccc@{}}
					      1 & 0 & 0       & 2        & 1        & -1 \\
					      0 & 4 & 0       & -8       & -2       & 4  \\
					      0 & 0 & \frac12 & -\frac32 & -\frac34 & 1
				      \end{array}\right]                                    \\
			      R_2 \leftarrow \frac14 R_2 \quad R_3 \leftarrow 2R_3                  & =
			      \left[\begin{array}{@{}ccc|ccc@{}}
					      1 & 0 & 0 & 2  & 1        & -1 \\
					      0 & 1 & 0 & -2 & -\frac12 & 1  \\
					      0 & 0 & 1 & -3 & -\frac32 & 2
				      \end{array}\right]
		      \end{align*}
		      Therefore,
		      $$
			      \mathbf{A}_{\mathbf{0}}^{-1} =
			      \begin{bmatrix}
				      2  & 1        & -1 \\
				      -2 & -\frac12 & 1  \\
				      -3 & -\frac32 & 2
			      \end{bmatrix}
		      $$
	      \end{proof}
	      }
	\item{
	      Let $b = (1, -4, 2)^t$, find the solution of $\mathbf{A}_{\mathbf{0}} x = b$

	      \begin{proof}
		      Let $x = (x_1, x_2, x_3)^t$.
		      \begin{align*}
			      \mathbf{A}_{\mathbf{0}} x & = b \\
			      \begin{bmatrix}
				      1 & -1 & 1 \\
				      2 & 2  & 0 \\
				      3 & 0  & 2
			      \end{bmatrix}
			      \begin{bmatrix}
				      x_1 \\ x_2\\ x_3
			      \end{bmatrix}
			                                & =
			      \begin{bmatrix}
				      1 \\ -4 \\ 2
			      \end{bmatrix}
		      \end{align*}
		      $$\systeme{
				      x_1 - x_2 + x_3 = 1,
				      2x_1 + 2x_2 = -4,
				      3x_1 +2x_3 = 2
			      }$$
		      \begin{align*}
			      x_1 & = -4 \\
			      x_2 & = 2  \\
			      x_3 & = 7
		      \end{align*}
	      \end{proof}

	      }
	\item{
	      Compute $\det(\mathbf{A}_{\mathbf{c}}^2)$
	      \begin{proof}
		      From Theorem 4.7, we know that
		      \begin{align*}
			      \det(\mathbf{A}_{\mathbf{c}}^2) & = \det(\mathbf{A}_{\mathbf{c}}\mathbf{A}_{\mathbf{c}})              \\
			                                      & = \det(\mathbf{A}_{\mathbf{c}}) \cdot \det(\mathbf{A}_{\mathbf{c}}) \\
			                                      & = (2c + 2) (2c + 2)                                                 \\
			                                      & = 4c^2+8c+4
		      \end{align*}
	      \end{proof}
	      }
	\item{
	      Compute $\det(5\mathbf{A}_{\mathbf{c}})$
	      \begin{proof}
		      Using the second "Properties of the Determinant" on pg 234,
		      \begin{align*}
			      \det(5\mathbf{A}_{\mathbf{c}}) & = 5^3\det(\mathbf{A}_{\mathbf{c}}) \\
			                                     & = 5^3(2c + 2)                      \\
			                                     & = 250c + 250
		      \end{align*}
	      \end{proof}
	      }
	\item{
	      Compute $\det(\mathbf{E}_{\mathbf{k}}\mathbf{A}_{\mathbf{c}})$ where,
	      $$
		      \mathbf{E}_{\mathbf{k}} = \begin{bmatrix}
			      1 & 0 & 0 \\
			      k & 1 & 0 \\
			      0 & 0 & 1
		      \end{bmatrix}
	      $$
	      \begin{proof}
		      \begin{align*}
			      \det(\mathbf{E}_{\mathbf{k}}) & = 1\cdot \det \begin{bmatrix}1&0\\ 0&1\end{bmatrix}-0\cdot \det \begin{bmatrix}k&0\\ 0&1\end{bmatrix}+0\cdot \det \begin{bmatrix}k&1\\ 0&0\end{bmatrix} \\
			                                    & = 1
		      \end{align*}
		      From Theorem 4.7, we know that,
		      \begin{align*}
			      \det(\mathbf{E}_{\mathbf{k}}\mathbf{A}_{\mathbf{c}}) & = \det(\mathbf{E}_{\mathbf{k}}) \det(\mathbf{A}_{\mathbf{c}}) \\
			                                                           & = (1) (2c + 2)                                                \\
			                                                           & = 2c + 2
		      \end{align*}
	      \end{proof}
	      }

	\item{
	      Compute $\det(\mathbf{D}_{\mathbf{k}}\mathbf{A}_{\mathbf{c}})$ where,
	      $$
		      \mathbf{D}_{\mathbf{k}} = \begin{bmatrix}
			      1 & 0 & 0 \\
			      0 & k & 0 \\
			      0 & 0 & 1
		      \end{bmatrix}
	      $$
	      \begin{proof}
		      \begin{align*}
			      \det(\mathbf{D}_{\mathbf{k}}) & = 1\cdot \det \begin{bmatrix}k&0\\ 0&1\end{bmatrix}-0\cdot \det \begin{bmatrix}0&0\\ 0&1\end{bmatrix}+0\cdot \det \begin{bmatrix}0&k\\ 0&0\end{bmatrix} \\
			                                    & = k
		      \end{align*}
		      From Theorem 4.7, we know that,
		      \begin{align*}
			      \det(\mathbf{E}_{\mathbf{k}}\mathbf{A}_{\mathbf{c}}) & = \det(\mathbf{D}_{\mathbf{k}}) \det(\mathbf{A}_{\mathbf{c}}) \\
			                                                           & = (k) (2c + 2)                                                \\
			                                                           & = 2kc + 2k
		      \end{align*}
	      \end{proof}
	      }
	\item{
	      Compute $\det(\mathbf{A}_{\mathbf{0}}^{-1})$
	      \begin{proof}
		      From (c) we know that,
		      $$
			      \mathbf{A}_{\mathbf{0}}^{-1} =
			      \begin{bmatrix}
				      2  & 1        & -1 \\
				      -2 & -\frac12 & 1  \\
				      -3 & -\frac32 & 2
			      \end{bmatrix}
		      $$
		      It follows that,
		      \begin{align*}
			      \det(\mathbf{A}_{\mathbf{0}}^{-1}) & = 2\cdot \det \begin{bmatrix}-\frac{1}{2}&1\\ -\frac{3}{2}&2\end{bmatrix}-1\cdot \det \begin{bmatrix}-2&1\\ -3&2\end{bmatrix}-1\cdot \det \begin{bmatrix}-2&-\frac{1}{2}\\ -3&-\frac{3}{2}\end{bmatrix} \\
			                                         & = 2 (\frac{1}{2}) -1 (-1) -1 (\frac{3}{2})                                                                             \\
			                                         & = \frac12
		      \end{align*}
	      \end{proof}
	      }
	\item{
	      Compute the eigenvalues of $\mathbf{A}_{\mathbf{0}}$. Can you diagonalize $\mathbf{A}_{\mathbf{0}}$?
	      \begin{proof}
		      Using Theorem 5.2, we compute the eigenvalues of $\mathbf{A}_{\mathbf{0}}$ by computing its
		      characteristic polynomial.
		      \begin{align*}
			      \det(\mathbf{A}_{\mathbf{0}} - \lambda I) & = \det \begin{bmatrix}
				      1 - \lambda & -1          & 1           \\
				      2           & 2 - \lambda & 0           \\
				      3           & 0           & 2 - \lambda
			      \end{bmatrix}         \\
			                                                & = -(\lambda -2)(\lambda^2 - 3 \lambda +1)
		      \end{align*}
		      We compute when $-(\lambda -2)(\lambda^2 - 3 \lambda +1) = 0$.
		      When $\lambda = 2$, the characteristic polynomial equals $0$.
		      Using the quadratic formula, when $\lambda = \frac{3 \pm \sqrt{5}}{2}$, the characteristic polynomial equals $0$.
		      Hence the eigenvalues of $\mathbf{A}_{\mathbf{0}}$ are $$2, \frac{3 + \sqrt{5}}{2}, \frac{3 - \sqrt{5}}{2}$$
		      We can rewrite the characteristic polynomial as
		      $$ f(\lambda) = -(\lambda -2)(\lambda - \frac{3 + \sqrt{5}}{2})(\lambda - \frac{3 - \sqrt{5}}{2})$$
		      Using the "Test for Diagonalization" outlined in pg 269,  we determine if
		      $\mathbf{A}_{\mathbf{0}}$ can be diagonalized.
		      It is clear that the first condition, the characteristic polynomial of T splits, holds.
		      By Theorem 5.7, condition 2 is automatically satisfied for eigenvalues
		      of multiplicity 1. Therefore, A is diagonalizable.
	      \end{proof}
	      }
	\item{
	      Compute the eigenvalues of $\mathbf{A}_{\mathbf{0}}^{-1}$
	      \begin{proof}
		      Notice that since eigenvalues are non zero,
		      \begin{align*}
			      \mathbf{A}_{\mathbf{0}} x                              & = \lambda x                             \\
			      \mathbf{A}_{\mathbf{0}}^{-1} \mathbf{A}_{\mathbf{0}} x & = \mathbf{A}_{\mathbf{0}}^{-1}\lambda x \\
			      x                                                      & = \mathbf{A}_{\mathbf{0}}^{-1}\lambda x \\
			      \frac{1}{\lambda}x                                     & = \mathbf{A}_{\mathbf{0}}^{-1} x
		      \end{align*}
		      Thus the eigenvalues of $\mathbf{A}_{\mathbf{0}}^{-1}$ are
		      $$\frac12, \frac{3 + \sqrt{5}}{2}, \frac{3 - \sqrt{5}}{2}$$
	      \end{proof}
	      }
\end{enumerate}


\section{}
Consider the transformation $T: \mathbb{R}^{3} \rightarrow \mathbb{R}^{4}$ given by
$$
	T(\mathbf{x})=\left(\begin{array}{c}
			x_{2}-x_{1}+\alpha\left(x_{2}^{2}+x_{3}^{3}\right) \\
			x_{1}+x_{2}+2 g x_{3}+\alpha x_{1}^{2}             \\
			x_{1}+x_{2}+2 x_{3}                                \\
			h x_{3}+q
		\end{array}\right), \text { where } \mathbf{x}=\left(\begin{array}{c}
			x_{1} \\
			x_{2} \\
			x_{3}
		\end{array}\right)
$$
in which, $h, g, q$ and $\alpha$ are real numbers.

\begin{enumerate}[label=\alph*.]
	\item {
	      What are the condition on $\alpha$ and $q$ such that the transformation is linear?
	      Explain briefly.
	      \begin{proof}
		      We know that if $T$ is linear then $T(0) = 0$. Hence $q = 0$ for $T$ to be linear.
		      Notice that we must not have any terms higher than degree 1 because for example
		      if we have a transformation $U(x) = x^2$ and let $c \in \mathbb{R}$ then,
		      \begin{align*}
			      U(cx) & = (cx)^2   \\
			            & = c^2x^2   \\
			            & \neq cU(x)
		      \end{align*}
		      which means $U$ is not a linear transformation.
		      Thus, $\alpha = 0$ so that there are no terms higher than degree 1.
	      \end{proof}
	      }
	\item{
	      From now we suppose that $q = 0$ and $\alpha = 0$. Write the representation
	      matrix $A = [T]_{\epsilon_3}^{\epsilon_4}$.
	      \begin{proof}
		      We now define
		      $$
			      T(\mathbf{x})=\left(\begin{array}{c}
					      x_{2}-x_{1}           \\
					      x_{1}+x_{2}+2 g x_{3} \\
					      x_{1}+x_{2}+2 x_{3}   \\
					      h x_{3}
				      \end{array}\right), \text { where } \mathbf{x}=\left(\begin{array}{c}
					      x_{1} \\
					      x_{2} \\
					      x_{3}
				      \end{array}\right)
		      $$
		      We now compute the representation matrix
		      $$
			      T(e_1)  = \left(\begin{array}{c}-1\\1\\1\\0\end{array}\right) \quad
			      T(e_2)  = \left(\begin{array}{c}1\\1\\1\\0\end{array}\right) \quad
			      T(e_3)  = \left(\begin{array}{c}0\\2g\\2\\h\end{array}\right)
		      $$
		      \begin{align*}
			      A & = [[T(e_1)]_{\epsilon_4}[T(e_2)]_{\epsilon_4}[T(e_3)]_{\epsilon_4}] \\
			        & = \begin{bmatrix}
				      -1 & 1 & 0  \\
				      1  & 1 & 2g \\
				      1  & 1 & 2  \\
				      0  & 0 & h
			      \end{bmatrix}
		      \end{align*}
	      \end{proof}
	      }
	\item{
	      What are the conditions on $h$ and $g$ such that the transformation $T$ maps $\mathbb{R}^3$
	      onto $\mathbb{R}^4$? Explain briefly.
	      \begin{proof}
		      Since $T : \mathbb{R}^3 \rightarrow \mathbb{R}^4$, there are no $h$ and $g$ such that $T$ is onto.
		      This is because $T$ is onto if and only if the range of $T$ equals the codomain, but in this
		      linear transformation, the dimension of codomain is greater than the domain so by rank nullity,
		      $T$ cannot be onto.
	      \end{proof}
	      }
	\item{
	      What are the conditions on $h$ and $g$ such that the transformation $T$ is one to one? Explain briefly.
	      \begin{proof}
		      By Theorem 2.4, $T$ is one to one if and only if $N(T) = \{0\}$. Using the rank nullity theorem,
		      this is equivalent to saying that $rank(T) = 3$, i.e.
		      \begin{align*}
			      dim(\mathbb{R}^3) & = rank(T) + nullity(T) \\
			      3                 & = rank(T)
		      \end{align*}
		      We know that $rank(T) = rank([T]_{\epsilon_3}^{\epsilon_4})$ by Theorem 3.3.
		      Additionally by Theorem 3.5., the rank of a matrix is the number of linearly independent columns.
		      It is clear that $A$ will have rank 3 if $h \neq 0 $. There are no conditions for $g$.
	      \end{proof}
	      }
	\item{
	      Suppose that $h = 0$, then
	      $$A =
		      \begin{bmatrix}
			      -1 & 1 & 0  \\
			      1  & 1 & 2g \\
			      1  & 1 & 2  \\
			      0  & 0 & 0
		      \end{bmatrix}
		      \text{ and let }
		      b =
		      \begin{bmatrix}
			      0 \\
			      2 \\
			      r \\
			      0
		      \end{bmatrix}
	      $$
	      What are the conditions on $r$ and $g$ such that the system $Ax = b$ has a solution? When
	      is the solution unique?
	      \begin{proof}
		      We begin by putting the augmented matrix $(A|b)$ in its reduced form.
		      $$(A'|b') = \left[\begin{array}{@{}ccc|c@{}}-1&1&0&0\\ 0&2&2g&2\\ 0&0&2-2g&r-2\\ 0&0&0&0\end{array}\right]$$
		      By Theorem 3.11 and 3.13, a system is consistent if and only if $rank (A') = rank (A'|b') .$
		      So for the system to have as solution, $2 - 2g \neq 0$ if $r - 2 \neq 0$.	If $r - 2 = 0$, there is no conditions
		      on $g$.\par
		      In summary, for the system to have as solution if $r \neq 2$ then $g \neq 1$. If $r = 2$, there are no conditions
		      on $g$.\par
		      Combining Theorem 3.10 and the corollary to Theorem 4.7, we get that a system has a
		      unique solution if and only if $\det(A) \neq 0$.
		      The determinant of an upper triangular matrix is the product of its diagonal
		      entries by property 4 of determinants in section 4.4.
		      Using this fact we can compute the condition of $g$ as such that
		      \begin{align*}
			      -1*2*(2-2g) & \neq 0 \\
			      4g -4       & \neq 0 \\
			      g           & \neq 1
		      \end{align*}
		      There is no conditions for $r$ when the solution is unique.
	      \end{proof}
	      }
	\item{
	      Suppose that $h = 0, g = 1, r = 2$. Solve $\mathbf{Ax=b}$ and give the answer
	      in parametric form.
	      \begin{proof}
		      We define $$\mathbf{A} =
			      \begin{bmatrix}
				      -1 & 1 & 0 \\
				      1  & 1 & 2 \\
				      1  & 1 & 2 \\
				      0  & 0 & 0
			      \end{bmatrix}
			      \quad
			      \mathbf{b} =
			      \begin{bmatrix}
				      0 \\
				      2 \\
				      2 \\
				      0
			      \end{bmatrix}
		      $$
		      We can compute a solution space to $\mathbf{Ax=b}$ as outlined in Theorem 3.9. We start by first computing the
		      solution set to $\mathbf{Ax}=0$ denoted by $K_H$. It is clear that $rank(\mathbf{A}) = 2$ because the first two
		      columns are linearly independent and the third column is the sum of the first two columns.
		      By Theorem 3.8, $dim(K_H) = 3 - 2 = 1$. Thus any nonzero solution constitutes a basis for K. For example, since
		      $$
			      \begin{pmatrix}
				      1  \\
				      1  \\
				      -1 \\
				      0
			      \end{pmatrix}
		      $$
		      is a solution to the $\mathbf{Ax}=0$, it is a basis for $K_H$ by Corollary 2 of Theorem 1.10.
		      So a solution set to $K_H$ would be
		      \[
			      K_H=
			      \left\{
			      t\begin{pmatrix}
				      1  \\
				      1  \\
				      -1 \\
				      0
			      \end{pmatrix}: t \in \mathbb{R}
			      \right\}
		      \]
		      A solution to $\mathbf{Ax=b}$ is
		      $$
			      \begin{pmatrix}
				      1 \\
				      1 \\
				      0 \\
				      0
			      \end{pmatrix}
		      $$
		      Therefore, by Theorem 3.9 we compute the solution space as
		      \[
			      K=
			      \left\{
			      \begin{pmatrix}
				      1 \\
				      1 \\
				      0 \\
				      0
			      \end{pmatrix}+
			      t\begin{pmatrix}
				      1  \\
				      1  \\
				      -1 \\
				      0
			      \end{pmatrix}: t \in \mathbb{R}
			      \right\}
		      \]
	      \end{proof}
	      }
\end{enumerate}

\section{}
Let $T$ and $U$ be positive semidefinite operators on an inner product space $V$. Prove the following
results
\begin{enumerate}[label=\alph*.]
	\item {
	      $T + U$ is positive semidefinite.
	      \begin{proof}
		      By definition,  $\langle T(x),x \rangle \geq 0$, $\langle U(x),x \rangle > 0$ for all $x \neq 0$
		      \begin{align*}
			      \langle (T + U)(x),x \rangle & = \langle T(x) + U(x),x \rangle                     \\
			                                   & = \langle T(x) ,x \rangle  + \langle U(x),x \rangle \\
			                                   & \geq 0
		      \end{align*}
		      $T + U$ is self adjoint because
		      \begin{align*}
			      (T + U)^* & = T^* + U^* \\
			                & = T + U
		      \end{align*}
		      Therefore, $T + U$ is positive semidefinite.
	      \end{proof}
	      }
	\item{
	      If $c > 0$, then $cI + T$ is positive definite, where $I$ is the identity transformation.
	      \begin{proof}
		      By definition, $\langle T(x),x \rangle \geq 0$, $\langle x,x \rangle > 0$ for all $x \neq 0$
		      \begin{align*}
			      \langle (cI + T)(x),x \rangle & = \langle cI(x) + T(x),x \rangle                      \\
			                                    & = \langle cI(x) ,x \rangle  + \langle  T(x),x \rangle \\
			                                    & = c \langle x ,x \rangle  + \langle  T(x),x \rangle   \\
			                                    & > 0
		      \end{align*}
		      $cI + T$ is self adjoint because
		      \begin{align*}
			      (cI + T)^* & = (cI)^* + T^*          \\
			                 & = \overline{c}I^* + T^* \\
			                 & = cI + T
		      \end{align*}
		      Therefore, $cI + T$ is positive definite.
	      \end{proof}
	      }
	\item{
	      $(cI + T)^{-1}$ is positive definite. From (b) we know that $cI + T$ is self adjoint because. It follows that
	      \begin{proof}
		      Let $ y = (cI + T)^{-1}(x)$.
		      \begin{align*}
			      \langle (cI + T)^{-1}(x),x \rangle & = \langle y,(cI + T)(y) \rangle      \\
			                                         & = \langle (cI + T)^* (y), y  \rangle \\
			                                         & = \langle (cI + T)(y), y  \rangle    \\
			                                         & > 0
		      \end{align*}
		      $(cI + T)^{-1}$ is self adjoint because
		      \begin{align*}
			      ((cI + T)^{-1})^* & = ((cI + T)^{*})^{-1} \\
			                        & = (cI + T)^{-1}
		      \end{align*}
		      Therefore, $(cI + T)^{-1}$ is positive definite.
	      \end{proof}
	      }
\end{enumerate}

\section{}
Let $V$ be a finite-dimensional inner product space. Suppose that U is a partial isometry of $W$
on $V$, where $W$ is a subspace of $V$, and let $\{v_1, v_2, \cdots , v_k\}$ be an orthonormal basis for $W$.

\begin{enumerate}[label=\alph*.]
	\item {
	      Show that $\{U(v_1), U(v_2), \cdots , U(v_k)\}$ is an orthonormal basis for $R(U)$
	      \begin{proof}
		      %We claim that $R(U) = U(W)$.\par
		      By definition $||U(w)|| = ||w||$ for all $w \in W$. By Theorem 6.18 (b), we know that
		      $\langle U(w_1), U(w_2) \rangle = \langle w_1, w_2 \rangle$ where $w_1, w_2 \in W$.\par
		      Notice that $\langle U(v_i), U(v_j) \rangle =\langle v_i, v_j \rangle$ which means that
		      $\{U(v_1), U(v_2), \cdots , U(v_k)\}$ is an orthonormal basis for $U(W)$ because
		      $\{v_1, v_2, \cdots , v_k\}$ is an orthonormal basis and the fact that $U$ is a partial isometry. \par
		      Additionally, suppose $w \in W$ and $w' \in W^\perp$. Then by definition of partial isometry we know
		      that $U(w + w') = w$. This implies that $U(W) = R(U)$.\par
		      Therefore, since $\{U(v_1), U(v_2), \cdots , U(v_k)\}$ is an orthonormal basis for $U(W)$,
		      it is an orthonormal basis for $R(U)$,
		      %$\{U(v_1), U(v_2), \cdots , U(v_k)\}$
	      \end{proof}

	      }
	\item{
	      Show that there exists an orthonormal basis $\gamma$ for $V$ such that the first k
	      columns of $[U]_\gamma$ form an orthonormal set and the remaining columns are zero.
	      \begin{proof}
		      Suppose the dimension of $V$ is n.
		      Using Corollary 2 of Theorem 1.10 (Replacement Theorem), we can extend
		      the orthonormal basis for $W$ from (a) $\{v_1, v_2, \cdots , v_k\}$ to
		      orthonormal basis for $V$, denoted as
		      $\gamma = \{v_1, v_2, \cdots ,v_k, v_{k+1}, \cdots, v_{n-1}, v_n\}$.
		      Notice that $U(v_i) \neq 0$ where $1 \leq i \leq k$ by (a). Since
		      $\gamma$ is an orthonormal basis, $v_i \in W^\perp \Rightarrow U(v_i) = 0$ where $k+1 \leq i \leq n$.\par
		      Therefore, there exists an orthonormal basis $\gamma$ for $V$ such that the first k
		      columns of $[U]_\gamma$ form an orthonormal set and the remaining columns are zero.
	      \end{proof}
	      }
	\item{
	      Let $\left\{w_{1}, w_{2}, \cdots, w_{j}\right\}$ be an orthonormal basis for $\mathrm{R}(\mathrm{U})^{\perp}$ and
	      $$
		      \beta=\left\{\mathrm{U}\left(v_{1}\right), \mathrm{U}\left(v_{2}\right), \cdots, \mathrm{U}\left(v_{k}\right), w_{1}, \cdots, w_{j}\right\}
	      $$
	      Show that $\beta$ is an orthonormal basis for $\mathrm{V}$
	      \begin{proof}\
		      \begin{lemma}
			      Let $V$ be an inner product space and $W$ be a finite-dimensional subspace of V.
			      Then, $V = W \oplus W^\perp$
			      \begin{proof}
				      From Theorem 6.6, $V = W + W^\perp$. \par
				      Let $x \in W \cap W^\perp$. Since $x \in W^\perp$, $\langle x, g \rangle = 0$ for any $g \in W$.
				      Since $x \in W$, this means that $\langle x, x \rangle = 0$. Thus, $x = 0$ and $ W \cap W^\perp = \{0\}$.\par
				      Therefore, $V = W \oplus W^\perp$.
			      \end{proof}
		      \end{lemma}
		      Since $R(U) \subseteq V$, by by our lemma we know that $V = R(U) \oplus R(U)^\perp$.
		      From (a), we know that
		      $\{\mathrm{U}\left(v_{1}\right), \mathrm{U}\left(v_{2}\right)\}$ is a basis for $R(U)$.
		      It directly follows that $\beta$ is an orthonormal basis for $\mathrm{V}$.
	      \end{proof}
	      }
	\item{
	      Show that $U^{*}$ is a partial isometry.
	      \begin{proof}
		      We want to show that there exists a subspace $X$ of $V$ such that
		      $||U^*(x)|| = ||x||$ for all $x \in X$ and $U(x) = 0$ for all $x \in X^\perp$.\par
		      We claim that $X = R(U)$. Let $x \in R(U)$. We can express $x$ as $\sum_{i = 1}^{k}c_i U(v_i)$. It follows that,
		      \begin{align*}
			      ||U^*(x)|| & = ||U^*(\sum_{i = 1}^{k}c_i U(v_i))|| \\
			                 & = ||\sum_{i = 1}^{k}c_i U^*(U(v_i))|| \\
			                 & = ||\sum_{i = 1}^{k}c_i v_i||         \\
			                 & = ||\sum_{i = 1}^{k}c_i U(v_i)||      \\
			                 & = || x ||
		      \end{align*}
		      by Theorem 6.18 and the fact that $U$ is a partial isometry, i.e. $||U(v_i)|| = ||v_i||$.\par
		      Let $x \in R(U)^\perp$. We can express $x$ as $\sum_{i = 1}^{j} \langle x, w_i \rangle w_i$ from (c) and Theorem 6.5.
		      \begin{align*}
			      U^*(x) & = U^*(\sum_{i = 1}^{j}\langle x, w_i \rangle w_i) \\
			             & = \sum_{i = 1}^{j}\langle U^*(x), w_i \rangle w_i \\
			             & = \sum_{i = 1}^{j}\langle x, U(w_i) \rangle w_i   \\
			             & = \sum_{i = 1}^{j}\langle x, 0 \rangle w_i        \\
			             & = 0
		      \end{align*}
		      Therefore, $U^{*}$ is a partial isometry.
	      \end{proof}
	      }
	\item{
	      Show that $U^{*}U$ is an orthogonal projection on $W$.
	      \begin{proof}
		      We first show that $U^{*}U$ is a projection on $W$. From the lemma in (c), $x \in V$ can be expressed as
		      $x = w + w'$ where $w \in W, w' \in W^\perp$. Then by definition of partial isometry of $U$,
		      \begin{align*}
			      U^{*}U(x) & = U^{*}U(w + w')         \\
			                & = U^{*}U(w) + U^{*}U(w') \\
			                & = U^{*}(w) + U^{*}U(0)   \\
			                & = U^{*}(w)
		      \end{align*}
		      Thus, $U^{*}U$ is a projection on $W$.
		      \begin{lemma}
			      Let $V$ be a finite dimensional inner product space and $W$ be a subspace of $V$. Then, $(W^\perp)^\perp = W$.
			      \begin{proof}
					Let $w \in W$. For all $w' \in W^\perp$, $ \langle w, w' \rangle = 0$. This implies that $w \in (W^\perp)^\perp$.\par
					Let $w' \in (W^\perp)^\perp$. That means for some $w \in W$, $ \langle w', w \rangle = 0$.This implies that $w \in W$. \par
					Therefore, $(W^\perp)^\perp = W$.
			      \end{proof}
		      \end{lemma}
		      By definition, $U^{*}U$ is an orthogonal projection if $R(U^{*}U)^\perp = N(U^{*}U)$ and $N(U^{*}U)^\perp = R(U^{*}U)$.
		      We only need to prove one conditions because from our lemma,
		      \begin{align*}
			      R(U^{*}U)^\perp         & = N(U^{*}U)       \\
			      (R(U^{*}U)^\perp)^\perp & = N(U^{*}U)^\perp \\
			      R(U^{*}U)               & = N(U^{*}U)^\perp
		      \end{align*}
		      We claim that $R(U^{*}U)^\perp = N(U^{*}U)$. Let $x \in R(U^{*}U)^\perp$.
		      \begin{align*}
			      \langle U^{*}U(x), x \rangle & = \langle U(x), U(x) \rangle \\
			                                   & = 0
		      \end{align*}
		      This implies that $U(x) = 0$, so $x \in N(U^{*}U)$.\par
		      Let $x \in N(U^{*}U)$. By definition, $U^{*}U(x) = 0$. Let $y \in R(U^{*}U)$,
		      \begin{align*}
			      \langle x, U^{*}U(y) \rangle & = \langle U(x), U(y) \rangle   \\
			                                   & = \langle U^{*}U(x), y \rangle \\
			                                   & = \langle 0, y \rangle\\
			                                   & = 0
		      \end{align*}
			  So $x \in R(U^{*}U)^\perp$.\par
			  Therefore, $U^{*}U$ is an orthogonal projection on $W$.
	      \end{proof}
	      }
\end{enumerate}

\end{document}
