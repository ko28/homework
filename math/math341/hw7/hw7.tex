\documentclass[11pt]{scrartcl}
\usepackage[sexy]{../../evan}
\usepackage{cmbright}
\usepackage{cancel}
\usepackage[T1]{fontenc}
%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
%\usepackage[pdfborder={0 0 0},colorlinks=true,citecolor=red{}]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{systeme}

\makeatletter
\newenvironment{Dequation}
  {%
  \def\tagform@##1{%
    \maketag@@@{\makebox[0pt][r]{(\ignorespaces##1\unskip\@@italiccorr)}}}%
  \ignorespaces
  }
  {%
  \def\tagform@##1{\maketag@@@{(\ignorespaces##1\unskip\@@italiccorr)}}%
  \ignorespacesafterend
  }
\makeatother

\title{Math 341: Homework 7}
\author{Daniel Ko}
\date{Spring 2020}

\begin{document}
\maketitle

\section{A}
For each of the following homogeneous systems of linear equations, find the dimension of and a basis for the
solution space.

\begin{enumerate}[label=\alph*.]
	\item{
		\systeme{
			x_1 + 3x_2 = 0,
			2x_1 + 6x_2 = 0
		}
		\begin{proof}
			Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
			\[
				A=
			\begin{bmatrix}
				1 & 3\\
				2 & 6\\
			\end{bmatrix}\\
			\]
			It is clear that rank(A) = 1 because the two columns are a multiples of each other. 
			If K is	the solution set of this system, then dim(K) = 2 - 1 = 1. 
			Thus any nonzero solution constitutes a basis for K. For example, since
			$
			\begin{pmatrix}
				-3\\
				1\\
			\end{pmatrix}
			$
			is a solution to the given system,
			$
			\begin{Bmatrix}
				\begin{pmatrix}
					-3\\
					1\\
				\end{pmatrix}
			\end{Bmatrix}
			$ 
			is a basis for K by Corollary 2 of Theorem 1.10.
		\end{proof}
	}

	\item{
		\systeme{
			x_1 + 2x_2 - x_3 = 0,
			2x_1 + x_2 + x_3 = 0
		}
		\begin{proof}
			Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
			\[
				A=
			\begin{bmatrix}
				1 & 2 & -1\\
				2 & 1 & 1\\
			\end{bmatrix}\\
			\]
			It is clear that rank(A) = 2 because there are two linearly independent rows (rank of a transposed matrix is the same as the rank of a matrix).
			If K is	the solution set of this system, then dim(K) = 3 - 2 = 1. 
			Thus any nonzero solution constitutes a basis for K. For example, since
			$
			\begin{pmatrix}
				1\\
				-1\\
				-1\\
			\end{pmatrix}
			$
			is a solution to the given system,
			$
			\begin{Bmatrix}
				\begin{pmatrix}
					1\\
					-1\\
					-1\\
				\end{pmatrix}
			\end{Bmatrix}
			$ 
			is a basis for K by Corollary 2 of Theorem 1.10.
		\end{proof}
	}
	
	\item{
		\systeme{
			x_1 + 2x_2 - 3x_3 + x_4 = 0
		}
		\begin{proof}
			Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
			\[
				A=
			\begin{bmatrix}
				1 & 2 & -1 & 1\\
			\end{bmatrix}
			\]
			It is clear that rank(A) = 1 because there are one linearly independent row (rank of a transposed matrix is the same as the rank of a matrix).
			If K is	the solution set of this system, then dim(K) = 4 - 1 = 3. 
			Note that, 
			$
			\begin{pmatrix}
				1\\
				1\\
				1\\
				0\\
			\end{pmatrix},
			\begin{pmatrix}
				0\\
				0\\
				1\\
				3\\
			\end{pmatrix},
			\begin{pmatrix}
				1\\
				0\\
				0\\
				-1\\
			\end{pmatrix}
			$
			are linearly independent vectors in K. Thus they form a basis by Corollary 2 of Theorem 1.10.
		\end{proof}
	}
	\item{
		\systeme{
		x_1 + 2x_2 + x_3 + x_4 = 0,
		x_2 - x_3 + x_4 = 0
		}	
		\begin{proof}
			Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
			\[
				A=
			\begin{bmatrix}
				1 & 2 & 1 & 1\\
				0 & 1 & -1 & 1\\
			\end{bmatrix}
			\]
			It is clear that rank(A) = 2 because there are 2 linearly independent rows (rank of a transposed matrix is the same as the rank of a matrix).
			If K is	the solution set of this system, then dim(K) = 4 - 2 = 2. 
			Note that, 
			$
			\begin{pmatrix}
				2\\
				0\\
				-1\\
				-1\\
			\end{pmatrix},
			\begin{pmatrix}
				-4\\
				2\\
				1\\
				-1\\
			\end{pmatrix}
			$
			are linearly independent vectors in K. Thus they form a basis by Corollary 2 of Theorem 1.10.
		\end{proof}
	}
	\item{
		\systeme{
			x_1 + x_2 - x_3 = 0,
			4x_1 + x_2 - 2x_3 = 0
			}	
		\begin{proof}
			Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
			\[
				A=
			\begin{bmatrix}
				1 & 1 & -1\\
				4 & 1 & -2\\
			\end{bmatrix}
			\]
			It is clear that rank(A) = 2 because there are 2 linearly independent rows (rank of a transposed matrix is the same as the rank of a matrix).
			If K is	the solution set of this system, then dim(K) = 3 - 2 = 1. 
			Thus any nonzero solution constitutes a basis for K. For example, since
			$
			\begin{pmatrix}
				1\\
				2\\
				3\\
			\end{pmatrix}
			$
			is a solution for K, it forms a basis by Corollary 2 of Theorem 1.10.
		\end{proof}
		}
	\item{
		\systeme{
			2x_1 + x_2 - x_3 = 0,
			x_1 - x_2 + x_3 = 0,
			x_1 + 2x_2 - 2x_3 = 0
		}
		\begin{proof}
			Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
			\[
				A=
			\begin{bmatrix}
				2 & 1 & -1\\
				1 & -1 & 1\\
				1 & 2 & -2\\
			\end{bmatrix}
			\]
			It is clear that rank(A) = 2 because there are 2 linearly independent columns (third column is second column multiplied by -1).
			If K is	the solution set of this system, then dim(K) = 3 - 2 = 1. 
			Thus any nonzero solution constitutes a basis for K. For example, since
			$
			\begin{pmatrix}
				0\\
				1\\
				1\\
			\end{pmatrix}
			$
			is a solution for K, it forms a basis by Corollary 2 of Theorem 1.10.
		\end{proof}
	}
	\item{
	\systeme{
		x_1 + 2x_2 = 0,
		x_1 - x_2 = 0
		}
	\begin{proof}
		Using Theorem 3.8 and following example 2 from the textbook, we define this system as an homogeneous system $Ax=0$, where 
		\[
			A=
		\begin{bmatrix}
			1 & 2\\
			1 & -1\\
		\end{bmatrix}
		\]
		It is clear that rank(A) = 2 because there are 2 linearly independent columns.
		If K is	the solution set of this system, then dim(K) = 2 - 2 = 0. 
		This means the zero vector is the basis for K, i.e.
		$
		\begin{pmatrix}
			0\\
			0\\
		\end{pmatrix}
		$.
	\end{proof}
	}
\end{enumerate}

\section{B}
Using the results of Exercise 2, find all solutions to the following systems.
\begin{enumerate}[label=\alph*.]

	\item{
		\systeme{
			x_1 + 3x_2 = 5,
			2x_1 + 6x_2 = 10
		}
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				2\\
				1\\
			\end{pmatrix}
			$.
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					2\\
					1\\
				\end{pmatrix}
				+ 
				t\begin{pmatrix}
					-3\\
					1\\
				\end{pmatrix}: t \in \mathbb{R}
			\right\}
			\]
		\end{proof}
	}
	
	\item{
		\systeme{
			x_1 + 2x_2 - x_3 = 3,
			2x_1 + x_2 + x_3 = 6
		}
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				1\\
				2\\
				2\\
			\end{pmatrix}
			$.
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					1\\
					2\\
					2\\
				\end{pmatrix}
				+ 
				t\begin{pmatrix}
					1\\
					-1\\
					-1\\
				\end{pmatrix}: t \in \mathbb{R}
			\right\}
			\]
		\end{proof}
	}
	\item{
		\systeme{
			x_1 + 2x_2 - 3x_3 + x_4 = 1
		}
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				1\\
				0\\
				0\\
				0\\
			\end{pmatrix}
			$.	
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					1\\
					0\\
					0\\
					0\\
				\end{pmatrix} +
				t_1\begin{pmatrix}
					1\\
					1\\
					1\\
					0\\
				\end{pmatrix}+
				t_2\begin{pmatrix}
					0\\
					0\\
					1\\
					3\\
				\end{pmatrix}+
				t_3\begin{pmatrix}
					1\\
					0\\
					0\\
					-1\\
				\end{pmatrix}: t_1, t_2, t_3 \in \mathbb{R}
			\right\}
			\]
		\end{proof}
		}

	\item{ 
		\systeme{
			x_1 + 2x_2 + x_3 + x_4 = 1,
			x_2 - x_3 + x_4 = 1	
		}	
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				0\\
				0\\
				0\\
				1\\
			\end{pmatrix}
			$.	
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					0\\
					0\\
					0\\
					1\\
				\end{pmatrix} +
				t_1\begin{pmatrix}
					2\\
					0\\
					-1\\
					-1\\
				\end{pmatrix}+
				t_2\begin{pmatrix}
					-4\\
					2\\
					1\\
					-1\\
				\end{pmatrix}: t_1, t_2 \in \mathbb{R}
			\right\}
			\]
		\end{proof}
	}
	\item{
		\systeme{
			x_1 + x_2 - x_3 = 1,
			4x_1 + x_2 - 2x_3 = 3
			}	
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				1\\
				1\\
				1\\
			\end{pmatrix}
			$.	
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					1\\
					1\\
					1\\
				\end{pmatrix} +
				t\begin{pmatrix}
					1\\
					2\\
					3\\
				\end{pmatrix}: t \in \mathbb{R}
			\right\}
			\]
		\end{proof}
	}
	\item{
		\systeme{
			2x_1 + x_2 - x_3 = 5,
			x_1 - x_2 + x_3 = 1,
			x_1 + 2x_2 - 2x_3 = 4
		}
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				2\\
				1\\
				0\\
			\end{pmatrix}
			$.	
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					2\\
					1\\
					0\\
				\end{pmatrix} +
				t\begin{pmatrix}
					0\\
					1\\
					1\\
				\end{pmatrix}: t \in \mathbb{R}
			\right\}
			\]
		\end{proof}
	}
	\item{
		\systeme{
			x_1 + 2x_2 = 5,
			x_1 - x_2 = -1
			}
		\begin{proof}
			A solution to the above system is  
			$
			\begin{pmatrix}
				1\\
				2\\
			\end{pmatrix}
			$.	
			Using Theorem 3.9 and following example 3 from the textbook, we know $K = \{s\} + K_H = \{s + k : k \in K_H \}$.
			So, 
			\[
			K=
			\left\{
				\begin{pmatrix}
					1\\
					2\\
				\end{pmatrix} +
				t\begin{pmatrix}
					0\\
					0\\
				\end{pmatrix}: t \in \mathbb{R}
			\right\} = 
			\left\{
				\begin{pmatrix}
					1\\
					2\\
				\end{pmatrix} 
			\right\}
			\]
		\end{proof}	
	}
\end{enumerate}

\section{C}
Prove that the system of linear equations $Ax = b$ has a solution if and only if $b \in R(L_A)$.
\begin{proof}
	Let $A$ be an $mxn$ matrix.
	\begin{enumerate}[i.]
		\item{
			$Ax = b$ has a solution $\Rightarrow b \in R(L_A)$.\\\-\\
			Let $s$ be a solution to $Ax = b$. This means $A_1s_1 + \cdots + A_ns_n = b$. 
			Notice that $b$ is a linear combination of the columns of $A$, 
			which is equivalent to $R(L_A)$ by the proof given in Theorem 3.5. 
			Therefore, $b \in R(L_A)$.
		}
		\item{
			$b \in R(L_A) \Rightarrow Ax = b$ has a solution.\\\-\\
			$b \in R(L_A)$ means that $b$ is a linear combination of the columns of $A$. 
			So, $b = A_1s_1 + \cdots + A_ns_n$. 
			This means there exists an $x$, composed of the $s_1, \cdots, s_n$ as seen in the previous equation, such that $Ax = b$.  
			Therfore, $Ax = b$ has a solution.
		}
	\end{enumerate}
\end{proof}

\section{D}
Prove or give a counterexample to the following statement: If the comatrix of a system of $m$ linear equations in
$n$ unknowns has rank $m$, then the system has a solution.
\begin{proof}
	If B is obtained from a matrix A by an elementary column operation,
	then there exists an elementary matrix E such that B = AE. By Theorem 3.2, 
	E is invertible, and hence rank(B) = rank(A) by Theorem 3.4. 
\end{proof}
	

\section{E}
Let $B$ and $B'$ is an $mxn$ matrix submatrix of B. Prove that rank($B$) = $r$, then rank($B'$) = $r - 1$
\begin{proof}
Consider the matrix 
\[
M=
\left[\begin{array}{@{}c|ccc@{}}
0 & & & \\
\vdots & & B' & \\
0 & & &
\end{array}\right]	
\]
$M$ has the same number of linearly independent columns to that of $B'$, so rank($M$) = rank($B'$)
Now consider the matrix below.
\[
B=
\left[\begin{array}{@{}c|ccc@{}}
1 & 0 &\cdots &0 \\ \hline
\vdots & & B' & \\
0 & & &
\end{array}\right]	
\]
$B$ has one more linearly independent row to that of M.\\ Then rank($B$) = rank($M$) + 1 = rank($B'$) + 1.\\
Therefore, if rank($B$) = r, then rank($M$) = rank($B'$) = r - 1.
\end{proof}
	

\section{F}
Let $B'$ and $D'$ be $mxn$ matrices, and let $B$ and $D$ be $(m + 1)x(n + 1)$ matrices respectively. 
Prove that if $B'$ can be transformed into $D'$ by an elementary row [column] operation, then $B$ can be transformed
into $D$ by an elementary row [column] operation.
\begin{proof}
If $B'$ can be tranformed into $D'$ by elementary row operations, there must exist an elementary matrix $E$ 
such that $D'=EB'$ by theorem 3.1. Now consider the matrix below.
\[
A=
\left[\begin{array}{@{}c|ccc@{}}
1 & 0 &\cdots &0 \\ \hline
\vdots & & E & \\
0 & & &
\end{array}\right]	
\]
A is also an elementary matrix, 
We can observe that $D = AB$, thus $B$ can be tranformed to $D$ by elementary row operations.
Without loss of generality, there exist a matrix $F$ such that $D' = B'F$ where $F$ is the elementary column matrix.
So, $D = BF$ where $B$ is like the $A$ matrix but with $F$ instead of $E$. Therefore. $B$ can be tranformed
to $D$ by elementary column operations. 
\end{proof}

\section{G}

\begin{enumerate}[label=\alph*.]
	\item{
		Find a $5x5$ matrix $M$ with rank 2 such that $AM=O$ where $O$ is the $4x5$ zero matrix.
		\begin{proof}
			By solving $Ax=0$, we get this system of equation:\\
			\systeme{
				x_1 - x_3 +  2x_4 + x_5 = 0,
				-x_1 + x_2 + 3x_3 - x_4 = 0,
				-2x_1 + x_2 + 4x_3 - x_4 + 3x_5 = 0,
				3x_1 - x_2 - 5x_3 + x_4 - 6x_5 = 0}\\
			Solving this system of equations by computing reduced row echelon form, 
			we get that $x_1, x_2, x_4$ are the pivot variables and $x_3, x_5$ are the 
			free variables. So solutions are in the form 
			$(x_3+3x_5,-2x_3+x_5, x_3, -2x_5,x_5)$. From this, we are able to 
			construct a basis for Ax=0, $\{(1,-2,1,0,0),(3,1,0,-2,1)\}$.
			Define   
			\[			
				M=
				\left[\begin{array}{ccccc}
				1 &3 & 0 & 0 &0 \\ 
				-2 &1 & 0 & 0 &0 \\ 
				1 &0 & 0 & 0 &0 \\ 
				0 &-2 & 0 & 0 &0 \\ 
				0 &1 & 0 & 0 &0 
				\end{array}\right]	
			\]
			It is obvious that this matrix has rank 2 and is $5x5$. Because the column is a basis for $Ax=0$, the resulting matrix will be $O$.
		\end{proof}
	}
	\item{
		Suppose that $B$ is a $5x5$ matrix such that $AB=O$. Prove that rank$(B) \leq 2$
		\begin{proof}
			Since $AB=O$, we know that the columns of $B$ is a solution to $Ax=0$, which is a subset of the nullspace of $L_A$.
			From the rank nullity theorem, we know that $\text{dim}(\mathbb{F}^5) = \text{rank}(L_A) + \text{nullity}(L_A)$.\\
			$\text{nullity}(L_A) = \text{dim}(\mathbb{F}^5) - \text{rank}(L_A) = 5 - 3 = 2$. So, rank($B$) cannot be greater than 2. 
			Therefore, rank$(B) \leq 2$.
		\end{proof}
	}
\end{enumerate}

\section{H} 
For each of the following linear transformations $T$, determine whether $T$ is invertible, and compute $T^{-1}$ if it exists.

\-\\
Let $\alpha$ be the standard basis for the domain and $\gamma$ be the standard basis for the codomain for 
each of these tranformations below. For each of these problems, we can use theorem 2.18 and
its corollaries to determine if the transformations are invertible, namely we need to show 
that $[T]_{\alpha}^{\gamma}$ is invertible.
\begin{enumerate}[label=\alph*.]
	\item{
	$T : P_2(R) \rightarrow P_2(R)$ defined by $T(f(x)) = f''(x) + 2f'(x) - f(x)$
	\begin{proof}
	\begin{align*}
	[T]_{\alpha}^{\gamma} & = [[T(1)]_{\gamma}, [T(x)]_{\gamma}, [T(x^2)]_{\gamma}]\\
	& = [[-1]_{\gamma}, [-x + 2]_{\gamma}, [-x^2 + 4x + 2]_{\gamma}]\\
	& = \begin{bmatrix}
		-1 & 2 & 2\\
		0 & -1 & 4 \\ 
		0 & 0 & -1 \\
	\end{bmatrix}
	\end{align*}
	To see if the matrix above is invertible, we can use an agumented matrix 
	$([T]_{\alpha}^{\gamma}|I_3)$ and apply elementary row operations to tranform it 
	into the form of $(I_3|[T^{-1}]_{\gamma}^{\alpha})$.
	\begin{align*}
	([T]_{\alpha}^{\gamma}|I_3)=
	\left[\begin{array}{ccc|ccc}
	-1 & 2 & 2 & 1 & 0 & 0 \\ 
	0 & -1 & 4 & 0 & 1 & 0 \\ 
	0 & 0 & -1 & 0 & 0 & 1 \\ 
	\end{array}\right]\\
	r_1 \leftarrow -r_1,\quad r_2 \leftarrow -r_2, \quad r_3 \leftarrow -r_3\\
	\left[\begin{array}{ccc|ccc}
	1 & 2 & 2 & -1 & 0 & 0 \\ 
	0 & 1 & 4 & 0 & -1 & 0 \\ 
	0 & 0 & 1 & 0 & 0 & -1 \\ 
	\end{array}\right]\\
	r_1 \leftarrow 2r_2 + r_1\\
	\left[\begin{array}{ccc|ccc}
	1 & 0 & -10 & -1 & -2 & 0 \\ 
	0 & 1 & -4 & 0 & -1 & 0 \\ 
	0 & 0 & 1 & 0 & 0 & -1 \\ 
	\end{array}\right]\\
	r_1 \leftarrow 10r_3 + r_1, \quad r_2 \leftarrow 4r_3 + r_2\\
	(I_3|[T^-1]_{\alpha}^{\gamma})=
	\left[\begin{array}{ccc|ccc}
	1 & 0 & 0 & -1 & -2 & -10 \\ 
	0 & 1 & 0 & 0 & -1 & -4 \\ 
	0 & 0 & 1 & 0 & 0 & -1 \\ 
	\end{array}\right]\\
	\end{align*}	
	Therefore, the inverse of $T$ exists and is \\$T^{-1}(a_0 + a_1x + a_2x^2) 
	=-a_0 -2a_2 -10a_2 + (-a_1 - 4a_2)x -a_2x^2$
	\end{proof}
	}

	\item{
	$T : P_2(R) \rightarrow P_2(R)$ defined by $T(f(x)) = (x + 1)f'(x)$
	\begin{proof}
	\-\\
	Similiar to (a), we will calculate $[T]_{\alpha}^{\gamma}$.
	\[
	[T]_{\alpha}^{\gamma} = 
	\begin{bmatrix}
		0 & 1 & 0\\
		0 & 1 & 2\\ 
		0 & 0 & 2\\
	\end{bmatrix}
	\]
	Notice that the rank of this matrix is 2, which means rank($[T]_{\alpha}^{\gamma}$) $\neq 3$.
	Therefore, there is no inverse for this matrix exists by the remark in chapter 3.2,
	"an $nxn$ matrix is invertible if and only if its rank is $n$".
	\end{proof}
	}

	\item{
	$T:R^3 \rightarrow R^3$ defined by $T(a_1, a_2, a_3) =
	(a_1 + 2a_2 + a_3, -a_1 + a_2 + 2a_3, a_1 + a_3)$ 
	\begin{proof}
	\-\\
	Similiar to (a), we will calculate $[T]_{\alpha}^{\gamma}$.
	\[
	[T]_{\alpha}^{\gamma} = 
	\begin{bmatrix}
		1 & 2 & 1\\
		-1 & 1 & 2\\
		1 & 0 & 1 \\
	\end{bmatrix}
	\]
	We can calculate the inverse of $T$ by following the same steps we did in (a).
	\[
	[T^{-1}]_{\gamma}^{\alpha} = 
	\begin{bmatrix}
		\frac16 & -\frac13 & \frac12\\
		\frac12 & 0 & -\frac12\\
		-\frac16 & \frac13 & \frac12 \\
	\end{bmatrix}
	\]
	Therefore, the inverse of $T$ exists and is \\$T^{-1}(a_1, a_2, a_3) 
	=(\frac16a_1 -\frac13a_2 + \frac12a_3,\frac12a_1 -\frac12a_3,
	-\frac16a_1 + \frac13a_2 + \frac12a_3)$
	\end{proof}
	}

	\item{
		$T:R^3 \rightarrow P_2(R)$ defined by $T(a_1, a_2, a_3) =
		(a_1 + a_2 + a_3) + (a_1 - a_2 + a_3)x + a_1x^2$ 
		\begin{proof}
		\-\\
		Similiar to (a), we will calculate $[T]_{\alpha}^{\gamma}$.
		\[
		[T]_{\alpha}^{\gamma} = 
		\begin{bmatrix}
			1 & 1 & 1\\
			1 & -1 & 1\\
			1 & 0 & 0 \\
		\end{bmatrix}
		\]
		We can calculate the inverse of $T$ by following the same steps we did in (a).
		\[
		[T^{-1}]_{\gamma}^{\alpha} = 
		\begin{bmatrix}
			0 & 0 & 1\\
			\frac12 & -\frac12 & 0\\
			\frac12 & \frac12 & -1\\
		\end{bmatrix}
		\]
		Therefore, the inverse of $T$ exists and is \\$T^{-1}(a_1 + a_2x + a_3x^2) 
		=(a_3, \frac12a_1 - \frac12a_2, \frac12a_1 + \frac12a_2 -a_3)$
		\end{proof}
	
	}

	\item{
		$T:P_2(R) \rightarrow R^3$ defined by $T(f(x)) = (f(-1),f(0),f(1))$
		\begin{proof}
		\-\\
		Similiar to (a), we will calculate $[T]_{\alpha}^{\gamma}$.
		\[
		[T]_{\alpha}^{\gamma} = 
		\begin{bmatrix}
			1 & -1 & 1\\
			1 & 0 & 0\\
			1 & 1 & 1 \\
		\end{bmatrix}
		\]
		We can calculate the inverse of $T$ by following the same steps we did in (a).
		\[
		[T^{-1}]_{\gamma}^{\alpha} = 
		\begin{bmatrix}
			0 & 1 & 0\\
			-\frac12 & 0 & \frac12\\
			\frac12 & -1 & \frac12\\
		\end{bmatrix}
		\]
		Therefore, the inverse of $T$ exists and is \\$T^{-1}(a_1, a_2, a_3) 
		=a_2 + (-\frac12 a_1 + \frac12 a_3)x + (\frac12a_1 -a_2 + \frac12a_3)x^2$
		\end{proof}
		}

		\item{
		$T:M_{2x2}(R) \rightarrow R^4$ defined by $T(A) = (tr(A), tr(A^t), tr(EA), tr(AE))$
		where 
		$E = 
		\begin{bmatrix}
		0 & 1 \\
		1 & 0 \\ 
		\end{bmatrix}
		$
		\begin{proof}
		\-\\
		Similiar to (a), we will calculate $[T]_{\alpha}^{\gamma}$.
		\[
		[T]_{\alpha}^{\gamma} = 
		\begin{bmatrix}
			1 & 0 & 0 & 1\\
			1 & 0 & 0 & 1\\
			0 & 1 & 1 & 0\\
			0 & 1 & 1 & 0\\
		\end{bmatrix}
		\]
		Notice that the rank of this matrix is 2, which means rank($[T]_{\alpha}^{\gamma}$) $\neq 4$.
		Therefore, there is no inverse for this matrix exists by the remark in chapter 3.2,
		"an $nxn$ matrix is invertible if and only if its rank is $n$".
		\end{proof}
		}
\end{enumerate}



\section{I} 
Express the invertible matrix
$
\begin{bmatrix}
1 & 2 & 1\\
1 & 0 & 1 \\ 
1 & 1 & 2 \\ 
\end{bmatrix}
$
as a product of elementary matrices
\begin{proof}
Corollary 1 of Theorem 3.6 states that for any invertible matrix $A$\\
$I_n= A^{-1}A = E_p E_{p-1} \cdots E_1 A$, where $E_i$ are invertible elementary matrices. 
So, $A = E_1^{-1} \cdots E_{p-1}^{-1} E_p^{-1}$.
To calcuate the values of $E_i$, we can use an agumented matrix 
$(A|I_3)$ and apply elementary row operations to tranform it 
into the form of $(I_3|A^{-1})$. Each transformation to the right matrix will be our $E$. 

 \begin{align*}
	(A|I_3)=
	\left[\begin{array}{ccc|ccc}
	1 & 2 & 1 & 1 & 0 & 0 \\ 
	1 & 0 & 1 & 0 & 1 & 0 \\ 
	1 & 1 & 2 & 0 & 0 & 1 \\ 
	\end{array}\right]\\
	r_3 \leftarrow r_3 - r_2
	\left[\begin{array}{ccc|ccc}
	1 & 2 & 1 & 1 & 0 & 0 \\ 
	1 & 0 & 1 & 0 & 1 & 0 \\ 
	0 & 1 & 1 & 0 & -1 & 1 \\ 
	\end{array}\right]\\
	r_2 \leftarrow r_2 - r_1
	\left[\begin{array}{ccc|ccc}
	1 & 2  & 1 & 1  & 0  & 0 \\ 
	0 & -2 & 0 & -1 & 1  & 0 \\ 
	0 & 1  & 1 & 0  & -1 & 1 \\ 
	\end{array}\right]\\
	r_1 \leftarrow r_1 + r_2
	\left[\begin{array}{ccc|ccc}
	1 & 0  & 1 & 0  & 1  & 0 \\ 
	0 & -2 & 0 & -1 & 1  & 0 \\ 
	0 & 1  & 1 & 0  & -1 & 1 \\ 
	\end{array}\right]\\
	r_2 \leftarrow -\frac12r_2
	\left[\begin{array}{ccc|ccc}
	1 & 0 & 1 & 0  & 1  & 0 \\ 
	0 & 1 & 0 & \frac12 & -\frac12  & 0 \\ 
	0 & 1 & 1 & 0  & -1 & 1 \\ 
	\end{array}\right]\\
	r_3 \leftarrow r_3 - r_2
	\left[\begin{array}{ccc|ccc}
	1 & 0 & 1 & 0  & 1  & 0 \\ 
	0 & 1 & 0 & \frac12 & -\frac12 & 0 \\ 
	0 & 0 & 1 & -\frac12 & -\frac12 & 1 \\ 
	\end{array}\right]\\
	r_1 \leftarrow r_1 - r_3
	\left[\begin{array}{ccc|ccc}
	1 & 0 & 0 & \frac12 & \frac32  & -1 \\ 
	0 & 1 & 0 & \frac12 & -\frac12 & 0 \\ 
	0 & 0 & 1 & -\frac12 & -\frac12 & 1 \\ 
	\end{array}\right]\\
\end{align*}
By construction, $A$ is the product of the inverse of the elementary matrices on the right 
of the agumented matrix starting the first matrix shown above. 
\end{proof}

\section{J}
Suppose that $A$ and $B$ are matrices having $n$ rows. Prove that $M(A|B) = (MA|MB)$ for any $mxn$ matrix
\begin{proof}
Let $A$ have $a$ number of columns and $B$ have $b$ number of columns. \\
Notice for $1 \leq j \leq a$:
$(M(A|B))_{i,j} = \sum_{k=1}^{n} M_{i,k}A_{k,j} = (MA)_{i,j} = (MA|MB)_{i,j}$\\
Similarily for $a < j \leq a + b$:
$(M(A|B))_{i,j} = \sum_{k=1}^{n} M_{i,k}B_{k,j} = (MB)_{i,j} = (MA|MB)_{i,j}$\\
Therefore, $M(A|B) = (MA|MB)$ for any $mxn$ matrix.
\end{proof}

\section{1}
Let $V = P_n(F)$, and let $c_0, c_1, \cdots, c_n$ be distinct scalars in $F$\\
For $0 \leq i \leq n$, define $f_i \in V^*$ by $f_i(p(x)) = p(c_i)$. Prove that 
$\{f_0, f_1, \cdots, f_n\}$ is a basis for $V^*$.
\begin{proof}
	To prove that $\{f_0, f_1, \cdots, f_n\}$ is a basis for $V^*$, we must show
	$f_i$ is linear, the cardinality of the set is equal to the dimension of
	$V^*$, and that the set is linearly independent by Theorem 1.10 corollary 2.
	\begin{enumerate}[label=\roman*.]
	\item{
		$f_i$ is linear\\
		\-\\
		Show that $f_i(g(x) + ch(x)) = f_i(g(x)) + cf_i(h(x))$ 
		where $g(x), h(x) \in P_n(F)$ and $c \in F$.
	\begin{align*}	
	f_i(g(x) + ch(x)) &= g(c_i) + ch(c_i)\\
	&= f_i(g(x)) + cf_i(h(x))
	\end{align*}
	}
	\item{
		Cardinality of the set is equal to the dimension of	$V^*$ \\
		\-\\
		By the corollary to Theorem 2.20, we know dim($V^*$) = dim($L(V,F)$) = 
		dim($V$) $\cdot$ dim($F$) = dim($V$). So, dim($V^*$) = dim($V$) = $n + 1$ because $V = P_n(F)$.
		So cardinality of the set is equal to the dimension of $V^*$.
	}
	\item{
		Set is linearly independent\\
		\-\\
		Let $c_0f_0(p(x)) + c_1f_1(p(x)) + \cdots + c_nf_n(p(x)) = 0$ for some $c_i$. \\
		Let $p_i(x) = \Pi_{i \neq j}(x-c_j)$, where $p_i(c_j) = 0$ and $p_i(c_i) \neq 0$.\\
		Thus, $c_0f_0(p(x)) + c_1f_1(p(x)) + \cdots + c_nf_n(p(x)) =
		c_0f_0(c_0) + c_1f_1(c_1) + \cdots + c_nf_n(c_1)$\\
		Because $f_0(c_0), f_1(c_1), \cdots, f_n(c_n) \neq 0$, $c_0$ must equal zero and 
		all $c_i$ must be zero as well. Therefore, this set is linearly independent.
	}
	\end{enumerate}
\end{proof}

\section{2}
Use the corollary to Theorem 2.26 and (a) to show that there exist unique polynomials
$p_0(x), p_1(x), \cdots, p_n(x)$ such that $p_i(c_j) = \delta_{ij}$ for $0 \leq i \leq n$.
These polynomials are the Lagrange polynomials defined in Section 1.6
\begin{proof}
Corollary to Theorem 2.26 states that every ordered basis for $V^*$, $\{f_0, f_1, \cdots, f_n\}$, 
is the dual basis for some basis, $\beta = \{p_0, p_1, \cdots, p_m\}$, for $V$ such that 
$p_i(c_j) = \delta_{ij}$ for $0 \leq i \leq n$. Define polynomial $z \in V$ such that 
$z(c_{j}) = \delta_{kj}$. Since $\beta$ is a basis for $V$ we can write $z$ as a linear
combination of $V$. So $z=\sum_{i=0}^{n}a_ip_i$. Notice that when $z(c_0) = 1 = \sum_{i=0}^{n}a_ip_i = a_1$ and
$z(c_j) = 0 = \sum_{i=0}^{n}a_ip_i = a_j$ when $j \neq 1$. Thus, by definition, $z=p_0$ and is unique because it was
constructed from basis $\beta$. Without loss of generality, we can define $p_1, p_2, \cdots, p_n$ in the same way. 
\end{proof}

\section{3}
For any scalars $a_0, a_1, \cdots, a_n$ (not necessarily distinct), deduce that
there exists a unique polynomial $q(x)$ of degree at most $n$ such that
$q(c_i) = a_i$ for $0 \leq i \leq n$. In fact, $q(x)=\sum_{i=0}^n a_ip_i(x)$

\begin{proof}
Let $\beta = \{p_0, p_1, \cdots, p_m\}$. Let $q_0(c_i), q_1(c_i) \in V$. We define them as:\\
$q_0(x)=\sum_{i=0}^n a^0_ip_i(x)$\\
$q_1(x)=\sum_{i=0}^n a^1_ip_i(x)$\\
Notice that  $a^0_i = q_1(c_i) = \sum_{i=0}^n a^1_ip_i(x) = a^1_i$
Therefore, $q_0 = q_1$ and $q_0$ is unique.
\end{proof}

\section{4}
\begin{proof}
Substitute $a^0_i$ with $p_1(c_i)$ from the equation in question 3.
\end{proof}

\section{5}
Prove that $$\int_a^b p(t)dt = \sum_{i=0}^n p(c_i)d_i$$
where $$d_i= \int_a^b p_i(t)dt$$
Suppose now that $$c_i = a + \frac{i(b-a)}{n} \text{ for } i = 0, 1, \cdots, n $$
For $n = 1$,the preceding result yields the trapezoidal rule for evaluating the definite integral of a polynomial. For
$n = 2$, this result yields Simpsonâ€™s rule for evaluating the definite integral of a polynomial.
\begin{proof}
	\begin{align*}
	\int_a^b p(t)dt & = \int_a^b \sum_{i=0}^n p(c_i)p_i(x)dt\\
	& = \sum_{i=0}^n \int_a^b p(c_i)p_i(x)dt\\
	& = \sum_{i=0}^n \int_a^b p(c_i)d_i \\
	& = \sum_{i=0}^n p(c_i)d_i 
	\end{align*}
\end{proof}

\end{document}
