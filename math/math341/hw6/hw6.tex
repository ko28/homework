\documentclass[11pt]{scrartcl}
\usepackage[sexy]{../../evan}
\usepackage{cmbright}
\usepackage{cancel}
\usepackage[T1]{fontenc}
%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
%\usepackage[pdfborder={0 0 0},colorlinks=true,citecolor=red{}]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{systeme}

\makeatletter
\newenvironment{Dequation}
  {%
  \def\tagform@##1{%
    \maketag@@@{\makebox[0pt][r]{(\ignorespaces##1\unskip\@@italiccorr)}}}%
  \ignorespaces
  }
  {%
  \def\tagform@##1{\maketag@@@{(\ignorespaces##1\unskip\@@italiccorr)}}%
  \ignorespacesafterend
  }
\makeatother

\title{Math 341: Homework 6}
\author{Daniel Ko}
\date{Spring 2020}

\begin{document}
\maketitle

\section{A}
Find the rank of the following matrices.

\begin{enumerate}[label=\alph*.]
	\item{
			2
			show work later
	}
	\item{
			3
		}
	\item{
			2
	}
	\item{
			1
		}
	\item{
			3
		}
	\item{
			3
	}
	\item{
			1
		}
\end{enumerate}

\section{B}
Prove that any elementary row [column] operation of type 1 can be obtained by a succession of three elementary
row [column] operations of type 3 followed by one elementary row [column] operation of type 2
\begin{proof}
Row operation type 1 on row $i$ and row $j$ can be done by the following:
\begin{enumerate}[label=\arabic*.]
	\item{
		Row operation type 3: Add $-1$ times row $i$ to row $j$ 
	}
	\item{
		Row operation type 3: Add row $j$ to row $i$
	}
	\item{
		Row operation type 3: Add $-1$ times row $i$ to row $j$
	}
	\item{
		Row operation type 2: Multiply row $j$ by $-1$
	}
\end{enumerate}
Without loss of generality, same could be done for a elementary column operation of type 1.
\end{proof}

\section{C}
Let A be an $mxn$ matrix. Prove that there exists a sequence of elementary row operations of types 1 and 3 that
transforms A into an upper triangular matrix.
\begin{proof}
	Iterate through each column, let this variable be c. If $A_{c,c}$ equals 0, go through all the elements in that column below
	$A_{c,c}$ and find the first non zero element. Perform a type 1 row operation on row $c$ and the row the non zero element
	was found. If there is no non zero element, do nothing and go to the next column. \\
	\-\\
	If $A_{c,c}$ does not equal 0, perform a type 3 row operation on each row below $A_{c,c}$. Multiply 
	$- \frac{A_{r,c}}{A_{c,c}}$ by the $c$th row to the $r$th row, where $r$ is every row below $c$.  
\end{proof}

\section{D}
Complete the proof of the corollary to Theorem 3.4 by showing that elementary column operations preserve rank.
\begin{proof}
	If B is obtained from a matrix A by an elementary column operation,
	then there exists an elementary matrix E such that B = AE. By Theorem 3.2
	(p. 150), E is invertible, and hence rank(B) = rank(A) by Theorem 3.4. 
\end{proof}
	

\section{E}
Let $B$ and $B'$ is an $mxn$ matrix submatrix of B. Prove that rank($B$) = $r$, then rank($B'$) = $r - 1$
\begin{proof}
By Theorem 3.5, we know that rank($B$) = dim($R(L_B)$), where $R(L_B) = \text{span}(B_1,B_2,\cdots,B_{n+1})$
and $B_i$ is the $i$th column of $B$. In other words, is the rank is the number of linearly independent
rows/columns in a matrix. Notice that $B'$ has one less linear independent row/column than $B$. It follows
that rank of $B'$ would be 1 less than the rank of $B$. Therefore, rank($B'$)$= r - 1$.
Consider the matrix 
\[
M=
\left[\begin{array}{@{}c|ccc@{}}
0 & & & \\
\vdots & & B' & \\
0 & & &
\end{array}\right]	
\]
$M$ has the same number of linearly independent columns to that of $B'$, so rank($M$) = rank($B'$)
Now consider the matrix below.
\[
B=
\left[\begin{array}{@{}c|ccc@{}}
1 & 0 &\cdots &0 \\ \hline
\vdots & & B' & \\
0 & & &
\end{array}\right]	
\]
$B$ has one more linearly independent row to that of M.\\ Then rank($B$) = rank($M$) + 1 = rank($B'$) + 1.\\
Therefore, if rank($B$) = r, then rank($M$) = rank($B'$) = r - 1.
\end{proof}
	

\section{F}
Let S be the subspace of $M_{nxn}(R)$ generated by all matrices of the form $AB - BA$ with $A$ and $B$ in $M_{nxn}(R)$.
Prove that dim$(S) = n^{2} - 1$. (You may want to use the trace together with the rank-nullity theorem)
\begin{proof}
	\-\\
	Trace is a linear transformation.\\
	$\text{Tr}: M_{nxn}(R) \rightarrow R $\\
	The subspace S is defined as $\{AB - BA : A,B \in M_{nxn}(R)\}$\\
	$\text{Tr}(AB - BA) = \text{Tr}(AB) - \text{Tr} (BA)$\\
	$= \text{Tr}(AB) - \text{Tr} (AB)$\\
	$= 0$\\
	All matrices that can be expressed as $AB - BA$ is in the null space of Tr. This means that N(Tr)$ = S$.\\
	The rank-nullity theorem states:\\
	dim(N(Tr)) + dim(R(Tr)) = dim($M_{nxn}(R)$)\\
	N(Tr)$ = S$, so dim(S) + dim(R(Tr)) = dim($M_{nxn}(R)$)\\
	dim(S) = dim($M_{nxn}(R)$) - dim(R(Tr))\\
	= $n^2 - $ dim(R)\\
	= $n^2 - $ 1 \\
\end{proof}

\section{G}
Let T be a linear transformation of a vector space V into itself. Suppose that $x \in V$ is such that $T^m (x) = 0$, and
$T^{m-1}(x) \neq 0$ for some positive m. Show that $x,T(x),T^2(x),\cdots ,T^{m-1}(x)$ are linearly independent.
\begin{proof}
	\-\\
	The linear combination of the above set is $$ a_0x + a_1T(x) + a_2T^2(x) + \cdots + a_{n-1}T^{m-1}(x) = 0$$
	Notice that $T^n(x) = 0$ for all $n \geq m$.\\
	$T^{m+1}(x) = T(T^{m}(x)) = T(0) = 0$\\
	Let's take $T^{m-1}$ on both sides of the linear combination.	
	\begin{Dequation}
	\begin{align*}
		T^{m-1}(a_0x + a_1T(x) + a_2T^2(x) + \cdots + a_{n-1}T^{m-1}(x)) &= T^{m-1}(0)\\
		T^{m-1}(a_0x) + T^{m-1}(a_1T(x)) + T^{m-1}(a_2T^2(x)) + \cdots + T^{m-1}(a_{n-1}T^{m-1}(x)) &= 0 \\
		T^{m-1}(a_0x) + 0 + 0 + \cdots + 0 &= 0\\
		T^{m-1}(a_0x) &= 0\\
		a_0 = \frac{0}{T^{m-1}(x)} &= 0
	\end{align*}
	\end{Dequation}
	By back substitution we know that $a_0 = a_1 = \cdots = a_{n-1} = 0$\\
	Therefore, $x,T(x),T^2(x),\cdots ,T^{m-1}(x)$ are linearly independent.
\end{proof}

\section{H} 
Let $T: R^3 \rightarrow R^3$
\begin{enumerate}[label=\alph*.]
	\item{
	If T(a,b,c) = (a,b,0), show that T is the projection on the xy-plane along the z-axis.	
	\begin{proof}
	\-\\
	We want to projection to be on the xy-plane along the z-axis. Let the projection be (x,y,0).\\
	To minimize the distance, we must choose x and y such that
	$$(a - x)^2 + (b - y)^2 + (c - 0)^2$$
	is minimum.
	Since the equation above is a difference of squares, x = a and b = y will give us the minimum value.
	Therefore, the projection on the xy-plane will be (a,b,0), which is T.
	\end{proof}
	}
	
	\item{
	Find a formula for T(a,b,c), where T represents the projection on the z-axis along the xy-plane.
	\begin{proof}
	\-\\
	We want to projection to be on the z-axis along the xy-plane. Let the projection be (0,0,z).\\
	To minimize the distance, we must choose z such that
	$$(a - 0)^2 + (b - 0)^2 + (c - z)^2$$
	is minimum.
	z = c will give us the minimum value.
	Therefore, the equation for T will be T(a,b,c)=(0,0,c).
	\end{proof}
	}

	\item{
	If T(a,b,c) = (a-c,b,0), show that T is the projection on the xy-plane along the line L $= \{(a,0,a):a \in R\}$
	\begin{proof}
	\-\\
	We want to projection to be on the xy-plane along the line L. Let the projection be $(x,y,0)$.\\
	A vector that is on L is $(1,0,1)$.
	To minimize the distance, we must choose $\lambda$ such that
	$$(a,b,c) + \lambda (1,0,1) = (x,y,0)$$
	is minimum. Writing the equation above as a system:
	\[
	\begin{aligned}
		a + \lambda &= x\\
		b &= y\\
		c + \lambda &= 0
	\end{aligned}
	\]
	Solving this system gives us, $x = a - c, y = b$\\ 
	Therefore, the projection on the xy-plane along the line L will be $(a-c,b,0)$.\\
	\end{proof}
	}

\end{enumerate}



\section{I} 
Suppose that the linear transformation $T : V \rightarrow V$ is the projection on $W \subset V$ along some subspace $W' \subset V$.
Prove that $W$ is T-invariant and that $T_W = I_W$
\begin{proof}
	\-\\
\begin{enumerate}[label=\alph*.]
	\item{
		$W$ is T-invariant.\\
		\-\\
		We need to show that $T(x) \in W$ for every $x \in W$.
		$$x = x_1 + x_2, T(x) = T(x_1 + x_2) = x_1 \text{ where } x_1 \in W, x_2 \in W'$$
		It is trivial to see that $T$ restricts to the identity on W.\\
		Therefore, $W$ is T-invariant.
		}
	\item{
		$T_W = I_W$\\
		\-\\
		For any $w \in W$, we can express $w$ as $w = w + 0$ where $0 \in W'$ because $W'$ is a subspace.\\
		Because $W_1$ and $W_2$ are a direct sum, there is no way to express $w$ as the sum of a vector in one and a vector in the other.\\
		$T(w + 0) = w$, shows that $T_w$ is the same as the indentity tranformation $I_W$.\\
		Therefore, $T_W = I_W$.
		}
\end{enumerate}
\end{proof}

\section{J}

\end{document}