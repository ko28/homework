\documentclass[11pt]{scrartcl}
\usepackage[sexy]{../../evan}
\usepackage{cmbright}
\usepackage{cancel}
\usepackage[T1]{fontenc}
%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
%\usepackage[pdfborder={0 0 0},colorlinks=true,citecolor=red{}]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{systeme}

\makeatletter
\newenvironment{Dequation}
  {%
  \def\tagform@##1{%
    \maketag@@@{\makebox[0pt][r]{(\ignorespaces##1\unskip\@@italiccorr)}}}%
  \ignorespaces
  }
  {%
  \def\tagform@##1{\maketag@@@{(\ignorespaces##1\unskip\@@italiccorr)}}%
  \ignorespacesafterend
  }
\makeatother

\title{Math 341: Homework 6}
\author{Daniel Ko}
\date{Spring 2020}

\begin{document}
\maketitle

\section{A}
Find the rank of the following matrices.

\begin{enumerate}[label=\alph*.]
	\item{
			Corollary 2a of Theorem 3.6 states that rank($A^t$) = rank($A$).
			It is clear that the rank of $A^t$ is 2 because the first and third
			columns are the same. So rank(A)=2.
	}
	\item{
		We will calculate the rank by performing row reduction on our matrix.  
		\begin{align*}
		A=
		\begin{bmatrix}
			1 & 1 & 0\\
			2 & 1 & 1\\
			1 & 1 & 1 \\
		\end{bmatrix}\\
		r_2 \leftarrow r_2 - 2r_1
		\begin{bmatrix}
			1 & 2 & 1\\
			0 & -1 & 1\\
			1 & 1 & 1 \\
		\end{bmatrix}\\ 
		r_3 \leftarrow r_3 - r_1
		\begin{bmatrix}
			1 & 2 & 1\\
			0 & -1 & 1\\
			0 & -1 & 0 \\
		\end{bmatrix}\\ 
		\end{align*}
		There are three linearly independent rows, so rank(A)=3.
		}
	\item{
		Corollary 2a of Theorem 3.6 states that rank($A^t$) = rank($A$).
		It is clear that the rank of $A^t$ is 2 because the second column
		has a zero in its second row. So rank(A)=2.
	}
	\item{
		\begin{align*}
			A=
			\begin{bmatrix}
				1 & 2 & 1\\
				2 & 4 & 2\\
			\end{bmatrix}\\
			r_2 \leftarrow r_2 - 2r_1
			\begin{bmatrix}
				1 & 2 & 1\\
				0 & 0 & 0\\
			\end{bmatrix}
		\end{align*}
		There is one linearly independent row, so rank(A)=1.
		}
	\item{
		Similiarly to (a) and (b), by performing row reduction we get the matrix
		\[
		\begin{bmatrix}
			1 & 0 & 6 & 1 & 0\\
			0 & 1 & -\frac32 & 0 & \frac12\\
			0 & 0 & -6 & -1 & 0\\
			0 & 0 & 0 & 0 & 0\\
		\end{bmatrix}
		\]
		There are three linearly independent columns, so rank(A)=3.
		}
	\item{
		Similiarly to (a) and (b), by performing row reduction we get the matrix
		\[
		\begin{bmatrix}
			1 & 2 & 0 & 1 & 0\\
			0 & 0 & 1 & 1 & 0\\
			0 & 0 & 0 & 0 & 1\\
			0 & 0 & 0 & 0 & 0\\
		\end{bmatrix}
		\]
		There are three linearly independent columns, so rank(A)=3.
	}
	\item{
		It is clear that the rank of $A$ is 1 because the three columns
		are the same and the other column is just zero. 
		}
\end{enumerate}

\section{B}
Prove that any elementary row [column] operation of type 1 can be obtained by a succession of three elementary
row [column] operations of type 3 followed by one elementary row [column] operation of type 2
\begin{proof}
Row operation type 1 on row $i$ and row $j$ can be done by the following:
\begin{enumerate}[label=\arabic*.]
	\item{
		Row operation type 3: Add $-1$ times row $i$ to row $j$ 
	}
	\item{
		Row operation type 3: Add row $j$ to row $i$
	}
	\item{
		Row operation type 3: Add $-1$ times row $i$ to row $j$
	}
	\item{
		Row operation type 2: Multiply row $j$ by $-1$
	}
\end{enumerate}
Without loss of generality, same could be done for a elementary column operation of type 1.
\end{proof}

\section{C}
Let A be an $mxn$ matrix. Prove that there exists a sequence of elementary row operations of types 1 and 3 that
transforms A into an upper triangular matrix.
\begin{proof}
	Iterate through each column, let this variable be c. If $A_{c,c}$ equals 0, go through all the elements in that column below
	$A_{c,c}$ and find the first non zero element. Perform a type 1 row operation on row $c$ and the row the non zero element
	was found. If there is no non zero element, do nothing and go to the next column. \\
	\-\\
	If $A_{c,c}$ does not equal 0, perform a type 3 row operation on each row below $A_{c,c}$. Multiply 
	$- \frac{A_{r,c}}{A_{c,c}}$ by the $c$th row to the $r$th row, where $r$ is every row below $c$.  
\end{proof}

\section{D}
Complete the proof of the corollary to Theorem 3.4 by showing that elementary column operations preserve rank.
\begin{proof}
	If B is obtained from a matrix A by an elementary column operation,
	then there exists an elementary matrix E such that B = AE. By Theorem 3.2, 
	E is invertible, and hence rank(B) = rank(A) by Theorem 3.4. 
\end{proof}
	

\section{E}
Let $B$ and $B'$ is an $mxn$ matrix submatrix of B. Prove that rank($B$) = $r$, then rank($B'$) = $r - 1$
\begin{proof}
Consider the matrix 
\[
M=
\left[\begin{array}{@{}c|ccc@{}}
0 & & & \\
\vdots & & B' & \\
0 & & &
\end{array}\right]	
\]
$M$ has the same number of linearly independent columns to that of $B'$, so rank($M$) = rank($B'$)
Now consider the matrix below.
\[
B=
\left[\begin{array}{@{}c|ccc@{}}
1 & 0 &\cdots &0 \\ \hline
\vdots & & B' & \\
0 & & &
\end{array}\right]	
\]
$B$ has one more linearly independent row to that of M.\\ Then rank($B$) = rank($M$) + 1 = rank($B'$) + 1.\\
Therefore, if rank($B$) = r, then rank($M$) = rank($B'$) = r - 1.
\end{proof}
	

\section{F}
Let $B'$ and $D'$ be $mxn$ matrices, and let $B$ and $D$ be $(m + 1)x(n + 1)$ matrices respectively. 
Prove that if $B'$ can be transformed into $D'$ by an elementary row [column] operation, then $B$ can be transformed
into $D$ by an elementary row [column] operation.
\begin{proof}
If $B'$ can be tranformed into $D'$ by elementary row operations, there must exist an elementary matrix $E$ 
such that $D'=EB'$ by theorem 3.1. Now consider the matrix below.
\[
A=
\left[\begin{array}{@{}c|ccc@{}}
1 & 0 &\cdots &0 \\ \hline
\vdots & & E & \\
0 & & &
\end{array}\right]	
\]
A is also an elementary matrix, 
We can observe that $D = AB$, thus $B$ can be tranformed to $D$ by elementary row operations.
Without loss of generality, there exist a matrix $F$ such that $D' = B'F$ where $F$ is the elementary column matrix.
So, $D = BF$ where $B$ is like the $A$ matrix but with $F$ instead of $E$. Therefore. $B$ can be tranformed
to $D$ by elementary column operations. 
\end{proof}

\section{G}

\begin{enumerate}[label=\alph*.]
	\item{
		Find a $5x5$ matrix $M$ with rank 2 such that $AM=O$ where $O$ is the $4x5$ zero matrix.
		\begin{proof}
			By solving $Ax=0$, we get this system of equation:\\
			\systeme{
				x_1 - x_3 +  2x_4 + x_5 = 0,
				-x_1 + x_2 + 3x_3 - x_4 = 0,
				-2x_1 + x_2 + 4x_3 - x_4 + 3x_5 = 0,
				3x_1 - x_2 - 5x_3 + x_4 - 6x_5 = 0}\\
			Solving this system of equations by computing reduced row echelon form, 
			we get that $x_1, x_2, x_4$ are the pivot variables and $x_3, x_5$ are the 
			free variables. So solutions are in the form 
			$(x_3+3x_5,-2x_3+x_5, x_3, -2x_5,x_5)$. From this, we are able to 
			construct a basis for Ax=0, $\{(1,-2,1,0,0),(3,1,0,-2,1)\}$.
			Define   
			\[			
				M=
				\left[\begin{array}{ccccc}
				1 &3 & 0 & 0 &0 \\ 
				-2 &1 & 0 & 0 &0 \\ 
				1 &0 & 0 & 0 &0 \\ 
				0 &-2 & 0 & 0 &0 \\ 
				0 &1 & 0 & 0 &0 
				\end{array}\right]	
			\]
			It is obvious that this matrix has rank 2 and is $5x5$. Because the column is a basis for $Ax=0$, the resulting matrix will be $O$.
		\end{proof}
	}
	\item{
		Suppose that $B$ is a $5x5$ matrix such that $AB=O$. Prove that rank$(B) \leq 2$
		\begin{proof}
			Since $AB=O$, we know that the columns of $B$ is a solution to $Ax=0$, which is a subset of the nullspace of $L_A$.
			From the rank nullity theorem, we know that $\text{dim}(\mathbb{F}^5) = \text{rank}(L_A) + \text{nullity}(L_A)$.\\
			$\text{nullity}(L_A) = \text{dim}(\mathbb{F}^5) - \text{rank}(L_A) = 5 - 3 = 2$. So, rank($B$) cannot be greater than 2. 
			Therefore, rank$(B) \leq 2$.
		\end{proof}
	}
\end{enumerate}

\section{H} 
For each of the following linear transformations $T$, determine whether $T$ is invertible, and compute $T^{-1}$ if it exists.

\-\\
Let $\alpha$ be the standard basis for the domain and $\gamma$ be the standard basis for the codomain for 
each of these tranformations below. For each of these problems, we can use theorem 2.18 and
its corollaries to determine if the transformations are invertible, namely we need to show 
that $[T]_{\alpha}^{\gamma}$ is invertible.
\begin{enumerate}[label=\alph*.]
	\item{
	$T : P_2(R) \rightarrow P_2(R)$ defined by $T(f(x)) = f''(x) + 2f'(x) - f(x)$
	\begin{proof}
	\begin{align*}
	[T]_{\alpha}^{\gamma} & = [[T(1)]_{\gamma}, [T(x)]_{\gamma}, [T(x^2)]_{\gamma}]\\
	& = [[-1]_{\gamma}, [-x + 2]_{\gamma}, [-x^2 + 4x + 2]_{\gamma}]\\
	& = \begin{bmatrix}
		-1 & 2 & 2\\
		0 & -1 & 4 \\ 
		0 & 0 & -1 \\
	\end{bmatrix}
	\end{align*}
	To see if the matrix above is invertible, we can use an agumented matrix 
	$([T]_{\alpha}^{\gamma}|I_3)$ and apply elementary row operations to tranform it 
	into the form of $(I_3|[T^{-1}]_{\gamma}^{\alpha})$.
	\begin{align*}
	([T]_{\alpha}^{\gamma}|I_3)=
	\left[\begin{array}{ccc|ccc}
	-1 & 2 & 2 & 1 & 0 & 0 \\ 
	0 & -1 & 4 & 0 & 1 & 0 \\ 
	0 & 0 & -1 & 0 & 0 & 1 \\ 
	\end{array}\right]\\
	r_1 \leftarrow -r_1,\quad r_2 \leftarrow -r_2, \quad r_3 \leftarrow -r_3\\
	\left[\begin{array}{ccc|ccc}
	1 & 2 & 2 & -1 & 0 & 0 \\ 
	0 & 1 & 4 & 0 & -1 & 0 \\ 
	0 & 0 & 1 & 0 & 0 & -1 \\ 
	\end{array}\right]\\
	r_1 \leftarrow 2r_2 + r_1\\
	\left[\begin{array}{ccc|ccc}
	1 & 0 & -10 & -1 & -2 & 0 \\ 
	0 & 1 & -4 & 0 & -1 & 0 \\ 
	0 & 0 & 1 & 0 & 0 & -1 \\ 
	\end{array}\right]\\
	r_1 \leftarrow 10r_3 + r_1, \quad r_2 \leftarrow 4r_3 + r_2\\
	(I_3|[T^-1]_{\alpha}^{\gamma})=
	\left[\begin{array}{ccc|ccc}
	1 & 0 & 0 & -1 & -2 & -10 \\ 
	0 & 1 & 0 & 0 & -1 & -4 \\ 
	0 & 0 & 1 & 0 & 0 & -1 \\ 
	\end{array}\right]\\
	\end{align*}	
	Therefore, the inverse of $T$ exists and is \\$T^{-1}(a_0 + a_1x + a_2x^2) 
	=-a_0 -2a_2 -10a_2 + (-a_1 - 4a_2)x -a_2x^2$
	\end{proof}
	}

	\item{
	$T : P_2(R) \rightarrow P_2(R)$ defined by $T(f(x)) = (x + 1)f'(x)$
	\begin{proof}
	\-\\
	Similiar to (a), we will calculate $[T]_{\alpha}^{\gamma}$.
	\[
	[T]_{\alpha}^{\gamma} = 
	\begin{bmatrix}
		0 & 1 & 0\\
		0 & 1 & 2\\ 
		0 & 0 & 2\\
	\end{bmatrix}
	\]
	Notice that the rank of this matrix is 2, which means rank($[T]_{\alpha}^{\gamma}$) $\neq 3$.
	Therefore, there is no inverse for this matrix exists by the remark in chapter 3.2,
	"an $nxn$ matrix is invertible if and only if its rank is $n$".
	\end{proof}
	}

	\item{
	$T:R^3 \rightarrow R^3$ defined by $T(a_1, a_2, a_3) =
	(a_1 + 2a_2 + a_3, -a_1 + a_2 + 2a_3, a_1 + a_3)$ 
	\begin{proof}
	\-\\
	Similiar to (a), we will calculate $[T]_{\alpha}^{\gamma}$.
	\[
	[T]_{\alpha}^{\gamma} = 
	\begin{bmatrix}
		1 & 2 & 1\\
		-1 & 1 & 2\\
		1 & 0 & 1 \\
	\end{bmatrix}
	\]
	We can calculate the inverse of $T$ by following the same steps we did in (a).
	\[
	[T^{-1}]_{\gamma}^{\alpha} = 
	\begin{bmatrix}
		\frac16 & -\frac13 & \frac12\\
		\frac12 & 0 & -\frac12\\
		-\frac16 & \frac13 & \frac12 \\
	\end{bmatrix}
	\]
	Therefore, the inverse of $T$ exists and is \\$T^{-1}(a_1, a_2, a_3) 
	=(\frac16a_1 -\frac13a_2 + \frac12a_3,\frac12a_1 -\frac12a_3,
	-\frac16a_1 + \frac13a_2 + \frac12a_3)$
	\end{proof}
	}

	\item{
		$T:R^3 \rightarrow P_2(R)$ defined by $T(a_1, a_2, a_3) =
		(a_1 + a_2 + a_3) + (a_1 - a_2 + a_3)x + a_1x^2$ 
		\begin{proof}
		\-\\
		Similiar to (a), we will calculate $[T]_{\alpha}^{\gamma}$.
		\[
		[T]_{\alpha}^{\gamma} = 
		\begin{bmatrix}
			1 & 1 & 1\\
			1 & -1 & 1\\
			1 & 0 & 0 \\
		\end{bmatrix}
		\]
		We can calculate the inverse of $T$ by following the same steps we did in (a).
		\[
		[T^{-1}]_{\gamma}^{\alpha} = 
		\begin{bmatrix}
			0 & 0 & 1\\
			\frac12 & -\frac12 & 0\\
			\frac12 & \frac12 & -1\\
		\end{bmatrix}
		\]
		Therefore, the inverse of $T$ exists and is \\$T^{-1}(a_1 + a_2x + a_3x^2) 
		=(a_3, \frac12a_1 - \frac12a_2, \frac12a_1 + \frac12a_2 -a_3)$
		\end{proof}
	
	}

	\item{
		$T:P_2(R) \rightarrow R^3$ defined by $T(f(x)) = (f(-1),f(0),f(1))$
		\begin{proof}
		\-\\
		Similiar to (a), we will calculate $[T]_{\alpha}^{\gamma}$.
		\[
		[T]_{\alpha}^{\gamma} = 
		\begin{bmatrix}
			1 & -1 & 1\\
			1 & 0 & 0\\
			1 & 1 & 1 \\
		\end{bmatrix}
		\]
		We can calculate the inverse of $T$ by following the same steps we did in (a).
		\[
		[T^{-1}]_{\gamma}^{\alpha} = 
		\begin{bmatrix}
			0 & 1 & 0\\
			-\frac12 & 0 & \frac12\\
			\frac12 & -1 & \frac12\\
		\end{bmatrix}
		\]
		Therefore, the inverse of $T$ exists and is \\$T^{-1}(a_1, a_2, a_3) 
		=a_2 + (-\frac12 a_1 + \frac12 a_3)x + (\frac12a_1 -a_2 + \frac12a_3)x^2$
		\end{proof}
		}

		\item{
		$T:M_{2x2}(R) \rightarrow R^4$ defined by $T(A) = (tr(A), tr(A^t), tr(EA), tr(AE))$
		where 
		$E = 
		\begin{bmatrix}
		0 & 1 \\
		1 & 0 \\ 
		\end{bmatrix}
		$
		\begin{proof}
		\-\\
		Similiar to (a), we will calculate $[T]_{\alpha}^{\gamma}$.
		\[
		[T]_{\alpha}^{\gamma} = 
		\begin{bmatrix}
			1 & 0 & 0 & 1\\
			1 & 0 & 0 & 1\\
			0 & 1 & 1 & 0\\
			0 & 1 & 1 & 0\\
		\end{bmatrix}
		\]
		Notice that the rank of this matrix is 2, which means rank($[T]_{\alpha}^{\gamma}$) $\neq 4$.
		Therefore, there is no inverse for this matrix exists by the remark in chapter 3.2,
		"an $nxn$ matrix is invertible if and only if its rank is $n$".
		\end{proof}
		}
\end{enumerate}



\section{I} 
Express the invertible matrix
$
\begin{bmatrix}
1 & 2 & 1\\
1 & 0 & 1 \\ 
1 & 1 & 2 \\ 
\end{bmatrix}
$
as a product of elementary matrices
\begin{proof}
Corollary 1 of Theorem 3.6 states that for any invertible matrix $A$\\
$I_n= A^{-1}A = E_p E_{p-1} \cdots E_1 A$, where $E_i$ are invertible elementary matrices. 
So, $A = E_1^{-1} \cdots E_{p-1}^{-1} E_p^{-1}$.
To calcuate the values of $E_i$, we can use an agumented matrix 
$(A|I_3)$ and apply elementary row operations to tranform it 
into the form of $(I_3|A^{-1})$. Each transformation to the right matrix will be our $E$. 

 \begin{align*}
	(A|I_3)=
	\left[\begin{array}{ccc|ccc}
	1 & 2 & 1 & 1 & 0 & 0 \\ 
	1 & 0 & 1 & 0 & 1 & 0 \\ 
	1 & 1 & 2 & 0 & 0 & 1 \\ 
	\end{array}\right]\\
	r_3 \leftarrow r_3 - r_2
	\left[\begin{array}{ccc|ccc}
	1 & 2 & 1 & 1 & 0 & 0 \\ 
	1 & 0 & 1 & 0 & 1 & 0 \\ 
	0 & 1 & 1 & 0 & -1 & 1 \\ 
	\end{array}\right]\\
	r_2 \leftarrow r_2 - r_1
	\left[\begin{array}{ccc|ccc}
	1 & 2  & 1 & 1  & 0  & 0 \\ 
	0 & -2 & 0 & -1 & 1  & 0 \\ 
	0 & 1  & 1 & 0  & -1 & 1 \\ 
	\end{array}\right]\\
	r_1 \leftarrow r_1 + r_2
	\left[\begin{array}{ccc|ccc}
	1 & 0  & 1 & 0  & 1  & 0 \\ 
	0 & -2 & 0 & -1 & 1  & 0 \\ 
	0 & 1  & 1 & 0  & -1 & 1 \\ 
	\end{array}\right]\\
	r_2 \leftarrow -\frac12r_2
	\left[\begin{array}{ccc|ccc}
	1 & 0 & 1 & 0  & 1  & 0 \\ 
	0 & 1 & 0 & \frac12 & -\frac12  & 0 \\ 
	0 & 1 & 1 & 0  & -1 & 1 \\ 
	\end{array}\right]\\
	r_3 \leftarrow r_3 - r_2
	\left[\begin{array}{ccc|ccc}
	1 & 0 & 1 & 0  & 1  & 0 \\ 
	0 & 1 & 0 & \frac12 & -\frac12 & 0 \\ 
	0 & 0 & 1 & -\frac12 & -\frac12 & 1 \\ 
	\end{array}\right]\\
	r_1 \leftarrow r_1 - r_3
	\left[\begin{array}{ccc|ccc}
	1 & 0 & 0 & \frac12 & \frac32  & -1 \\ 
	0 & 1 & 0 & \frac12 & -\frac12 & 0 \\ 
	0 & 0 & 1 & -\frac12 & -\frac12 & 1 \\ 
	\end{array}\right]\\
\end{align*}
By construction, $A$ is the product of the inverse of the elementary matrices on the right 
of the agumented matrix starting the first matrix shown above. 
\end{proof}

\section{J}
Suppose that $A$ and $B$ are matrices having $n$ rows. Prove that $M(A|B) = (MA|MB)$ for any $mxn$ matrix
\begin{proof}
Let $A$ have $a$ number of columns and $B$ have $b$ number of columns. \\
Notice for $1 \leq j \leq a$:
$(M(A|B))_{i,j} = \sum_{k=1}^{n} M_{i,k}A_{k,j} = (MA)_{i,j} = (MA|MB)_{i,j}$\\
Similarily for $a < j \leq a + b$:
$(M(A|B))_{i,j} = \sum_{k=1}^{n} M_{i,k}B_{k,j} = (MB)_{i,j} = (MA|MB)_{i,j}$\\
Therefore, $M(A|B) = (MA|MB)$ for any $mxn$ matrix.
\end{proof}

\section{1}
Let $V = P_n(F)$, and let $c_0, c_1, \cdots, c_n$ be distinct scalars in $F$\\
For $0 \leq i \leq n$, define $f_i \in V^*$ by $f_i(p(x)) = p(c_i)$. Prove that 
$\{f_0, f_1, \cdots, f_n\}$ is a basis for $V^*$.
\begin{proof}
	To prove that $\{f_0, f_1, \cdots, f_n\}$ is a basis for $V^*$, we must show
	$f_i$ is linear, the cardinality of the set is equal to the dimension of
	$V^*$, and that the set is linearly independent by Theorem 1.10 corollary 2.
	\begin{enumerate}[label=\roman*.]
	\item{
		$f_i$ is linear\\
		\-\\
		Show that $f_i(g(x) + ch(x)) = f_i(g(x)) + cf_i(h(x))$ 
		where $g(x), h(x) \in P_n(F)$ and $c \in F$.
	\begin{align*}	
	f_i(g(x) + ch(x)) &= g(c_i) + ch(c_i)\\
	&= f_i(g(x)) + cf_i(h(x))
	\end{align*}
	}
	\item{
		Cardinality of the set is equal to the dimension of	$V^*$ \\
		\-\\
		By the corollary to Theorem 2.20, we know dim($V^*$) = dim($L(V,F)$) = 
		dim($V$) $\cdot$ dim($F$) = dim($V$). So, dim($V^*$) = dim($V$) = $n + 1$ because $V = P_n(F)$.
		So cardinality of the set is equal to the dimension of $V^*$.
	}
	\item{
		Set is linearly independent\\
		\-\\
		Let $c_0f_0(p(x)) + c_1f_1(p(x)) + \cdots + c_nf_n(p(x)) = 0$ for some $c_i$. \\
		Let $p_i(x) = \Pi_{i \neq j}(x-c_j)$, where $p_i(c_j) = 0$ and $p_i(c_i) \neq 0$.\\
		Thus, $c_0f_0(p(x)) + c_1f_1(p(x)) + \cdots + c_nf_n(p(x)) =
		c_0f_0(c_0) + c_1f_1(c_1) + \cdots + c_nf_n(c_1)$\\
		Because $f_0(c_0), f_1(c_1), \cdots, f_n(c_n) \neq 0$, $c_0$ must equal zero and 
		all $c_i$ must be zero as well. Therefore, this set is linearly independent.
	}
	\end{enumerate}
\end{proof}

\section{2}
Use the corollary to Theorem 2.26 and (a) to show that there exist unique polynomials
$p_0(x), p_1(x), \cdots, p_n(x)$ such that $p_i(c_j) = \delta_{ij}$ for $0 \leq i \leq n$.
These polynomials are the Lagrange polynomials defined in Section 1.6
\begin{proof}
Corollary to Theorem 2.26 states that every ordered basis for $V^*$, $\{f_0, f_1, \cdots, f_n\}$, 
is the dual basis for some basis, $\beta = \{p_0, p_1, \cdots, p_m\}$, for $V$ such that 
$p_i(c_j) = \delta_{ij}$ for $0 \leq i \leq n$. Define polynomial $z \in V$ such that 
$z(c_{j}) = \delta_{kj}$. Since $\beta$ is a basis for $V$ we can write $z$ as a linear
combination of $V$. So $z=\sum_{i=0}^{n}a_ip_i$. Notice that when $z(c_0) = 1 = \sum_{i=0}^{n}a_ip_i = a_1$ and
$z(c_j) = 0 = \sum_{i=0}^{n}a_ip_i = a_j$ when $j \neq 1$. Thus, by definition, $z=p_0$ and is unique because it was
constructed from basis $\beta$. Without loss of generality, we can define $p_1, p_2, \cdots, p_n$ in the same way. 
\end{proof}

\section{3}
For any scalars $a_0, a_1, \cdots, a_n$ (not necessarily distinct), deduce that
there exists a unique polynomial $q(x)$ of degree at most $n$ such that
$q(c_i) = a_i$ for $0 \leq i \leq n$. In fact, $q(x)=\sum_{i=0}^n a_ip_i(x)$

\begin{proof}
Let $\beta = \{p_0, p_1, \cdots, p_m\}$. Let $q_0(c_i), q_1(c_i) \in V$. We define them as:\\
$q_0(x)=\sum_{i=0}^n a^0_ip_i(x)$\\
$q_1(x)=\sum_{i=0}^n a^1_ip_i(x)$\\
Notice that  $a^0_i = q_1(c_i) = \sum_{i=0}^n a^1_ip_i(x) = a^1_i$
Therefore, $q_0 = q_1$ and $q_0$ is unique.
\end{proof}

\section{4}
\begin{proof}
Substitute $a^0_i$ with $p_1(c_i)$ from the equation in question 3.
\end{proof}

\section{5}
Prove that $$\int_a^b p(t)dt = \sum_{i=0}^n p(c_i)d_i$$
where $$d_i= \int_a^b p_i(t)dt$$
Suppose now that $$c_i = a + \frac{i(b-a)}{n} \text{ for } i = 0, 1, \cdots, n $$
For $n = 1$,the preceding result yields the trapezoidal rule for evaluating the definite integral of a polynomial. For
$n = 2$, this result yields Simpson’s rule for evaluating the definite integral of a polynomial.
\begin{proof}
	\begin{align*}
	\int_a^b p(t)dt & = \int_a^b \sum_{i=0}^n p(c_i)p_i(x)dt\\
	& = \sum_{i=0}^n \int_a^b p(c_i)p_i(x)dt\\
	& = \sum_{i=0}^n \int_a^b p(c_i)d_i \\
	& = \sum_{i=0}^n p(c_i)d_i 
	\end{align*}
\end{proof}

\end{document}