\documentclass[11pt]{scrartcl}
\usepackage[sexy]{../../evan}
\usepackage{cmbright}
\usepackage{cancel}
\usepackage[T1]{fontenc}
%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
%\usepackage[pdfborder={0 0 0},colorlinks=true,citecolor=red{}]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{systeme}

\title{Math 341: Final}
\author{Daniel Ko}
\date{Spring 2020}

\begin{document}
\maketitle

%problem 1
\section{}
For $c \in \mathbb{R},$ we define the matrix $\mathbf{A}_{\mathbf{c}} \in \mathbb{R}^{3 \times 3}$ by
$$
	\mathbf{A}_{\mathbf{c}}=\begin{bmatrix}
		1 & -1 & 1 \\
		2 & 2  & 0 \\
		3 & c  & 2
	\end{bmatrix}
$$
\begin{enumerate}[label=\alph*.]
	\item {
	      Compute $\det(\mathbf{A}_{\mathbf{c}})$. Does it depend of $c$?
	      \begin{proof}
		      \begin{align*}
			      \det(\mathbf{A}_{\mathbf{c}}) & =1 \cdot \det \begin{bmatrix}2&0\\ c&2\end{bmatrix}-\left(-1\right)\det \begin{bmatrix}2&0\\ 3&2\end{bmatrix}+1\cdot \det \begin{bmatrix}2&2\\ 3&c\end{bmatrix} \\
			                                    & = (4) + (4) + (2c-6)                                                                                                        \\
			                                    & = 2c + 2
		      \end{align*}
		      Yes, it depends on $c$.
	      \end{proof}
	      }
	\item{
	      For which $c$ is the matrix $\mathbf{A}_{\mathbf{c}}$ invertible?
	      \begin{proof}
		      Corollary to Theorem 4.7 states that a matrix is invertible if and only its determinant does not equal 0.
		      From (a),
		      \begin{align*}
			      \det(\mathbf{A}_{\mathbf{c}}) = 2c + 2 & = 0  \\
			      c                                      & = -1
		      \end{align*}
		      $\det(\mathbf{A}_{\mathbf{c}}) = 0$ when $c = -1$. Hence, $\mathbf{A}_{\mathbf{c}}$ is invertible
		      for all $c$ except $c = -1$.
	      \end{proof}
	      }
	\item{
	      Compute $\mathbf{A}_{\mathbf{0}}^{-1}$ (i.e. when $c = 0$).
	      \begin{proof}
		      Using the proof to Theorem 3.2, we compute the inverse by constructing an agumented matrix
		      $(\mathbf{A}_{\mathbf{0}}|I_3)$ and applying elementary row operations to transform it into
		      the form of $(I_3|\mathbf{A}_{\mathbf{0}}^{-1})$.
		      \begin{align*}
			      (\mathbf{A}_{\mathbf{0}}|I_3)                                         & =
			      \left[\begin{array}{@{}ccc|ccc@{}}
					      1 & -1 & 1 & 1 & 0 & 0 \\
					      2 & 2  & 0 & 0 & 1 & 0 \\
					      3 & 0  & 2 & 0 & 0 & 1
				      \end{array}\right]                                    \\
			      R_2 \leftarrow R_2 - 2R_1 \quad R_3 \leftarrow R_3 - 3R_1             & =
			      \left[\begin{array}{@{}ccc|ccc@{}}
					      1 & -1 & 1  & 1  & 0 & 0 \\
					      0 & 4  & -2 & -2 & 1 & 0 \\
					      0 & 3  & -1 & -3 & 0 & 1
				      \end{array}\right]                                    \\
			      R_1 \leftarrow R_1 + \frac14R_2 \quad R_3 \leftarrow R_3 - \frac34R_2 & =
			      \left[\begin{array}{@{}ccc|ccc@{}}
					      1 & 0 & \frac12 & \frac12  & \frac14  & 0 \\
					      0 & 4 & -2      & -2       & 1        & 0 \\
					      0 & 0 & \frac12 & -\frac32 & -\frac34 & 1
				      \end{array}\right]                                    \\
			      R_1 \leftarrow R_1 - R_3 \quad R_2 \leftarrow R_2 + 4R_3              & =
			      \left[\begin{array}{@{}ccc|ccc@{}}
					      1 & 0 & 0       & 2        & 1        & -1 \\
					      0 & 4 & 0       & -8       & -2       & 4  \\
					      0 & 0 & \frac12 & -\frac32 & -\frac34 & 1
				      \end{array}\right]                                    \\
			      R_2 \leftarrow \frac14 R_2 \quad R_3 \leftarrow 2R_3                  & =
			      \left[\begin{array}{@{}ccc|ccc@{}}
					      1 & 0 & 0 & 2  & 1        & -1 \\
					      0 & 1 & 0 & -2 & -\frac12 & 1  \\
					      0 & 0 & 1 & -3 & -\frac32 & 2
				      \end{array}\right]
		      \end{align*}
		      Therefore,
		      $$
			      \mathbf{A}_{\mathbf{0}}^{-1} =
			      \begin{bmatrix}
				      2  & 1        & -1 \\
				      -2 & -\frac12 & 1  \\
				      -3 & -\frac32 & 2
			      \end{bmatrix}
		      $$
	      \end{proof}
	      }
	\item{
	      Let $b = (1, -4, 2)^t$, find the solution of $\mathbf{A}_{\mathbf{0}} x = b$

	      \begin{proof}
		      Let $x = (x_1, x_2, x_3)^t$.
		      \begin{align*}
			      \mathbf{A}_{\mathbf{0}} x & = b \\
			      \begin{bmatrix}
				      1 & -1 & 1 \\
				      2 & 2  & 0 \\
				      3 & 0  & 2
			      \end{bmatrix}
			      \begin{bmatrix}
				      x_1 \\ x_2\\ x_3
			      \end{bmatrix}
			                                & =
			      \begin{bmatrix}
				      1 \\ -4 \\ 2
			      \end{bmatrix}
		      \end{align*}
		      $$\systeme{
				      x_1 - x_2 + x_3 = 1,
				      2x_1 + 2x_2 = -4,
				      3x_1 +2x_3 = 2
			      }$$
		      \begin{align*}
			      x_1 & = -4 \\
			      x_2 & = 2  \\
			      x_3 & = 7
		      \end{align*}
	      \end{proof}

	      }
	\item{
	      Compute $\det(\mathbf{A}_{\mathbf{c}}^2)$
	      \begin{proof}
		      From Theorem 4.7, we know that
		      \begin{align*}
			      \det(\mathbf{A}_{\mathbf{c}}^2) & = \det(\mathbf{A}_{\mathbf{c}}\mathbf{A}_{\mathbf{c}})              \\
			                                      & = \det(\mathbf{A}_{\mathbf{c}}) \cdot \det(\mathbf{A}_{\mathbf{c}}) \\
			                                      & = (2c + 2) (2c + 2)                                                 \\
			                                      & = 4c^2+8c+4
		      \end{align*}
	      \end{proof}
	      }
	\item{
	      Compute $\det(5\mathbf{A}_{\mathbf{c}})$
	      \begin{proof}
		      Using the second "Properties of the Determinant" on pg 234,
		      \begin{align*}
			      \det(5\mathbf{A}_{\mathbf{c}}) & = 5^3\det(\mathbf{A}_{\mathbf{c}}) \\
			                                     & = 5^3(2c + 2)                      \\
			                                     & = 250c + 250
		      \end{align*}
	      \end{proof}
	      }
	\item{
	      Compute $\det(\mathbf{E}_{\mathbf{k}}\mathbf{A}_{\mathbf{c}})$ where,
	      $$
		      \mathbf{E}_{\mathbf{k}} = \begin{bmatrix}
			      1 & 0 & 0 \\
			      k & 1 & 0 \\
			      0 & 0 & 1
		      \end{bmatrix}
	      $$
	      \begin{proof}
		      \begin{align*}
			      \det(\mathbf{E}_{\mathbf{k}}) & = 1\cdot \det \begin{bmatrix}1&0\\ 0&1\end{bmatrix}-0\cdot \det \begin{bmatrix}k&0\\ 0&1\end{bmatrix}+0\cdot \det \begin{bmatrix}k&1\\ 0&0\end{bmatrix} \\
			                                    & = 1
		      \end{align*}
		      From Theorem 4.7, we know that,
		      \begin{align*}
			      \det(\mathbf{E}_{\mathbf{k}}\mathbf{A}_{\mathbf{c}}) & = \det(\mathbf{E}_{\mathbf{k}}) \det(\mathbf{A}_{\mathbf{c}}) \\
			                                                           & = (1) (2c + 2)                                                \\
			                                                           & = 2c + 2
		      \end{align*}
	      \end{proof}
	      }

	\item{
	      Compute $\det(\mathbf{D}_{\mathbf{k}}\mathbf{A}_{\mathbf{c}})$ where,
	      $$
		      \mathbf{D}_{\mathbf{k}} = \begin{bmatrix}
			      1 & 0 & 0 \\
			      0 & k & 0 \\
			      0 & 0 & 1
		      \end{bmatrix}
	      $$
	      \begin{proof}
		      \begin{align*}
			      \det(\mathbf{D}_{\mathbf{k}}) & = 1\cdot \det \begin{bmatrix}k&0\\ 0&1\end{bmatrix}-0\cdot \det \begin{bmatrix}0&0\\ 0&1\end{bmatrix}+0\cdot \det \begin{bmatrix}0&k\\ 0&0\end{bmatrix} \\
			                                    & = k
		      \end{align*}
		      From Theorem 4.7, we know that,
		      \begin{align*}
			      \det(\mathbf{E}_{\mathbf{k}}\mathbf{A}_{\mathbf{c}}) & = \det(\mathbf{D}_{\mathbf{k}}) \det(\mathbf{A}_{\mathbf{c}}) \\
			                                                           & = (k) (2c + 2)                                                \\
			                                                           & = 2kc + 2k
		      \end{align*}
	      \end{proof}
	      }
	\item{
	      Compute $\det(\mathbf{A}_{\mathbf{0}}^{-1})$
	      \begin{proof}
		      From (c) we know that,
		      $$
			      \mathbf{A}_{\mathbf{0}}^{-1} =
			      \begin{bmatrix}
				      2  & 1        & -1 \\
				      -2 & -\frac12 & 1  \\
				      -3 & -\frac32 & 2
			      \end{bmatrix}
		      $$
		      It follows that,
		      \begin{align*}
			      \det(\mathbf{A}_{\mathbf{0}}^{-1}) & = 2\cdot \det \begin{bmatrix}-\frac{1}{2}&1\\ -\frac{3}{2}&2\end{bmatrix}-1\cdot \det \begin{bmatrix}-2&1\\ -3&2\end{bmatrix}-1\cdot \det \begin{bmatrix}-2&-\frac{1}{2}\\ -3&-\frac{3}{2}\end{bmatrix} \\
			                                         & = 2 (\frac{1}{2}) -1 (-1) -1 (\frac{3}{2})                                                                             \\
			                                         & = \frac12
		      \end{align*}
	      \end{proof}
	      }
	\item{
	      Compute the eigenvalues of $\mathbf{A}_{\mathbf{0}}$. Can you diagonalize $\mathbf{A}_{\mathbf{0}}$?
	      \begin{proof}
		      Using Theorem 5.2, we compute the eigenvalues of $\mathbf{A}_{\mathbf{0}}$ by computing its
		      characteristic polynomial.
		      \begin{align*}
			      \det(\mathbf{A}_{\mathbf{0}} - \lambda I) & = \det \begin{bmatrix}
				      1 - \lambda & -1          & 1           \\
				      2           & 2 - \lambda & 0           \\
				      3           & 0           & 2 - \lambda
			      \end{bmatrix}         \\
			                                                & = -(\lambda -2)(\lambda^2 - 3 \lambda +1)
		      \end{align*}
		      We compute when $-(\lambda -2)(\lambda^2 - 3 \lambda +1) = 0$.
		      When $\lambda = 2$, the characteristic polynomial equals $0$.
		      Using the quadratic formula, when $\lambda = \frac{3 \pm \sqrt{5}}{2}$, the characteristic polynomial equals $0$.
		      Hence the eigenvalues of $\mathbf{A}_{\mathbf{0}}$ are $$2, \frac{3 + \sqrt{5}}{2}, \frac{3 - \sqrt{5}}{2}$$
		      We can rewrite the characteristic polynomial as
		      $$ f(\lambda) = -(\lambda -2)(\lambda - \frac{3 + \sqrt{5}}{2})(\lambda - \frac{3 - \sqrt{5}}{2})$$
		      Using the "Test for Diagonalization" outlined in pg 269,  we determine if
		      $\mathbf{A}_{\mathbf{0}}$ can be diagonalized.
		      It is clear that the first condition, the characteristic polynomial of T splits, holds.
		      By Theorem 5.7, condition 2 is automatically satisfied for eigenvalues
		      of multiplicity 1. Therefore, A is diagonalizable.
	      \end{proof}
	      }
	\item{
	      Compute the eigenvalues of $\mathbf{A}_{\mathbf{0}}^{-1}$
	      \begin{proof}
		      Notice that since eigenvalues are non zero,
		      \begin{align*}
			      \mathbf{A}_{\mathbf{0}} x                              & = \lambda x                             \\
			      \mathbf{A}_{\mathbf{0}}^{-1} \mathbf{A}_{\mathbf{0}} x & = \mathbf{A}_{\mathbf{0}}^{-1}\lambda x \\
			      x                                                      & = \mathbf{A}_{\mathbf{0}}^{-1}\lambda x \\
			      \frac{1}{\lambda}x                                     & = \mathbf{A}_{\mathbf{0}}^{-1} x
		      \end{align*}
		      Thus the eigenvalues of $\mathbf{A}_{\mathbf{0}}^{-1}$ are
		      $$\frac12, \frac{3 + \sqrt{5}}{2}, \frac{3 - \sqrt{5}}{2}$$
	      \end{proof}
	      }
\end{enumerate}


\section{}
Consider the transformation $T: \mathbb{R}^{3} \rightarrow \mathbb{R}^{4}$ given by
$$
	T(\mathbf{x})=\left(\begin{array}{c}
			x_{2}-x_{1}+\alpha\left(x_{2}^{2}+x_{3}^{3}\right) \\
			x_{1}+x_{2}+2 g x_{3}+\alpha x_{1}^{2}             \\
			x_{1}+x_{2}+2 x_{3}                                \\
			h x_{3}+q
		\end{array}\right), \text { where } \mathbf{x}=\left(\begin{array}{c}
			x_{1} \\
			x_{2} \\
			x_{3}
		\end{array}\right)
$$
in which, $h, g, q$ and $\alpha$ are real numbers.

\begin{enumerate}[label=\alph*.]
	\item {
	      What are the condition on $\alpha$ and $q$ such that the transformation is linear?
	      Explain briefly.
	      \begin{proof}
		      We know that if $T$ is linear then $T(0) = 0$. Hence $q = 0$ for $T$ to be linear.
		      Notice that we must not have any terms higher than degree 1 because for example
		      if we have a transformation $U(x) = x^2$ and let $c \in \mathbb{R}$ then,
		      \begin{align*}
			      U(cx) & = (cx)^2   \\
			            & = c^2x^2   \\
			            & \neq cU(x)
		      \end{align*}
		      which means $U$ is not a linear transformation.
		      Thus, $\alpha = 0$ so that there are no terms higher than degree 1.
	      \end{proof}
	      }
	\item{
	      From now we suppose that $q = 0$ and $\alpha = 0$. Write the representation
	      matrix $A = [T]_{\epsilon_3}^{\epsilon_4}$.
	      \begin{proof}
		      We now define
		      $$
			      T(\mathbf{x})=\left(\begin{array}{c}
					      x_{2}-x_{1}           \\
					      x_{1}+x_{2}+2 g x_{3} \\
					      x_{1}+x_{2}+2 x_{3}   \\
					      h x_{3}
				      \end{array}\right), \text { where } \mathbf{x}=\left(\begin{array}{c}
					      x_{1} \\
					      x_{2} \\
					      x_{3}
				      \end{array}\right)
		      $$
		      We now compute the representation matrix
		      $$
			      T(e_1)  = \left(\begin{array}{c}-1\\1\\1\\0\end{array}\right) \quad
			      T(e_2)  = \left(\begin{array}{c}1\\1\\1\\0\end{array}\right) \quad
			      T(e_3)  = \left(\begin{array}{c}0\\2g\\2\\h\end{array}\right)
		      $$
		      \begin{align*}
			      A & = [[T(e_1)]_{\epsilon_4}[T(e_2)]_{\epsilon_4}[T(e_3)]_{\epsilon_4}] \\
			        & = \begin{bmatrix}
				      -1 & 1 & 0  \\
				      1  & 1 & 2g \\
				      1  & 1 & 2  \\
				      0  & 0 & h
			      \end{bmatrix}
		      \end{align*}
	      \end{proof}
	      }
	\item{
	      What are the conditions on $h$ and $g$ such that the transformation $T$ maps $\mathbb{R}^3$
	      onto $\mathbb{R}^4$? Explain briefly.
	      \begin{proof}
		      Since $T : \mathbb{R}^3 \rightarrow \mathbb{R}^4$, there are no $h$ and $g$ such that $T$ is onto.
		      This is because $T$ is onto if and only if the range of $T$ equals the codomain, but in this
		      linear transformation, the dimension of codomain is greater than the domain so by rank nullity,
		      $T$ cannot be onto.
	      \end{proof}
	      }
	\item{
	      What are the conditions on $h$ and $g$ such that the transformation $T$ is one to one? Explain briefly.
	      \begin{proof}
		      By Theorem 2.4, $T$ is one to one if and only if $N(T) = \{0\}$. Using the rank nullity theorem,
		      this is equivalent to saying that $rank(T) = 3$, i.e.
		      \begin{align*}
			      dim(\mathbb{R}^3) & = rank(T) + nullity(T) \\
			      3                 & = rank(T)
		      \end{align*}
		      We know that $rank(T) = rank([T]_{\epsilon_3}^{\epsilon_4})$ by Theorem 3.3.
		      Additionally by Theorem 3.5., the rank of a matrix is the number of linearly independent columns.
		      It is clear that $A$ will have rank 3 if $h \neq 0 $. There are no conditions for $g$.
	      \end{proof}
	      }
	\item{
	      Suppose that $h = 0$, then
	      $$A =
		      \begin{bmatrix}
			      -1 & 1 & 0  \\
			      1  & 1 & 2g \\
			      1  & 1 & 2  \\
			      0  & 0 & 0
		      \end{bmatrix}
		      \text{ and let }
		      b =
		      \begin{bmatrix}
			      0 \\
			      2 \\
			      r \\
			      0
		      \end{bmatrix}
	      $$
	      What are the conditions on $r$ and $g$ such that the system $Ax = b$ has a solution? When
	      is the solution unique?
	      \begin{proof}
		      We begin by putting the augmented matrix $(A|b)$ in its reduced form.
		      $$(A'|b') = \left[\begin{array}{@{}ccc|c@{}}-1&1&0&0\\ 0&2&2g&2\\ 0&0&2-2g&r-2\\ 0&0&0&0\end{array}\right]$$
		      By Theorem 3.11 and 3.13, a system is consistent if and only if $rank (A') = rank (A'|b') .$
		      So for the system to have as solution, $2 - 2g \neq 0$ if $r - 2 \neq 0$.	If $r - 2 = 0$, there is no conditions
		      on $g$.\par
		      In summary, for the system to have as solution if $r \neq 2$ then $g \neq 1$. If $r = 2$, there are no conditions
		      on $g$.\par
		      Combining Theorem 3.10 and the corollary to Theorem 4.7, we get that a system has a
		      unique solution if and only if $\det(A) \neq 0$.
		      The determinant of an upper triangular matrix is the product of its diagonal
		      entries by property 4 of determinants in section 4.4.
		      Using this fact we can compute the condition of $g$ as such that
		      \begin{align*}
			      -1*2*(2-2g) & \neq 0 \\
			      4g -4       & \neq 0 \\
			      g           & \neq 1
		      \end{align*}
		      There is no conditions for $r$ when the solution is unique.
	      \end{proof}
	      }
	\item{
	      Suppose that $h = 0, g = 1, r = 2$. Solve $\mathbf{Ax=b}$ and give the answer
	      in parametric form.
	      \begin{proof}
		      We define $$\mathbf{A} =
			      \begin{bmatrix}
				      -1 & 1 & 0 \\
				      1  & 1 & 2 \\
				      1  & 1 & 2 \\
				      0  & 0 & 0
			      \end{bmatrix}
			      \quad
			      \mathbf{b} =
			      \begin{bmatrix}
				      0 \\
				      2 \\
				      2 \\
				      0
			      \end{bmatrix}
		      $$
		      We can compute a solution space to $\mathbf{Ax=b}$ as outlined in Theorem 3.9. We start by first computing the
		      solution set to $\mathbf{Ax}=0$ denoted by $K_H$. It is clear that $rank(\mathbf{A}) = 2$ because the first two
		      columns are linearly independent and the third column is the sum of the first two columns.
		      By Theorem 3.8, $dim(K_H) = 3 - 2 = 1$. Thus any nonzero solution constitutes a basis for K. For example, since
		      $$
			      \begin{pmatrix}
				      1 \\
				      1 \\
					  -1  \\
				      0
			      \end{pmatrix}
		      $$
		      is a solution to the $\mathbf{Ax}=0$, it is a basis for $K_H$ by Corollary 2 of Theorem 1.10.
		      So a solution set to $K_H$ would be
		      \[
			      K_H=
			      \left\{
			      t\begin{pmatrix}
				      1  \\
				      1 \\
				      -1  \\
				      0
			      \end{pmatrix}: t \in \mathbb{R}
			      \right\}
		      \]
		      A solution to $\mathbf{Ax=b}$ is
		      $$
			      \begin{pmatrix}
				      1 \\
				      1 \\
				      0 \\
				      0
			      \end{pmatrix}
		      $$
		      Therefore, by Theorem 3.9 we compute the solution space as
		      \[
			      K=
			      \left\{
			      \begin{pmatrix}
				      1 \\
				      1 \\
				      0 \\
				      0
			      \end{pmatrix}+
			      t\begin{pmatrix}
				      1  \\
				      1 \\
				      -1  \\
				      0
			      \end{pmatrix}: t \in \mathbb{R}
			      \right\}
		      \]
	      \end{proof}
	      }
\end{enumerate}

\section{}


\end{document}
