\documentclass[11pt]{scrartcl}
\usepackage[sexy]{../../evan}
\usepackage{cmbright}
\usepackage{cancel}
\usepackage[T1]{fontenc}
%\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
%\usepackage[pdfborder={0 0 0},colorlinks=true,citecolor=red{}]{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{systeme}


\title{Math 341: Homework 8}
\author{Daniel Ko}
\date{Spring 2020}

\begin{document}
\maketitle

%5.1.6
\section{A}
Let $T$ be a linear operator on a finite-dimensional vector space $V$, and let $\beta$ be an ordered basis for $V$.
Prove that $\lambda$ is an eigenvalue of $T$ if and only if $\lambda$ is an eigenvalue of $[T]_\beta$
\begin{proof}\
	\begin{enumerate}[label=\alph*.]
		\item{
		      $\lambda$ is an eigenvalue of $T \Rightarrow \lambda$ is an eigenvalue of $[T]_\beta$\par
		      By definition, there exists a eigenvector $v \in V$ such that $T(v) = \lambda v$.
		      Using Theorem 2.14,
		      \begin{align*}
			      T(v)                & = \lambda v         \\
			      [T(v)]_\beta        & = [\lambda v]_\beta \\
			      [T]_\beta [v]_\beta & = \lambda [v]_\beta
		      \end{align*}
		      as desired.
		      Thus, $\lambda$ is an eigenvalue of $[T]_\beta$.
		      }
		\item{
		      $\lambda$ is an eigenvalue of $[T]_\beta \Rightarrow \lambda$ is an eigenvalue of $T$ \par
		      By definition, there exists a eigenvector $v \in V$ such that $[T]_\beta [v]_\beta = \lambda [v]_\beta$.
		      Using Theorem 2.14,
		      \begin{align*}
			      [T]_\beta [v]_\beta & = \lambda [v]_\beta \\
			      [T(v)]_\beta        & = [\lambda v]_\beta \\
			      T(v)                & = \lambda v
		      \end{align*}
		      as desired.
		      Thus, $\lambda$ is an eigenvalue of $T$.
		      }
	\end{enumerate}
	Therefore, $\lambda$ is an eigenvalue of $T$ if and only if $\lambda$ is an eigenvalue of $[T]_\beta$.
\end{proof}

%5.1.8
\section{B}
\begin{enumerate}[label=\alph*.]
	\item{
	      Prove that a linear operator $T$ on a finite-dimensional vector space is invertible if and only if zero is not an
	      eigenvalue of $T$.
	      \begin{proof}\
		      \begin{enumerate}[label=\roman*.]
			      \item{
			            Linear operator $T$ on a finite-dimensional vector space is invertible $\Rightarrow$ zero is not an
			            eigenvalue of $T$.\par
			            By the corollary of Theorem 4.7, $\text{det}(T) \neq 0$. Assume, for the sake of contradiction, suppose
			            zero is an eigenvalue of $T$.
			            It follows from Theorem 5.2 that
			            \begin{align*}
				            \text{det}(T - \lambda I) & = 0 \\
				            \text{det}(T - 0I)        & = 0 \\
				            \text{det}(T)             & = 0
			            \end{align*}
			            which is a contradiction. Thus, zero is not an eigenvalue of $T$.
			            }
			      \item{
			            Zero is not an eigenvalue of $T\Rightarrow$ linear operator $T$ on a finite-dimensional vector space is invertible.\par
			            By contrapositive, we will instead prove that if linear operator $T$ on a finite-dimensional vector space is not invertible
			            then zero is an eigenvalue of $T$. If $T$ is not invertible then $\text{det}(T) = 0$ by corollary of Theorem 4.7.
			            It follows from Theorem 5.2 that
			            \begin{align*}
				            \text{det}(T - \lambda I) & = 0
			            \end{align*}
			            It directly follows that zero is an eigenvalue of $T$.
			            }
		      \end{enumerate}
		      Therefore, linear operator $T$ on a finite-dimensional vector space is invertible if and only if zero is not an
		      eigenvalue of $T$.
	      \end{proof}
	      }
	\item{
	      Let $T$ be an invertible linear operator. Prove that a scalar $\lambda$ is an eigenvalue of $T$ if and only if
	      $\lambda^{-1}$ is an eigenvalue of $T^{-1}$.
	      \begin{proof}\
		      \begin{enumerate}[label=\roman*.]
			      \item{
			            A scalar $\lambda$ is an eigenvalue of T $\Rightarrow$ $\lambda^{-1}$ is an eigenvalue of $T^{-1}$.\par
			            By definition, there exists a eigenvector $v \in V$ such that $T(v) = \lambda v$. Given that $T$ is invertible
			            and by definition eigenvalues are non zero,
			            \begin{align*}
				            T(v)          & = \lambda v         \\
				            T^{-1}(T(v))  & = T^{-1}(\lambda v) \\
				            v             & = T^{-1}(\lambda v) \\
				            v             & = \lambda T^{-1}(v) \\
				            \lambda^{-1}v & =  T^{-1}(v)
			            \end{align*}
			            as desired. Thus, $\lambda^{-1}$ is an eigenvalue of $T^{-1}$.
			            }
			      \item{
			            $\lambda^{-1}$ is an eigenvalue of $T^{-1}$ $\Rightarrow$  a scalar $\lambda$ is an eigenvalue of T.\par
			            By definition, there exists a eigenvector $v \in V$ such that $T^{-1}(v) = \lambda^{-1} v$.
			            Given that $T^{-1}$ is invertible linear operator,
			            \begin{align*}
				            T^{-1}(v)    & = \lambda^{-1} v    \\
				            T(T^{-1}(v)) & = T(\lambda^{-1} v) \\
				            v            & = \lambda^{-1}T(v)  \\
				            \lambda v    & = T(v)
			            \end{align*}
			            as desired. Thus, $\lambda$ is an eigenvalue of $T$.
			            }
		      \end{enumerate}
		      Therefore, a scalar $\lambda$ is an eigenvalue of $T$ if and only if
		      $\lambda^{-1}$ is an eigenvalue of $T^{-1}$.
	      \end{proof}
	      }
	      %PROBABLY WRONG FIX LATER
	\item{
	      State and prove results analogous to (a) and (b) for matrices.
	      \begin{enumerate}[label=(\alph*)]
		      \item{
		            A matrix $A$ is invertible if and only if zero is not an eigenvalue of $A$.
		            \begin{proof}
			            Since $A$ is an invertible matrix, the corresponding left multiplication transformation
			            is also invertible by corollary 2 of Theorem 2.18. It directly follows from the proof from (a)
			            that "matrix $A$ is invertible if and only if zero is not an eigenvalue of $A$."
			            is true.
		            \end{proof}
		            }
		      \item{
		            Let $A$ be an invertible matrix.
		            $\lambda$ is an eigenvalue of $A$ if and only if $\lambda^{-1}$ is an eigenvalue of $A^{-1}$.
		            \begin{proof}
			            Since $A$ is an invertible matrix, the corresponding left multiplication transformation
			            is also invertible by corollary 2 of Theorem 2.18. It directly follows from the proof from (b)
			            that "$\lambda$ is an eigenvalue of $A$ if and only if $\lambda^{-1}$ is an eigenvalue of $A^{-1}$"
			            is true.
		            \end{proof}
		            }
	      \end{enumerate}
	      }
\end{enumerate}

%5.1.14
\section{C}
For any square matrix $A$, prove that $A$ and $A^t$ have the same characteristic polynomial, and hence the same
eigenvalues.
\begin{proof}\
	Let $A \in M_{nxn}(F)$. The characteristic polynomial for A is $f(t) = \text{det}(A - tI_n)$.
	Using Theorem 4.8, Theorem 2.12, and the trivial fact that the identity matrix is symmetric,
	\begin{align*}
		f(t) & = det(A - tI_n)        \\
		     & = det((A - tI_n)^t)    \\
		     & = det(A^t - (tI_n)^t)) \\
		     & = det(A^t - tI_n)
	\end{align*}
	which is exactly the characteristic polynomial for $A^t$. Therefore, $A$ and $A^t$ have the same characteristic polynomial, and hence the same
	eigenvalues.
\end{proof}

\section{D}
Let $T$ be a linear operator on a finite-dimensional vector space $V$, and let $c$ be any scalar.
\begin{enumerate}[label=\alph*.]
	\item{
	      Determine the relationship between the eigenvalues and eigenvectors of $T$ (if any) and the eigenvalues and
	      eigenvectors of $U = T - cI$ (where $I$ is the identity transformation) Justify your answers.
	      \begin{proof}
		      Suppose $v \in V$ is an eigenvector of $T$ where $\lambda$ is its eigenvalue,
		      $$Tv = \lambda v$$
		      Applying the transformation $U$ to $v$,
		      \begin{align*}
			      Uv & = (T - cI)v      \\
			         & = Tv - cIv       \\
			         & = \lambda v - cv \\
			         & = (\lambda - c)v
		      \end{align*}
		      Thus, if $v$ is an eigenvector of $T$, then it is an eigenvector $U$ with its corresponding
		      eigenvalue being $c$.
	      \end{proof}
	      }
	\item{
	      Prove that $T$ is diagonalizable if and only if $U$ is diagonalizable.
	      \begin{proof}
		      Since $T$ is diagonalizable, there exists an ordered basis $\beta$ for $V$
		      consisting of eigenvectors of $T$ by Theorem 5.1.
			  From (a), we know that all eigenvectors of $T$ are eigenvectors of $U$.
			  Thus, $\beta$ is an ordered basis consisting of eigenvectors of $U$.
			  Thus, $U$ is diagonalizable. Without loss of generality, if $U$ is diagonalizable
			  then $T$ is diagonalizable. Therefore, $T$ is diagonalizable if and only if $U$ is diagonalizable.

	      \end{proof}
	      }
\end{enumerate}

%5.2.2 
\section{E}

%5.2.7 

%5.2.9 \section{F}

%5.2.9 \section{G}

%5.3.17 + 5.3.18	

%5.4.19

%5.4.23 + 24 + 25



\end{document}